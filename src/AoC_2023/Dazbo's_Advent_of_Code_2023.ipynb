{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "This notebook contains my solutions for **<a href=\"https://adventofcode.com/2023\" target=\"_blank\">Advent of Code 2023</a>**.\n",
    "\n",
    "A few notes...\n",
    "- The source for this notebook source lives in my GitHub repo, <a href=\"https://github.com/derailed-dash/Advent-of-Code/blob/master/src/AoC_2023/Dazbo's_Advent_of_Code_2023.ipynb\" target=\"_blank\">here</a>.\n",
    "- You can run this Notebook wherever you like. For example, you could...\n",
    "  - Run it locally, in your own Jupyter environment.\n",
    "  - Run it in a cloud-based Jupyter environment, with no setup required on your part!  For example, <a href=\"https://colab.research.google.com/github/derailed-dash/Advent-of-Code/blob/master/src/AoC_2023/Dazbo's_Advent_of_Code_2023.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Google Colab\"/></a>\n",
    "- **To run the notebook, execute the cells in the [Setup](#Setup) section, as described below. Then you can run the code for any given day.**\n",
    "- Be mindful that the first time you run this notebook, you will need to **obtain your AoC session key** and store it, if you have not done so already. This allows the notebook to automatically retrieve your input data. (See the guidance in the **[Get Access to Your AoC Data](#Get-Access-to-Your-AoC-Data)** section for details.)\n",
    "- Use the navigation menu on the left to jump to any particular day.\n",
    "- All of my AoC solutions are documented in my <a href=\"https://aoc.just2good.co.uk/\" target=\"_blank\">AoC Python Walkthrough site</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3PMQJRrR38P",
    "tags": []
   },
   "source": [
    "# Setup\n",
    "\n",
    "**You need to run all cells in this section, before running any particular day solution.**\n",
    "\n",
    "You can run all the setup steps in one go:\n",
    "\n",
    "- From the notebook in VS Code, go to \"Day 1\", and then click on the `Run all above cells` button.\n",
    "- From the notebook in Google Colab, click on `Setup` in the _Table of Contents_, click on the `...` and then click on `Run cells in section`.\n",
    "\n",
    "![Run cells in section](https://aoc.just2good.co.uk/assets/images/run_cells_in_section.png)\n",
    "\n",
    "The setup steps will try to install dependent software applications like _ffmpeg_ and _graphviz_. These are used in some of the visualisations. If these applications are not yet installed in your environment, you may be prompted to install them.  You don't need to install these applications, if you're not fussed about all the visualisations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWxsAMaXTGEQ"
   },
   "source": [
    "## Packages and Imports\n",
    "\n",
    "Here we use `pip` to install the packages used by my solutions in this event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Dgeww1rMrkQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install jupyterlab-lsp colorama python-dotenv ipykernel ffmpeg graphviz mediapy sympy shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5Ki_HvOJUWk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import abc\n",
    "import ast\n",
    "from collections import Counter, deque, defaultdict\n",
    "import copy\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from enum import Enum, auto\n",
    "from functools import cache, reduce\n",
    "import heapq\n",
    "import imageio.v3 as iio\n",
    "from io import BytesIO\n",
    "from itertools import permutations, combinations, count, cycle\n",
    "import logging\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import platform\n",
    "import re\n",
    "import requests\n",
    "from typing import Optional, Callable, cast\n",
    "import sympy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "import graphviz\n",
    "from IPython.display import display, YouTubeVideo, HTML\n",
    "from IPython.core.display import Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.markers import MarkerStyle\n",
    "from matplotlib import path as pltpath\n",
    "import mediapy as media\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely import Polygon\n",
    "from sympy.plotting import plot as sympyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZNTgtGzUImv",
    "tags": []
   },
   "source": [
    "## Logging and Output\n",
    "\n",
    "Set up a new logger that uses `ColouredFormatter`, such that we have coloured logging.  The log colour depends on the logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwzjfUFCKhXe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# SETUP LOGGING\n",
    "#\n",
    "# Create a new instance of \"logger\" in the client application\n",
    "# Set to your preferred logging level\n",
    "# And add the stream_handler from this module, if you want coloured output\n",
    "##########################################################################\n",
    "\n",
    "# logger for aoc_commons only\n",
    "logger = logging.getLogger(__name__) # aoc_common.aoc_commons\n",
    "logger.setLevel(logging.INFO)\n",
    "stream_handler = None\n",
    "\n",
    "class ColouredFormatter(logging.Formatter):\n",
    "    \"\"\" Custom Formater which adds colour to output, based on logging level \"\"\"\n",
    "\n",
    "    level_mapping = {\"DEBUG\": (Fore.BLUE, \"DBG\"),\n",
    "                     \"INFO\": (Fore.GREEN, \"INF\"),\n",
    "                     \"WARNING\": (Fore.YELLOW, \"WRN\"),\n",
    "                     \"ERROR\": (Fore.RED, \"ERR\"),\n",
    "                     \"CRITICAL\": (Fore.MAGENTA, \"CRT\")\n",
    "    }\n",
    "\n",
    "    def __init__(self, *args, apply_colour=True, shorten_lvl=True, **kwargs) -> None:\n",
    "        \"\"\" Args:\n",
    "            apply_colour (bool, optional): Apply colouring to messages. Defaults to True.\n",
    "            shorten_lvl (bool, optional): Shorten level names to 3 chars. Defaults to True.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._apply_colour = apply_colour\n",
    "        self._shorten_lvl = shorten_lvl\n",
    "\n",
    "    def format(self, record):\n",
    "        if record.levelname in ColouredFormatter.level_mapping:\n",
    "            new_rec = copy.copy(record)\n",
    "            colour, new_level = ColouredFormatter.level_mapping[record.levelname]\n",
    "\n",
    "            if self._shorten_lvl:\n",
    "                new_rec.levelname = new_level\n",
    "\n",
    "            if self._apply_colour:\n",
    "                msg = colour + super().format(new_rec) + Fore.RESET\n",
    "            else:\n",
    "                msg = super().format(new_rec)\n",
    "\n",
    "            return msg\n",
    "\n",
    "        # If our logging message is not using one of these levels...\n",
    "        return super().format(record)\n",
    "\n",
    "if not stream_handler:\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_fmt = ColouredFormatter(fmt='%(asctime)s.%(msecs)03d:%(name)s - %(levelname)s: %(message)s',\n",
    "                                   datefmt='%H:%M:%S')\n",
    "    stream_handler.setFormatter(stream_fmt)\n",
    "    \n",
    "if not logger.handlers:\n",
    "    # Add our ColouredFormatter as the default console logging\n",
    "    logger.addHandler(stream_handler)\n",
    "\n",
    "def retrieve_console_logger(script_name):\n",
    "    \"\"\" Create and return a new logger, named after the script\n",
    "    So, in your calling code, add a line like this:\n",
    "    logger = ac.retrieve_console_logger(locations.script_name)\n",
    "    \"\"\"\n",
    "    a_logger = logging.getLogger(script_name)\n",
    "    a_logger.addHandler(stream_handler)\n",
    "    a_logger.propagate = False\n",
    "    return a_logger\n",
    "\n",
    "def setup_file_logging(a_logger: logging.Logger, folder: str|Path=\"\"):\n",
    "    \"\"\" Add a FileHandler to the specified logger. File name is based on the logger name.\n",
    "    In calling code, we can add a line like this:\n",
    "    td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "    Args:\n",
    "        a_logger (Logger): The existing logger\n",
    "        folder (str): Where the log file will be created. Will be created if it doesn't exist\n",
    "    \"\"\"\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)     # Create directory if it does not exist\n",
    "    file_handler = logging.FileHandler(Path(folder, a_logger.name + \".log\"), mode='w')\n",
    "    file_fmt = logging.Formatter(fmt=\"%(asctime)s.%(msecs)03d:%(name)s:%(levelname)8s: %(message)s\",\n",
    "                                datefmt='%H:%M:%S')\n",
    "    file_handler.setFormatter(file_fmt)\n",
    "    a_logger.addHandler(file_handler)\n",
    "    \n",
    "def top_and_tail(data, block_size=5, include_line_numbers=True, zero_indexed=False):\n",
    "    \"\"\" Print a summary of a large amount of data \n",
    "\n",
    "    Args:\n",
    "        data (_type_): The data to present in summary form.\n",
    "        block_size (int, optional): How many rows to include in the top, and in the tail.\n",
    "        include_line_numbers (bool, optional): Prefix with line number. Defaults to True.\n",
    "        zero_indexed (bool, optional): Lines start at 0? Defaults to False.\n",
    "    \"\"\"\n",
    "    if isinstance(data, list):\n",
    "        # Get the number of digits of the last item for proper alignment\n",
    "        num_digits_last_item = len(str(len(data)))\n",
    "\n",
    "        # Format the string with line number\n",
    "        def format_with_line_number(idx, line):\n",
    "            start = 0 if zero_indexed else 1\n",
    "            if include_line_numbers:\n",
    "                return f\"{idx + start:>{num_digits_last_item}}: {line}\"\n",
    "            else:\n",
    "                return line\n",
    "\n",
    "        start = 0 if zero_indexed else 1\n",
    "        if len(data) < 11:\n",
    "            return \"\\n\".join(format_with_line_number(i, line) for i, line in enumerate(data))\n",
    "        else:\n",
    "            top = [format_with_line_number(i, line) for i, line in enumerate(data[:block_size])]\n",
    "            tail = [format_with_line_number(i, line) for i, line in enumerate(data[-block_size:], start=len(data)-block_size)]\n",
    "            return \"\\n\".join(top + [\"...\"] + tail)\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages\n",
    "\n",
    "- [ffmpeg](https://ffmpeg.org/): in order to render video output, i.e. for visualisations.\n",
    "- graphviz: for visualising graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_software():\n",
    "    os_name = platform.system()\n",
    "    logger.info(f\"Installing packages on {os_name}...\")\n",
    "    if os_name == \"Windows\":\n",
    "        os.system(\"winget install ffmpeg --silent --no-upgrade\")\n",
    "        os.system(\"winget install graphviz --silent --no-upgrade\")\n",
    "    elif os_name == \"Linux\":\n",
    "        os.system(\"apt-get -qq update && apt-get -qq -y install ffmpeg\")\n",
    "        os.system(\"apt -qq -y install graphviz\")\n",
    "    elif os_name == \"Darwin\":\n",
    "        os.system(\"brew install ffmpeg\")\n",
    "        os.system(\"brew install graphviz\")\n",
    "    else:\n",
    "        logger.error(f\"Unsupported operating system: {os_name}\")\n",
    "\n",
    "install_software()\n",
    "\n",
    "logger.info(\"Note that installed applications may not be immediately available after first installing.\\n\" \\\n",
    "            \"It may be necessary to relaunch the notebook environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking `ffmpeg` version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking `graphviz dot` version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured\n",
    "!dot -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"version\" not in captured.stderr:\n",
    "    logger.error(captured.stderr)\n",
    "    logger.info(\"If dot is not found, consider explicitly adding your Graphviz installation location to your user path.\")\n",
    "    logger.info(\"E.g. 'C:\\\\Program Files\\\\Graphviz\\\\bin'\")\n",
    "else:\n",
    "    logger.info(captured.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y61RhVrHSjVA"
   },
   "source": [
    "## Get Access to Your AoC Data\n",
    "\n",
    "Now provide your unique AoC session key, in order to download your input data. You can get this by:\n",
    "1. Logging into [Advent of Code](https://adventofcode.com/).\n",
    "1. From your browser, open Developer Tools. (In Chrome, you can do this by pressing F12.)\n",
    "1. Open the `Application` tab.\n",
    "1. Storage -> Cookies -> https://adventofcode.com\n",
    "1. Copy the value associated with the cookie called `session`.\n",
    "1. Once you've determiend your session key, I recommend you store it in a file called `.env`, in your `Advent-of-Code` folder, like this: \\\n",
    "`AOC_SESSION_COOKIE=536...your-own-session-key...658` \\\n",
    "This notebook will try to retrieve the key from that location.  If it is unable to retrieve the key, it will prompt you to enter your key in the cell below.\n",
    "\n",
    "![Finding the session cookie](https://aoc.just2good.co.uk/assets/images/aoc-cookie.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_envs_from_file() -> bool:\n",
    "    \"\"\" Look for .env files, read variables from it, and store as environment variables \"\"\"\n",
    "    potential_path = \".env\"\n",
    "    for _ in range(3):\n",
    "        logger.debug(\"Trying .env at %s\", os.path.realpath(potential_path))\n",
    "        if os.path.exists(potential_path):\n",
    "            logger.info(\"Using .env at %s\", os.path.realpath(potential_path))\n",
    "            load_dotenv(potential_path, override=True, verbose=True)\n",
    "            return True\n",
    "        \n",
    "        potential_path = os.path.join('..', potential_path)\n",
    "   \n",
    "    logger.warning(\"No .env file found.\")\n",
    "    return False\n",
    "\n",
    "get_envs_from_file() # read env variables from a .env file, if we can find one\n",
    "\n",
    "if os.getenv('AOC_SESSION_COOKIE'):\n",
    "    logger.info('Session cookie retrieved: %s...%s', os.environ['AOC_SESSION_COOKIE'][0:6], os.environ['AOC_SESSION_COOKIE'][-6:])\n",
    "else: # it's not in our environment variables, so we'll need to input the value\n",
    "    os.environ['AOC_SESSION_COOKIE'] = getpass('Enter AoC session key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9A24B4MSyhT"
   },
   "source": [
    "## Load Helpers and Useful Classes\n",
    "\n",
    "Now we load a bunch of helper functions and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPOsmENmYf8h",
    "tags": []
   },
   "source": [
    "### Locations\n",
    "\n",
    "Where any input and output files get stored.\n",
    "\n",
    "<img src=\"https://aoc.just2good.co.uk/assets/images/notebook-content-screenshot.png\" width=\"320\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VK27bcGiK0_M",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Paths and Locations\n",
    "#################################################################\n",
    "\n",
    "@dataclass\n",
    "class Locations:\n",
    "    \"\"\" Dataclass for storing various location properties \"\"\"\n",
    "    script_name: str\n",
    "    script_dir: Path\n",
    "    input_dir: Path\n",
    "    output_dir: Path\n",
    "    input_file: Path\n",
    "\n",
    "def get_locations(script_name, folder=\"\") -> Locations:\n",
    "    \"\"\" Set various paths, based on the location of the calling script. \"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    script_dir = Path(Path().resolve(), folder, script_name)\n",
    "    input_dir = Path(script_dir, \"input\")\n",
    "    output_dir = Path(script_dir, \"output\")\n",
    "    input_file = Path(input_dir, \"input.txt\")\n",
    "\n",
    "    return Locations(script_name, script_dir,\n",
    "                     input_dir,\n",
    "                     output_dir,\n",
    "                     input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14ISscMTadAO"
   },
   "source": [
    "### Retrieve the Input Data\n",
    "\n",
    "This works by using your unique session cookie to retrieve your input data. E.g. from a URL like:\n",
    "\n",
    "`https://adventofcode.com/2015/day/1/input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwP0r3BAaxjt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Retrieving input data\n",
    "##################################################################\n",
    "\n",
    "def write_puzzle_input_file(year: int, day, locations: Locations):\n",
    "    \"\"\" Use session key to obtain user's unique data for this year and day.\n",
    "    Only retrieve if the input file does not already exist.\n",
    "    Return True if successful.\n",
    "    Requires env: AOC_SESSION_COOKIE, which can be set from the .env.\n",
    "    \"\"\"\n",
    "    if os.path.exists(locations.input_file):\n",
    "        logger.debug(\"%s already exists\", os.path.basename(locations.input_file))\n",
    "        return os.path.basename(locations.input_file)\n",
    "\n",
    "    session_cookie = os.getenv('AOC_SESSION_COOKIE')\n",
    "    if not session_cookie:\n",
    "        raise ValueError(\"Could not retrieve session cookie.\")\n",
    "\n",
    "    logger.info('Session cookie retrieved: %s...%s', session_cookie[0:6], session_cookie[-6:])\n",
    "\n",
    "    # Create input folder, if it doesn't exist\n",
    "    if not locations.input_dir.exists():\n",
    "        locations.input_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url = f\"https://adventofcode.com/{year}/day/{day}/input\"\n",
    "    \n",
    "    # Don't think we need to set a user-agent\n",
    "    # headers = {\n",
    "    #     \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    # }\n",
    "    cookies = { \n",
    "        \"session\": session_cookie\n",
    "    }\n",
    "    response = requests.get(url, cookies=cookies, timeout=5)\n",
    "\n",
    "    data = \"\"\n",
    "    if response.status_code == 200:\n",
    "        data = response.text\n",
    "\n",
    "        with open(locations.input_file, 'w') as file:\n",
    "            logger.debug(\"Writing input file %s\", os.path.basename(locations.input_file))\n",
    "            file.write(data)\n",
    "            return data\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to retrieve input data.\\n\" +\n",
    "                         f\"HTTP response: {response.status_code}\\n\" +\n",
    "                         f\"{response.reason}: {response.content.decode('utf-8').strip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yakibhW73Rqi"
   },
   "source": [
    "### Testing\n",
    "\n",
    "A really simple function for testing that our solution produces the expected test output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6nbd6WMryWi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(test, answer):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        test: the answer given by our solution\n",
    "        answer: the expected answer, e.g. from instructions\n",
    "    \"\"\"\n",
    "    if test != answer:\n",
    "        raise AssertionError(f\"{test} != {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bcBQkP0a2zA"
   },
   "source": [
    "### Useful Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8sU4Ez_bBKl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# POINTS, VECTORS AND GRIDS\n",
    "#################################################################\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Point:\n",
    "    \"\"\" Class for storing a point x,y coordinate \"\"\"\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "    def __add__(self, other: Point):\n",
    "        return Point(self.x + other.x, self.y + other.y)\n",
    "\n",
    "    def __mul__(self, other: Point):\n",
    "        \"\"\" (x, y) * (a, b) = (xa, yb) \"\"\"\n",
    "        return Point(self.x * other.x, self.y * other.y)\n",
    "\n",
    "    def __sub__(self, other: Point):\n",
    "        return self + Point(-other.x, -other.y)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        # Arbitrary comparison logic\n",
    "        return (self.x, self.y) < (other.x, other.y)\n",
    "    \n",
    "    def yield_neighbours(self, include_diagonals=True, include_self=False):\n",
    "        \"\"\" Generator to yield neighbouring Points \"\"\"\n",
    "\n",
    "        deltas: list\n",
    "        if not include_diagonals:\n",
    "            deltas = [vector.value for vector in Vectors if abs(vector.value[0]) != abs(vector.value[1])]\n",
    "        else:\n",
    "            deltas = [vector.value for vector in Vectors]\n",
    "\n",
    "        if include_self:\n",
    "            deltas.append((0, 0))\n",
    "\n",
    "        for delta in deltas:\n",
    "            yield Point(self.x + delta[0], self.y + delta[1])\n",
    "\n",
    "    def neighbours(self, include_diagonals=True, include_self=False) -> list[Point]:\n",
    "        \"\"\" Return all the neighbours, with specified constraints.\n",
    "        They are returned clockwise, starting with N.\n",
    "        It wraps the generator with a list. \"\"\"\n",
    "        return list(self.yield_neighbours(include_diagonals, include_self))\n",
    "\n",
    "    def get_specific_neighbours(self, directions: list[Vectors]) -> list[Point]:\n",
    "        \"\"\" Get neighbours, given a specific list of allowed locations. \"\"\"\n",
    "        return [(self + Point(*vector.value)) for vector in list(directions)]\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan_distance(a_point: Point) -> int:\n",
    "        \"\"\" Return the Manhattan distance value of this vector \"\"\"\n",
    "        return sum(abs(coord) for coord in asdict(a_point).values())\n",
    "\n",
    "    def manhattan_distance_from(self, other: Point) -> int:\n",
    "        \"\"\" Manhattan distance between this Vector and another Vector \"\"\"\n",
    "        diff = self-other\n",
    "        return Point.manhattan_distance(diff)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"P({self.x},{self.y})\"\n",
    "\n",
    "class Vectors(Enum):\n",
    "    \"\"\" Enumeration of 8 directions.\n",
    "    Note: y axis increments in the North direction, i.e. N = (0, 1) \"\"\"\n",
    "    N = (0, -1)\n",
    "    NE = (1, -1)\n",
    "    E = (1, 0)\n",
    "    SE = (1, 1)\n",
    "    S = (0, 1)\n",
    "    SW = (-1, 1)\n",
    "    W = (-1, 0)\n",
    "    NW = (-1, -1)\n",
    "\n",
    "class VectorDicts():\n",
    "    \"\"\" Contains constants for Vectors \"\"\"\n",
    "    ARROWS = {\n",
    "        '^': Vectors.N.value,\n",
    "        '>': Vectors.E.value,\n",
    "        'v': Vectors.S.value,\n",
    "        '<': Vectors.W.value\n",
    "    }\n",
    "\n",
    "    DIRS = {\n",
    "        'U': Vectors.N.value,\n",
    "        'R': Vectors.E.value,\n",
    "        'D': Vectors.S.value,\n",
    "        'L': Vectors.W.value\n",
    "    }\n",
    "\n",
    "    NINE_BOX: dict[str, tuple[int, int]] = {\n",
    "        # x, y vector for adjacent locations\n",
    "        'tr': (1, 1),\n",
    "        'mr': (1, 0),\n",
    "        'br': (1, -1),\n",
    "        'bm': (0, -1),\n",
    "        'bl': (-1, -1),\n",
    "        'ml': (-1, 0),\n",
    "        'tl': (-1, 1),\n",
    "        'tm': (0, 1)\n",
    "    }\n",
    "\n",
    "class Grid():\n",
    "    \"\"\" 2D grid of point values. \"\"\"\n",
    "    def __init__(self, grid_array: list, animating: bool=False) -> None:\n",
    "        self.array = grid_array.copy()\n",
    "        self._width = len(self.array[0])\n",
    "        self._height = len(self.array)\n",
    "        \n",
    "        self.animating = animating\n",
    "        if self.animating:\n",
    "            self._plot_info = self._setup_fig()\n",
    "            self._frame_index = 0\n",
    "    \n",
    "    def _setup_fig(self):\n",
    "        my_dpi = 120\n",
    "        fig, axes = plt.subplots(figsize=(1024/my_dpi, 768/my_dpi), dpi=my_dpi, facecolor=\"black\") # set size in pixels\n",
    "\n",
    "        axes.get_xaxis().set_visible(False)\n",
    "        axes.get_yaxis().set_visible(False)\n",
    "        axes.set_aspect('equal') # set x and y to equal aspect\n",
    "        axes.set_facecolor('xkcd:orange')\n",
    "        \n",
    "        min_x, max_x = -0.5, self.width - 0.5\n",
    "        min_y, max_y = -0.5, self.height - 0.5\n",
    "        axes.set_xlim(min_x, max_x)\n",
    "        axes.set_ylim(max_y, min_y)\n",
    "\n",
    "        # dynamically compute the marker size\n",
    "        fig.canvas.draw()\n",
    "        mkr_size = ((axes.get_window_extent().width / (max_x-min_x) * (45/fig.dpi)) ** 2)\n",
    "        \n",
    "        return fig, axes, mkr_size\n",
    "\n",
    "    def animate_step(self, i):\n",
    "        \"\"\" Update the plot for the ith step in the animation.\n",
    "        Abstract implementation: add your logic when extending this class. \"\"\"\n",
    "        \n",
    "        # if self._frame_index < len(self.some_updating_attribute):\n",
    "        #     self._render_plot()\n",
    "        #     self._frame_index += 1\n",
    "        return []\n",
    "    \n",
    "    def create_animation(self, output_path='animation.mp4', fps=10):\n",
    "        \"\"\" Abstract implementation: add your logic when extending this class.\n",
    "        Creates the animation, by calling the animate_step() method to generate frames. \"\"\"\n",
    "        \n",
    "        self._plot_info = self._setup_fig()  # Set up the figure for plotting\n",
    "        fig, axes, mkr_size = self._plot_info\n",
    "        \n",
    "        # # Creating the animation\n",
    "        # anim = FuncAnimation(fig, self.animate_step, frames=len(self.some_updating_attribute), \n",
    "        #                      interval=1000/fps, blit=True)\n",
    "\n",
    "        # # Save the animation\n",
    "        # anim.save(output_path, writer='ffmpeg')\n",
    "    \n",
    "    def _render_plot(self):\n",
    "        \"\"\" Abstract implementation: add your rendering logic when extending this class. \"\"\"\n",
    "        fig, axes, mkr_size = self._plot_info\n",
    "        axes.clear() # clear for each frame\n",
    "        \n",
    "        # plot stuff - IT SHOULD DRAW WHATEVER IS RELEVANT FOR A GIVEN FRAME\n",
    "        # E.g.\n",
    "        # Only plot the path up to the current frame index\n",
    "        # for point, dirn in self.path_taken[:self._frame_index + 1]:\n",
    "    \n",
    "    def value_at_point(self, point: Point):\n",
    "        \"\"\" The value at this point \"\"\"\n",
    "        return self.array[point.y][point.x]\n",
    "\n",
    "    def set_value_at_point(self, point: Point, value):\n",
    "        if isinstance(self.array[0], str):\n",
    "            row = self.array[point.y]\n",
    "            self.array[point.y] = row[:point.x] + value + row[point.x + 1:]\n",
    "        else: # assume we have a replaceable type, like list[int]\n",
    "            self.array[point.y][point.x] = value\n",
    "\n",
    "    def valid_location(self, point: Point) -> bool:\n",
    "        \"\"\" Check if a location is within the grid \"\"\"\n",
    "        if (0 <= point.x < self._width and  0 <= point.y < self._height):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        \"\"\" Array width (cols) \"\"\"\n",
    "        return self._width\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        \"\"\" Array height (rows) \"\"\"\n",
    "        return self._height\n",
    "\n",
    "    def all_points(self) -> list[Point]:\n",
    "        points = [Point(x, y) for x in range(self.width) for y in range(self.height)]\n",
    "        return points\n",
    "\n",
    "    def rows_as_str(self):\n",
    "        \"\"\" Return the grid \"\"\"\n",
    "        return [\"\".join(str(char) for char in row) for row in self.array]\n",
    "\n",
    "    def cols_as_str(self):\n",
    "        \"\"\" Render columns as str. Returns: list of str \"\"\"\n",
    "        cols_list = list(zip(*self.array))\n",
    "        return [\"\".join(str(char) for char in col) for col in cols_list]\n",
    "\n",
    "    def transpose(self) -> Grid:\n",
    "        return Grid(list(zip(*self.array)))\n",
    "    \n",
    "    def flip_vertical(self) -> Grid:\n",
    "        \"\"\" Flip the array about the vertical axis of symmetry \"\"\"\n",
    "        return Grid([row[::-1] for row in self.array])\n",
    "    \n",
    "    def flip_horizontal(self) -> Grid:\n",
    "        \"\"\" Flip the array about the horizontal axis of symmetry \"\"\"\n",
    "        return Grid(self.array[::-1])\n",
    "    \n",
    "    def rotate_90(self, ccw=False) -> Grid:\n",
    "        \"\"\" Rotates 90 degrees CW. It works by transposing, then flipping. \"\"\"\n",
    "        if ccw:\n",
    "            flipped = self.flip_vertical()\n",
    "        else:\n",
    "            flipped = self.flip_horizontal()\n",
    "    \n",
    "        return flipped.transpose()\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Grid(size={self.width}*{self.height})\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"\\n\".join(\"\".join(map(str, row)) for row in self.array)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xNijk6xbCVa"
   },
   "source": [
    "### Useful Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DT5FSYliC9wp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# CONSOLE STUFF\n",
    "#################################################################\n",
    "\n",
    "def cls():\n",
    "    \"\"\" Clear console \"\"\"\n",
    "    os.system('cls' if os.name=='nt' else 'clear')\n",
    "\n",
    "#################################################################\n",
    "# USEFUL FUNCTIONS\n",
    "#################################################################\n",
    "\n",
    "def binary_search(target, low:int, high:int, func, *func_args, reverse_search=False):\n",
    "    \"\"\" Generic binary search function that takes a target to find,\n",
    "    low and high values to start with, and a function to run, plus its args.\n",
    "    Implicitly returns None if the search is exceeded. \"\"\"\n",
    "\n",
    "    res = None  # just set it to something that isn't the target\n",
    "    candidate = 0  # initialise; we'll set it to the mid point in a second\n",
    "\n",
    "    while low < high:  # search exceeded\n",
    "        candidate = int((low+high) // 2)  # pick mid-point of our low and high\n",
    "        res = func(candidate, *func_args) # run our function, whatever it is\n",
    "        logger.debug(\"%d -> %d\", candidate, res)\n",
    "        if res == target:\n",
    "            return candidate  # solution found\n",
    "\n",
    "        comp = operator.lt if not reverse_search else operator.gt\n",
    "        if comp(res, target):\n",
    "            low = candidate\n",
    "        else:\n",
    "            high = candidate\n",
    "\n",
    "def merge_intervals(intervals: list[list]) -> list[list]:\n",
    "    \"\"\" Takes intervals in the form [[a, b][c, d][d, e]...]\n",
    "    Intervals can overlap.  Compresses to minimum number of non-overlapping intervals. \"\"\"\n",
    "    intervals.sort()\n",
    "    stack = []\n",
    "    stack.append(intervals[0])\n",
    "\n",
    "    for interval in intervals[1:]:\n",
    "        # Check for overlapping interval\n",
    "        if stack[-1][0] <= interval[0] <= stack[-1][-1]:\n",
    "            stack[-1][-1] = max(stack[-1][-1], interval[-1])\n",
    "        else:\n",
    "            stack.append(interval)\n",
    "\n",
    "    return stack\n",
    "\n",
    "@cache\n",
    "def get_factors(num: int) -> set[int]:\n",
    "    \"\"\" Gets the factors for a given number. Returns a set[int] of factors.\n",
    "        # E.g. when num=8, factors will be 1, 2, 4, 8 \"\"\"\n",
    "    factors = set()\n",
    "\n",
    "    # Iterate from 1 to sqrt of 8,\n",
    "    # since a larger factor of num must be a multiple of a smaller factor already checked\n",
    "    for i in range(1, int(num**0.5) + 1):  # e.g. with num=8, this is range(1, 3)\n",
    "        if num % i == 0: # if it is a factor, then dividing num by it will yield no remainder\n",
    "            factors.add(i)  # e.g. 1, 2\n",
    "            factors.add(num//i)  # i.e. 8//1 = 8, 8//2 = 4\n",
    "\n",
    "    return factors\n",
    "\n",
    "def to_base_n(number: int, base: int):\n",
    "    \"\"\" Convert any integer number into a base-n string representation of that number.\n",
    "    E.g. to_base_n(38, 5) = 123\n",
    "\n",
    "    Args:\n",
    "        number (int): The number to convert\n",
    "        base (int): The base to apply\n",
    "\n",
    "    Returns:\n",
    "        [str]: The string representation of the number\n",
    "    \"\"\"\n",
    "    ret_str = \"\"\n",
    "    curr_num = number\n",
    "    while curr_num:\n",
    "        ret_str = str(curr_num % base) + ret_str\n",
    "        curr_num //= base\n",
    "\n",
    "    return ret_str if number > 0 else \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YtEtBnfNUKw"
   },
   "source": [
    "### Generic Initialisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbdA-geUNqAF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "FOLDER = \"aoc\"\n",
    "YEAR = 2023\n",
    "logger_identifier = \"aoc\" + str(YEAR)\n",
    "logger = retrieve_console_logger(logger_identifier)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y68fv1BebLsG"
   },
   "source": [
    "# Days\n",
    "\n",
    "Here you'll find a template to build a solution for a given day, and then the solutions for all days in this event.\n",
    "\n",
    "To copy the template day, select all the cells in the `Day n` template, add a new cell at the end, and then paste the cells there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FT4HLZLwevr"
   },
   "source": [
    "---\n",
    "## Day 1: Trebuchet?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VoMC3MaJ1I9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DAY = 1\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNvM0hOF0tBR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.INFO)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAh6_AWRbYPo"
   },
   "source": [
    "### Day 1 Part 1\n",
    "\n",
    "And we're off!!  Welcome to the first day of Advent of Code 2023!!\n",
    "\n",
    "Today was a troublesome start for me.  My Internet was out.  (Thanks, Virgin Media.) So, after unsuccessful restarts of the router and home network, I switched over to mobile hotspot.\n",
    "\n",
    "Part 1 is pretty trivial, as we've come to expect. You need to identify the first and last digits of each line of a string. Concatenating these two values gives you a two digit number, which the puzzle calls a _calibration value_. Then we just add them all together.\n",
    "\n",
    "**My Solution**\n",
    "\n",
    "- For each line, I simply loop through each char in the line, and use the `isdigit()` method to determine if it is a digit.\n",
    "- Then repeat, but this time, looping from the end using the Python construct `[::-1]` which just means: start from the end, and then step with increments of `-1`. I.e. move backwards.\n",
    "- Finally, concatenate the two digits (still as strings), to update a two digit number. Then convert it to an int.\n",
    "- Store all these ints in a list.  And at the end, return the sum of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = { k: v for v, k in \n",
    "             enumerate(\"one two three four five six seven eight nine\".split(), start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhWomZ6ewNi-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def solve(data: list[str], with_spelled_nums=False):\n",
    "    calibration_vals = []\n",
    "    for line in data:\n",
    "        logger.debug(line)\n",
    "        \n",
    "        first_posn = 1e6        \n",
    "        last_posn = -1\n",
    "        first = last = \"\"\n",
    "\n",
    "        for posn, char in enumerate(line): # read from start\n",
    "            if char.isdigit():\n",
    "                first_posn = posn\n",
    "                first = char\n",
    "                break\n",
    "            \n",
    "        for posn, char in enumerate(line[::-1]): # read from the end\n",
    "            if char.isdigit():\n",
    "                last_posn = len(line) - posn - 1 # remember, we're now counting from the end!!\n",
    "                last = char\n",
    "                break\n",
    "\n",
    "        if with_spelled_nums:\n",
    "            for num_word in num_words:\n",
    "                posn = line.find(num_word)\n",
    "                if 0 <= posn < first_posn:\n",
    "                    first_posn = posn\n",
    "                    first = str(num_words[num_word]) # map it back to int\n",
    "            \n",
    "                posn = line.rfind(num_word)\n",
    "                if posn > last_posn:\n",
    "                    last_posn = posn\n",
    "                    last = str(num_words[num_word]) # map to the int\n",
    "        \n",
    "        calibration_vals.append(int(first + last))\n",
    "    \n",
    "    return sum(calibration_vals)  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uw4xFe7R7jf0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"1abc2\", \"pqr3stu8vwx\", \"a1b2c3d4e5f\", \"treb7uchetabcdef\"]]\n",
    "sample_answers = [142]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "soln = solve(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6ARzwH_AHU0"
   },
   "source": [
    "### Day 1 Part 2\n",
    "\n",
    "For Day 1, this wasn't quite as trivial as I was expecting! Now we have to also find the positions of any \"spelled\" versions of the digits 0-9.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- Create a `dict` to store the spelled versions of 1-9, and map them to their respective int values.\n",
    "- Now, with each line, perform the same code as we did for Part 1 to find the first and last positions of the digit representation. \n",
    "  - But this time, store the positions found, as well as the values. I use the [`enumerate()`](https://aoc.just2good.co.uk/python/enumerate) to give me the current position of each char in my line.\n",
    "  - Be really careful when storing the position when counting from the end.  This tripped me up for a couple of minutes!!  When we're looping through chars from the end, backwards, we want to store the position in the string, not the current enumeration value. \n",
    "- Then, run another loop that looks for each spelled number in our dict of spelled numbers.\n",
    "  - To search for our current spelled number in our line from the start, using the `find()` method.\n",
    "  - To search for our current spelled number in our line from the end, using the `rfind()` method.\n",
    "  - Whenever we find a spelled number, check whether we found it at a position that is earlier / later (as required) than the digit we found before.\n",
    "  - Whenever I find such a spelled number, I convert the int value in the dict to a string, so that I can concatenate the string values, just as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcJdRHyK8TRY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"two1nine\", \n",
    "                  \"eightwothree\", \n",
    "                  \"abcone2threexyz\", \n",
    "                  \"xtwone3four\", \n",
    "                  \"4nineeightseven2\", \n",
    "                  \"zoneight234\", \n",
    "                  \"7pqrstsixteen\"]]\n",
    "sample_answers = [281]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input, with_spelled_nums=True), curr_ans) # test with sample data\n",
    "\n",
    "soln = solve(input_data, with_spelled_nums=True)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 2: Cube Conundrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"2\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 2 Part 1\n",
    "\n",
    "In each game, we have a bag containing some number of red, green and blue cubes.  The bag is sampled several times per game. Our input data shows these random samples for each game. E.g.\n",
    "\n",
    "```text\n",
    "Game 1: 3 blue, 4 red; 1 red, 2 green, 6 blue; 2 green\n",
    "Game 2: 1 blue, 2 green; 3 green, 4 blue, 1 red; 1 green, 1 blue\n",
    "Game 3: 8 green, 6 blue, 20 red; 5 blue, 4 red, 13 green; 5 green, 1 red\n",
    "Game 4: 1 green, 3 red, 6 blue; 3 green, 6 red; 3 green, 15 blue, 14 red\n",
    "Game 5: 6 red, 1 blue, 3 green; 2 blue, 1 red, 2 green\n",
    "```\n",
    "\n",
    "**Determine which games would have been possible if the bag had been loaded with only 12 red cubes, 13 green cubes, and 14 blue cubes. What is the sum of the IDs of those games?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- I create a CubeSample [class](https://aoc.just2good.co.uk/python/classes) to store each sample, i.e. the number of r, g, b cubes.\n",
    "- I create a Game class to store the game ID and all the samples for that game.\n",
    "- I parse the input with [regex](https://aoc.just2good.co.uk/python/regex). My approach was:\n",
    "  - Split the game line into the game part, and the samples part. Retrieving the game ID is trivial.\n",
    "  - For the samples, use a regex that looks for \"n colour\", and use a regex `finditer()` to find all matches for this.\n",
    "  - Create a [defaultdict](https://aoc.just2good.co.uk/python/defaultdict) that sets the initial values for r, g, b to 0.\n",
    "  - Then iterate over the matches from `finditer()``, and update the r, g, b as required.\n",
    "- Now I simply loop through each game. \n",
    "  - For each game, I loop through the samples. If any sample has more r, g, b than we're allowed, then this game is impossible.\n",
    "  - Build up a list of the games that are possible. Then sum up the IDs with a [comprehension](https://aoc.just2good.co.uk/python/comprehensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CubeSample:\n",
    "    \"\"\" A sample contains a number of red, blue, and green cubes \"\"\"\n",
    "    red: int=0\n",
    "    blue: int=0\n",
    "    green: int=0\n",
    "\n",
    "@dataclass\n",
    "class Game:\n",
    "    \"\"\" A game has an ID, and a random number of samples \"\"\"\n",
    "    id: int\n",
    "    samples: list[CubeSample]\n",
    "\n",
    "def parse_input(data) -> list[Game]:\n",
    "    game_pattern = re.compile(r\"Game (\\d+)\")\n",
    "    cubes_pattern = re.compile(r\"(\\d+) (\\w+)\") # E.g. \"3 blue\" \n",
    "    \n",
    "    games = []\n",
    "    for line in data:\n",
    "        game_part, samples_part = line.split(\":\")\n",
    "        game_id = int(game_pattern.findall(game_part)[0])\n",
    "        samples = samples_part.split(\";\")\n",
    "        \n",
    "        cube_samples = []\n",
    "        for sample in samples: # e.g. \" 3 blue, 4 red\"\n",
    "            matches = cubes_pattern.finditer(sample)\n",
    "            cube_counts = defaultdict(int) # reset cube counts for each sample\n",
    "            for match in matches:\n",
    "                cube_count, cube_colour = match.groups()\n",
    "                cube_counts[cube_colour] = int(cube_count)\n",
    "            \n",
    "            cube_samples.append(CubeSample(red=cube_counts[\"red\"], \n",
    "                                           blue=cube_counts[\"blue\"], \n",
    "                                           green=cube_counts[\"green\"]))\n",
    "        \n",
    "        games.append(Game(game_id, cube_samples))\n",
    "        \n",
    "    return games\n",
    "      \n",
    "def solve(games: list[Game]):\n",
    "    \"\"\" Return the sum of the IDs for games that are possible. \"\"\"\n",
    "    \n",
    "    allowed_red = 12\n",
    "    allowed_green = 13\n",
    "    allowed_blue = 14\n",
    "    \n",
    "    possible_games = []\n",
    "    for game in games:\n",
    "        possible = True\n",
    "        for game_sample in game.samples:\n",
    "            if (game_sample.red > allowed_red\n",
    "                    or game_sample.green > allowed_green\n",
    "                    or game_sample.blue > allowed_blue):\n",
    "                possible = False\n",
    "            \n",
    "        if possible:\n",
    "            possible_games.append(game)\n",
    "            \n",
    "    return sum(game.id for game in possible_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"Game 1: 3 blue, 4 red; 1 red, 2 green, 6 blue; 2 green\",\n",
    "                  \"Game 2: 1 blue, 2 green; 3 green, 4 blue, 1 red; 1 green, 1 blue\",\n",
    "                  \"Game 3: 8 green, 6 blue, 20 red; 5 blue, 4 red, 13 green; 5 green, 1 red\",\n",
    "                  \"Game 4: 1 green, 3 red, 6 blue; 3 green, 6 red; 3 green, 15 blue, 14 red\",\n",
    "                  \"Game 5: 6 red, 1 blue, 3 green; 2 blue, 1 red, 2 green\"]\n",
    "                ]\n",
    "sample_answers = [8]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    sample_games = parse_input(curr_input)\n",
    "    validate(solve(sample_games), curr_ans) # test with sample data\n",
    "\n",
    "games = parse_input(input_data)\n",
    "soln = solve(games)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 2 Part 2\n",
    "\n",
    "**For each game, find the minimum set of cubes that must have been present. What is the sum of the power of these sets?**\n",
    "\n",
    "Here, we need to look at all the samples for a given game, and determine the largest number of cubes shown of each colour, across the samples.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "Fortunately, since we already have our list of Games, this is now trivial to do. Simply iterate through the games, and for each game, iterate over all the samples. For each sample, determine if the number of any of r, g, b is greater than the biggest number of which we've found so far.\n",
    "\n",
    "Then, multiply the r, g, b to get the `power` of the game. Then sum up all the powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(games: list[Game]):\n",
    "    \"\"\" Return the sum of the powers of all the games \"\"\"\n",
    "    game_powers = []\n",
    "    for game in games:\n",
    "        max_blue = max_green = max_red = 0\n",
    "        for game_sample in game.samples:\n",
    "            max_blue = max(max_blue, game_sample.blue)\n",
    "            max_green = max(max_green, game_sample.green)\n",
    "            max_red = max(max_red, game_sample.red)\n",
    "     \n",
    "        # We're told that power = product of r, g, b   \n",
    "        game_powers.append(max_blue*max_green*max_red)\n",
    "    \n",
    "    return sum(game_powers)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"Game 1: 3 blue, 4 red; 1 red, 2 green, 6 blue; 2 green\",\n",
    "                  \"Game 2: 1 blue, 2 green; 3 green, 4 blue, 1 red; 1 green, 1 blue\",\n",
    "                  \"Game 3: 8 green, 6 blue, 20 red; 5 blue, 4 red, 13 green; 5 green, 1 red\",\n",
    "                  \"Game 4: 1 green, 3 red, 6 blue; 3 green, 6 red; 3 green, 15 blue, 14 red\",\n",
    "                  \"Game 5: 6 red, 1 blue, 3 green; 2 blue, 1 red, 2 green\"]\n",
    "                ]\n",
    "sample_answers = [2286]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    sample_games = parse_input(curr_input)\n",
    "    validate(solve_part2(sample_games), curr_ans) # test with sample data\n",
    "\n",
    "soln = solve_part2(games)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 3: Gear Ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"3\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 3 Part 1\n",
    "\n",
    "I'm finding AoC fairly tough this year. I wasn't expecting the early challenges to be this tricky.\n",
    "\n",
    "Anyhoo...\n",
    "\n",
    "We're given a 2D grid, called the _engine schematic_. That grid contains numbers, periods (which should be ignored), and symbols (anything else). We need to determine the _part numbers_, which we are told are any numbers adjacent to a symbol.\n",
    "\n",
    "It looks like this:\n",
    "\n",
    "```text\n",
    "467..114..\n",
    "...*......\n",
    "..35..633.\n",
    "......#...\n",
    "617*......\n",
    ".....+.58.\n",
    "..592.....\n",
    "......755.\n",
    "...$.*....\n",
    ".664.598..\n",
    "```\n",
    "\n",
    "**What is the sum of all of the part numbers in the engine schematic?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "We need to find any numbers that are adjacent to a symbol.\n",
    "\n",
    "- I get to reuse one of my [helper classes](https://aoc.just2good.co.uk/python/reusable_code).  Yay!  \n",
    "  - I'm going to reuse my `Point` class, which stores x, y coordinates, but also has the ability to return all of its adjacent neighbours.\n",
    "  - I'm going to reuse my `Grid` class, which already knows how to create a 2D grid, iterate through the points in the grid, get the values at any location, and determine if a specied point is in the grid.\n",
    "- I create a new [class](https://aoc.just2good.co.uk/python/classes) called `EngineGrid` by extending `Grid`.\n",
    "  - This class knows how to return all the points that are symbols.\n",
    "- To solve:\n",
    "  - First, get all the symbol locations. This is trivial.\n",
    "  - Get the value of this symbol, and store it alongside the location in a dictionary.\n",
    "  - Then, get all the neighbour locations for each symbol location. I.e. the 8 adjacent locations (including diagonals).\n",
    "  - Check if this neighbour is part of a part number range we've already found.  If it is, then we don't need to check this neighbour; we already know it's part of a part number.\n",
    "  - If the neighbour is a valid location, check if it is a digit. If it is, then this location is _in_ a part number. If so, use the method `get_part_number_continugous_range()` to determine the full set of points that make up this part number. It works by taking this location on this line of the grid, and walking backwards and fowards, until the value found is no longer a digit. We return the full set of contiguous digits as a part number range.\n",
    "  - Add this to the [set](https://aoc.just2good.co.uk/python/sets) of ranges for this symbol, i.e. in the `ranges_for_symbol` dict. Note, here I'm using a [defaultdict](https://aoc.just2good.co.uk/python/defaultdict), such that I can initialise each entry with an empty `set`, and then add to the set whenever we find a range.\n",
    "  - Also, add all the points from this range to a `set` called `all_range_locations_for_symbol`. We use this when checking each neighbour, to see if this neighbour is already part of a range associated with this symbol.\n",
    "  - Also, determine the part number for this range, using our `get_part_number_for_range()` method. Store the result in a `dict` called `part_range_to_num`, where the key is the range `tuple` itself.\n",
    "  - Finally, we can sum up all the part number values, by summing the values from the `dict` of `part_range_to_num`.\n",
    "\n",
    "### Day 3 Part 2\n",
    "\n",
    "Now we're told we need to find symbols that are _gears_, i.e. the symbols that are simply `*`. And we need to find all the gears that have exactly two adjacent part numbers. Where this is true, the product of the two part numbers is the _gear ratio_. Then we need to add up all the gear ratios.\n",
    "\n",
    "**What is the sum of all of the gear ratios in your engine schematic?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "This is a fairly trivial addition. We already have the `ranges_for_symbol` dictionary. So, in our loop that processes each symbol, we now:\n",
    "\n",
    "- Check if the symbol is a gear, i.e. `*`\n",
    "- If so, check how many ranges are associated with this symbol.\n",
    "- If there are exactly two ranges, then we need to determine the part number values of this ranges, and multiply them together to obtain our `gear_ratio` for this gear.\n",
    "- Finally, add up all the gear ratios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngineGrid(Grid):\n",
    "    def get_symbol_locations(self) -> list[Point]:\n",
    "        \"\"\" Return all locations that contain a symbol \"\"\"\n",
    "        symbol_locations = [point for point in self.all_points() if self._is_symbol(point)]\n",
    "        return symbol_locations\n",
    "    \n",
    "    def _is_symbol(self, point: Point) -> bool:\n",
    "        \"\"\" A symbol is anything that is not numeric, or not a period. \"\"\"\n",
    "        val = str(self.value_at_point(point))\n",
    "        if val.isdigit():\n",
    "            return False\n",
    "        \n",
    "        if val == \".\":\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def get_part_number_contiguous_range(self, point: Point) -> tuple[Point, ...]:\n",
    "        \"\"\" Given a point within a part number, we want to return the entire range or points that make up\n",
    "        that part number. Part numbers are always contiguous digits on a horizontal line.\n",
    "        Do this by walking backwards and forwards until we're no longer on a number. \"\"\"\n",
    "        line = self.array[point.y] # get the row this point is on\n",
    "    \n",
    "        # Find the start of the contiguous digits\n",
    "        start = point.x\n",
    "        while start > 0 and line[start - 1].isdigit():\n",
    "            start -= 1\n",
    "\n",
    "        # Find the end of the contiguous digits\n",
    "        end = point.x\n",
    "        while end < len(line) - 1 and line[end + 1].isdigit():\n",
    "            end += 1\n",
    "\n",
    "        # Return the contiguous locations that make up a part number\n",
    "        contiguous_locations = [Point(x, point.y) for x in range(start, end+1)]\n",
    "        return tuple(contiguous_locations)\n",
    "\n",
    "    def get_part_number_for_range(self, part_range: tuple[Point, ...]) -> int:\n",
    "        \"\"\" Given a set of points that make up a part number, return the part number they contain. \"\"\"\n",
    "        part_num = \"\"\n",
    "        for point in part_range:\n",
    "            part_num += self.value_at_point(point)\n",
    "            \n",
    "        return int(part_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(input_data) -> tuple[int, int]:\n",
    "    \"\"\" Part 1: determine the sum of all part numbers, where a part number is \n",
    "                a full set of continguous digits adjacent to a symbol. \n",
    "        Part 2: determine the gear ratios, where a gear ratio is the product of the part numbers\n",
    "                adjacent to a gear, for any gear that has exactly two adjacent part numbers. \"\"\"\n",
    "    points_and_symbols = [] # (point, value-for-point)\n",
    "    part_number_ranges = set() # all the points in a range. Use set so we don't double count ranges.\n",
    "    ranges_for_symbol = defaultdict(set) # { (point, value), {ranges} }\n",
    "    part_range_to_num = {} # so we can cache the part number corresponding to a range\n",
    "    gear_ratios = [] # a gear ratio is given by the product of its two adjacent part numbers\n",
    "    \n",
    "    engine = EngineGrid(input_data)\n",
    "    \n",
    "    # get the locations of symbols, e.g. * ?, but not .\n",
    "    symbol_locations = engine.get_symbol_locations()\n",
    "    for point in symbol_locations:\n",
    "        all_range_locations_for_symbol = set()\n",
    "        symbol_val = engine.value_at_point(point)\n",
    "        points_and_symbols.append((point, symbol_val))\n",
    "        \n",
    "        # get adjacent locations to this symbol\n",
    "        for neighbour in point.neighbours():\n",
    "            if neighbour in all_range_locations_for_symbol:\n",
    "                continue    # we don't care about neighbours in ranges we've already found\n",
    "            \n",
    "            if engine.valid_location(neighbour): # check it is in the grid\n",
    "                val = str(engine.value_at_point(neighbour))\n",
    "                if val.isdigit(): # this neighbour is a point in a part number\n",
    "                    range_for_locn = engine.get_part_number_contiguous_range(neighbour) # gets range as tuple\n",
    "                    part_range_to_num[range_for_locn] = engine.get_part_number_for_range(range_for_locn)\n",
    "                    ranges_for_symbol[(point, symbol_val)].add(range_for_locn) # add this range for this symbol\n",
    "                    all_range_locations_for_symbol.update(set(range_for_locn)) # add all locations from this range\n",
    "                \n",
    "        if symbol_val == \"*\": # if this is a gear\n",
    "            gear_ranges = ranges_for_symbol[(point, symbol_val)]\n",
    "            if gear_ranges and len(gear_ranges) == 2: # if this gear has exactly two ranges\n",
    "                gear_part_nums = [part_range_to_num[gear_range] for gear_range in gear_ranges]\n",
    "                gear_ratios.append(gear_part_nums[0]*gear_part_nums[1])\n",
    "                # if we had an arbitrary number of gear parts to multiply, we could do this instead...\n",
    "                # gear_ratios.append(reduce(operator.mul, gear_part_nums))\n",
    "                \n",
    "        part_number_ranges.update(ranges_for_symbol[(point, symbol_val)])\n",
    "        \n",
    "    return sum(part_range_to_num.values()), sum(gear_ratios)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"467..114..\",\n",
    "                  \"...*......\",\n",
    "                  \"..35..633.\",\n",
    "                  \"......#...\",\n",
    "                  \"617*......\",\n",
    "                  \".....+.58.\",\n",
    "                  \"..592.....\",\n",
    "                  \"......755.\",\n",
    "                  \"...$.*....\",\n",
    "                  \".664.598..\"]]\n",
    "sample_answers = [(4361, 467835)]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "part_num_sum, gear_ratios_sum = solve(input_data)\n",
    "logger.info(f\"Part 1 soln: {part_num_sum=}\")\n",
    "logger.info(f\"Part 2 soln: {gear_ratios_sum=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 4: Scratchcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"4\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.INFO)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 4 Part 1\n",
    "\n",
    "We have a pile of scratch cards. Our input data represets one card per row:\n",
    "\n",
    "```text\n",
    "Card 1: 41 48 83 86 17 | 83 86  6 31 17  9 48 53\n",
    "Card 2: 13 32 20 16 61 | 61 30 68 82 17 32 24 19\n",
    "Card 3:  1 21 53 59 44 | 69 82 63 72 16 21 14  1\n",
    "Card 4: 41 92 73 84 69 | 59 84 76 51 58  5 54 83\n",
    "Card 5: 87 83 26 28 32 | 88 30 70 12 93 22 82 36\n",
    "Card 6: 31 18 13 56 72 | 74 77 10 23 35 67 36 11\n",
    "```\n",
    "\n",
    "Each card has an `id`, then a set of `winning` numbers, then a set of `actual` numbers that were scratched. The score of each scratch card depends on the number of winning numbers we have matched. The first match gives us 1 point, and each additional match doubles the score.\n",
    "\n",
    "**How points are the pile of scratch cards worth, in total?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "This is pretty simple.\n",
    "\n",
    "- I create a `ScratchCard` [dataclass](https://aoc.just2good.co.uk/python/classes#dataclass) which:\n",
    "  - Stores the card ID.\n",
    "  - Stores the winning numbers, and the scratched numbers, both as sets.  I want to use sets because I will want to determine the intersection of these two sets, i.e. how many winning numbers were matched. [Set algebra](https://aoc.just2good.co.uk/python/sets) makes this super easy.\n",
    "  - Has a `score()` method which determines the score, according to the rules.\n",
    "- Then I parse the input data using [regex](https://aoc.just2good.co.uk/python/regex). Let me explain how \\\n",
    "  `Card\\s+(\\d+):\\s*((?:\\d+\\s*)+)(?: \\|\\s+)((?:\\d+\\s*)+)` works:\n",
    "  - `Card\\s+`: this matches \"Card\" followed by one or more spaces. The real input contains a variable number of spaces after `Card`. This part of the expression is not surrounded by brackets, so it is NOT returned as a captured group.\n",
    "  - `(\\d+)`: This is a capturing group that matches one or more digits. The `\\d` is a shorthand character class that matches any digit (0-9), and the `+` means one or more of the preceding element. This is how I capture the `id`.\n",
    "  - `\\s*`: This matches zero or more whitespace characters. The `*` means zero or more of the preceding element.\n",
    "  - `((?:\\d+\\s*)+)`: This is a capturing group containing a non-capturing group. The non-capturing group `(?:\\d+\\s*)` matches one or more digits followed by zero or more whitespace characters. The outer capturing group with the `+` at the end repeats this pattern one or more times. This is how I capture ALL of the `winning` numbers.\n",
    "  - `(?: \\|\\s+)`: This is another non-capturing group. It matches a literal space, followed by a vertical bar `|`, followed by one or more whitespace characters.\n",
    "  - `((?:\\d+\\s*)+)`: The same as before. This is how I capture ALL of the `actual` numbers.\n",
    "- I use this regex to parse each line:\n",
    "  - This gives me the `id`, a string containing all the `winning` numbers, and a string containing all the `actual` numbers.\n",
    "  - I split the `winning` and `actual` numbers at the space, using `split()`. This gives me a list of string values. Then I convert the string values to `int` using `map()`, and finally convert each `list` to a `set`.\n",
    "  - Now I can create an instance of `ScoreCard` from the `id` and the two sets.\n",
    "\n",
    "- I've added a `matches()` method to my `ScatchCard`. This method uses the _intersects_ operator (`&`) to determine which numbers are present in both the `winning` set and the `actual` set. See more on this [here](https://aoc.just2good.co.uk/python/sets).\n",
    "\n",
    "- Finally, just add up all the scores from each card using `sum` and a [list comprehension](https://aoc.just2good.co.uk/python/comprehensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ScratchCard:\n",
    "    \"\"\" A scratchcard has an ID, winning numbers, and (actual) numbers scratched. \"\"\"\n",
    "    id: int\n",
    "    winning: set[int]\n",
    "    actual: set[int]\n",
    "    \n",
    "    def matches(self) -> int:\n",
    "        \"\"\" Return the number of winning numbers we have matched. \"\"\"\n",
    "        return len(self.winning & self.actual)\n",
    "    \n",
    "    def score(self) -> int:\n",
    "        \"\"\" For every winning number, double the score. Examples scores...\n",
    "        0 matches -> 0\n",
    "        1 match   -> 1\n",
    "        2 matches -> 2\n",
    "        3 matches -> 4, etc \"\"\"\n",
    "        num_matches = self.matches()\n",
    "        return 2**(num_matches-1) if num_matches > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cards(data) -> list[ScratchCard]:\n",
    "    scratch_card_pattern = r\"Card\\s+(\\d+):\\s*((?:\\d+\\s*)+)(?: \\|\\s+)((?:\\d+\\s*)+)\"\n",
    "    scratch_card_matcher = re.compile(scratch_card_pattern)\n",
    "    scratch_cards = []\n",
    "    for line in data:\n",
    "        id, winning, actual = scratch_card_matcher.findall(line)[0]\n",
    "        \n",
    "        id = int(id)\n",
    "        winning = set(map(int, winning.split()))\n",
    "        actual = set(map(int, actual.split()))\n",
    "        scratch_cards.append(ScratchCard(id, winning, actual))\n",
    "        \n",
    "    return scratch_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(cards: list[ScratchCard]):  \n",
    "    return sum(card.score() for card in cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"Card 1: 41 48 83 86 17 | 83 86  6 31 17  9 48 53\",\n",
    "                  \"Card 2: 13 32 20 16 61 | 61 30 68 82 17 32 24 19\",\n",
    "                  \"Card 3:  1 21 53 59 44 | 69 82 63 72 16 21 14  1\",\n",
    "                  \"Card 4: 41 92 73 84 69 | 59 84 76 51 58  5 54 83\",\n",
    "                  \"Card 5: 87 83 26 28 32 | 88 30 70 12 93 22 82 36\",\n",
    "                  \"Card 6: 31 18 13 56 72 | 74 77 10 23 35 67 36 11\"]\n",
    "]\n",
    "sample_answers = [13]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    sample_cards = parse_cards(curr_input)  \n",
    "    validate(solve_part1(sample_cards), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "cards = parse_cards(input_data)\n",
    "soln = solve_part1(cards)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 4 Part 2\n",
    "\n",
    "Now we're told that there's no such thing as a _score_. Instead scratchcards only cause you to win more scratchcards equal to the number of winning numbers you have. You win copies of the scratchcards below the winning card equal to the number of matches.\n",
    "\n",
    "**How many total scratchcards do you end up with?**\n",
    "\n",
    "**My solution**\n",
    "\n",
    "I spent several minutes thinking about how to solve this problem, before writing any code.  At first, I was thinking about some sort of `while loop` that iterates through cards, and only exits when there are no more cards. I was thinking I could insert duplicate cards into the list as I iterate.\n",
    "\n",
    "But then I realised that I didn't need to do that.  I simply needed to keep a count of how many there are of each card ID. I can do that with a `dict`. So my strategy:\n",
    "\n",
    "- Create a `dict` that stores the number of cards for each card ID.\n",
    "- Loop through all the cards we have to initialise the dict. There will be one of each.\n",
    "- Then, loop through the cards again, in order. For each card:\n",
    "  - Get the number of matches.\n",
    "  - Use this number of matches to determine the successive cards that need to be duplicated.\n",
    "  - Increment the count of each of those successive cards, by the count of the card we're currently on. (Because it might not be 1.)\n",
    "\n",
    "Even though it took my tiny brain a little while to realise this approach, the actual code is pretty trivial.\n",
    "\n",
    "The solution is fast, and only takes a couple of milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(cards: list[ScratchCard]):\n",
    "    card_counts = { card.id: 1 for card in cards } # initialise count to 1 for each original card\n",
    "    \n",
    "    for card in cards:\n",
    "        count_this_card = card_counts[card.id]\n",
    "        for i in range(card.id+1, card.id+1+card.matches()):\n",
    "            card_counts[i] += count_this_card\n",
    "    \n",
    "    logger.debug(card_counts)\n",
    "    total_card_count = sum(card_counts.values())\n",
    "    return total_card_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"Card 1: 41 48 83 86 17 | 83 86  6 31 17  9 48 53\",\n",
    "                  \"Card 2: 13 32 20 16 61 | 61 30 68 82 17 32 24 19\",\n",
    "                  \"Card 3:  1 21 53 59 44 | 69 82 63 72 16 21 14  1\",\n",
    "                  \"Card 4: 41 92 73 84 69 | 59 84 76 51 58  5 54 83\",\n",
    "                  \"Card 5: 87 83 26 28 32 | 88 30 70 12 93 22 82 36\",\n",
    "                  \"Card 6: 31 18 13 56 72 | 74 77 10 23 35 67 36 11\"]\n",
    "]\n",
    "sample_answers = [30]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    sample_cards = parse_cards(curr_input)  \n",
    "    validate(solve_part2(sample_cards), curr_ans) # test with sample data\n",
    "\n",
    "soln = solve_part2(cards)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 5: If You Give A Seed A Fertilizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"5\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.INFO)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 5 Part 1\n",
    "\n",
    "We're told there are a number of seeds to be planted. Types of seeds require types of soil, types of soil require types of fertiliser, etc.\n",
    "\n",
    "Our input looks like this:\n",
    "\n",
    "```text\n",
    "seeds: 79 14 55 13\n",
    "\n",
    "seed-to-soil map:\n",
    "50 98 2\n",
    "52 50 48\n",
    "\n",
    "soil-to-fertilizer map:\n",
    "0 15 37\n",
    "37 52 2\n",
    "39 0 15\n",
    "\n",
    "... etc\n",
    "```\n",
    "\n",
    "It describes: \n",
    "\n",
    "- Seeds to be planted, as ints.\n",
    "- Maps which describe how to map from source number to destination number, in the the format: \n",
    "  - _destination-range-start_, _source-range-start_, _range-length_. \\\n",
    "    The source and destination ranges will be the same length.\n",
    "- We have the following maps:\n",
    "  - seed-to-soil\n",
    "  - soil-to-fertilizer\n",
    "  - fertilizer-to-water\n",
    "  - water-to-light\n",
    "  - light-to-temperature\n",
    "  - temperature-to-humidity\n",
    "  - humidity-to-location\n",
    "\n",
    "Unmapped source numbers map to the same destination number.\n",
    "\n",
    "The gardener wants to know the closest location that needs a seed. So:\n",
    "\n",
    "**What is the lowest location number that corresponds to any of the initial seed numbers?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "We can start with any _seed number_, process it through every map, and ultimately arrive at a _location number_.\n",
    "\n",
    "- We need a rules engine that takes a source value and determines the target value for that map.\n",
    "- I've done this by creating a class called `GardinerMap`.\n",
    "- Each instance of `GardinerMap` contains the source and target ranges for that map type.\n",
    "- The `GardinerMap`:\n",
    "  - Stores all source ranges in that map as a list of tuples, with each tuple being `(source start, range length)`.\n",
    "  - Stores all destination ranges in that map as a list of tuples, with each tuple being `(destination start, range length)`.\n",
    "  - All the actual mapping is done in the method `get_target()`. It iterates through each range, and determines if our input value is part of that range. If it is, we apply the required _delta_, which is the difference between the destination range start and the source range start. Now we've finished mapping, so we return the mapped value.\n",
    "\n",
    "We need to start by passing the seed value to `get_target()` of our first `GardinerMap`. This returns a new value.  We then take this value and pass it into `get_target()` of the second `GardinerMap`.  And so on, until we've done this for every `GardinerMap`.\n",
    "\n",
    "When we're done with each `GardinerMap` we end up with a collection of _location_ numbers. We just need to return the smallest one.\n",
    "\n",
    "Conclusion: Part 1... Not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GardinerMap():\n",
    "    source_type: str\n",
    "    dest_type: str\n",
    "    \n",
    "    src_ranges: list[tuple[int,int]] = field(default_factory=list) # [(src_start, length), (src_start, length), ... ]\n",
    "    dest_ranges: list[tuple[int,int]] = field(default_factory=list) # [(dest_start, length), (dest_start, length), ... ]\n",
    "    \n",
    "    def add_range(self, src_start: int, dest_start: int, range_length: int):\n",
    "        \"\"\" Add a range which contains src start, dest start, and the length of the range \"\"\"\n",
    "        self.src_ranges.append((src_start, range_length)) \n",
    "        self.dest_ranges.append((dest_start, range_length))\n",
    "        self._sort_ranges()\n",
    "        \n",
    "    def _sort_ranges(self):\n",
    "        \"\"\" Sort the range into ascending numeric, based on source range start values \"\"\"\n",
    "        \n",
    "        # Sort src_ranges and get the order of indices\n",
    "        index_order = sorted(range(len(self.src_ranges)), key=lambda i: self.src_ranges[i][0])\n",
    "\n",
    "        # Now sort both ranges\n",
    "        self.src_ranges = [self.src_ranges[i] for i in index_order]\n",
    "        self.dest_ranges = [self.dest_ranges[i] for i in index_order]\n",
    "\n",
    "    def get_target(self, src_val: int):\n",
    "        \"\"\" Map a source value to a target value \"\"\"\n",
    "        target = src_val # if our source isn't in a range, then return the same value\n",
    "        \n",
    "        for i, curr_range in enumerate(self.src_ranges):\n",
    "            src_start = curr_range[0]\n",
    "            src_end = curr_range[0] + curr_range[1] # start+length; exclusive end\n",
    "            if src_start <= src_val < src_end: # if our source is in a range, then apply the shift\n",
    "                target = src_val - src_start + self.dest_ranges[i][0]\n",
    "                break # we've mapped the value, so no more ranges need to be checked\n",
    "        \n",
    "        return target\n",
    "\n",
    "    # For Part 2\n",
    "    def map_intervals(self, src_intervals: list[tuple[int, int]]) -> list[tuple[int, int]]:\n",
    "        \"\"\" \n",
    "        Take input ranges and return output ranges.\n",
    "        - src_ranges: [(rng1_start, rng1_end), (rng2_start, rng2_end), ... ] \n",
    "        \"\"\"\n",
    "        new_intervals = []\n",
    "        \n",
    "        # Iterate through the ranges, just as we did when mapping a single seed\n",
    "        for i, curr_range in enumerate(self.src_ranges): \n",
    "            src_start = curr_range[0]\n",
    "            src_end = curr_range[0] + curr_range[1] # exclusive end\n",
    "            dest = self.dest_ranges[i][0]\n",
    "\n",
    "            temp_intervals = []\n",
    "\n",
    "            while src_intervals: # process the current interval       \n",
    "                (int_start, int_end) = src_intervals.pop()\n",
    "                \n",
    "                # Split the interval using the ranges in our map\n",
    "                #### Scenario 1: ####\n",
    "                # [int_start                                  int_end]\n",
    "                #            [src_start      src_end]\n",
    "                # [left     ][mid                   ][right          ]\n",
    "                #\n",
    "                #### Scenario 2: ####\n",
    "                #                [int_start         int_end]\n",
    "                #   [src_start     src_end]\n",
    "                #   [n/a        ][mid     ][right          ]\n",
    "                left = (int_start, min(int_end, src_start))\n",
    "                mid = (max(int_start, src_start), min(src_end, int_end))\n",
    "                right = (max(src_end, int_start), int_end)\n",
    "                \n",
    "                if left[1] > left[0]: # if left has +ve length, then scenario 1, else scenario 2\n",
    "                    temp_intervals.append(left) # pass on the interval unchanged\n",
    "                if mid[1] > mid[0]: # if mid has +ve length, then we need to apply the shift to this interval\n",
    "                    # furthermore, once mapped, we know this interval wont appear in another range\n",
    "                    new_intervals.append((mid[0]-src_start+dest, mid[1]-src_start+dest))\n",
    "                if right[1] > right[0]:  # if right has +ve length\n",
    "                    temp_intervals.append(right) # pass on the interval unchanged\n",
    "            \n",
    "            src_intervals = temp_intervals\n",
    "                    \n",
    "        return new_intervals + src_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data: str) -> tuple[list[int], list[GardinerMap]]:\n",
    "    \"\"\" Parse input data, and convert to:\n",
    "    - seeds: list of int\n",
    "    - GardinerMap instances: list of GardinerMap \n",
    "    \"\"\"\n",
    "    \n",
    "    seeds = []\n",
    "    maps = [] # Store our GardinerMap instances\n",
    "    \n",
    "    # split into blocks. Store the first in seeds_line, and the rest in *blocks\n",
    "    seeds_line, *blocks = data.split(\"\\n\\n\") \n",
    "    assert seeds_line.startswith(\"seeds\"), \"First line contains the seeds values\"\n",
    "    _, seeds_part = seeds_line.split(\":\")\n",
    "    seeds = [int(x) for x in seeds_part.split()]\n",
    "    \n",
    "    for block in blocks:\n",
    "        block_header, *block_record = block.splitlines() # Split the block into a header row and remaining rows\n",
    "        map_src_type, _, map_dest_type = block_header.split()[0].split(\"-\")\n",
    "        map = GardinerMap(map_src_type, map_dest_type) # initialise our GardinerMap\n",
    "        for line in block_record:\n",
    "            assert line[0].isdigit(), \"Line must start with numbers\"\n",
    "            dest_start, src_start, interval_len = [int(x) for x in line.split()]\n",
    "            map.add_range(src_start=src_start, dest_start=dest_start, range_length=interval_len)\n",
    "        \n",
    "        maps.append(map)\n",
    "            \n",
    "    return seeds, maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data):\n",
    "    seeds, maps = parse_data(data)\n",
    "    location_map = {}\n",
    "    \n",
    "    logger.debug(f\"{seeds=}\")\n",
    "    for current_map in maps:\n",
    "        logger.debug(f\"{current_map=}\")\n",
    "        \n",
    "    for seed in seeds:\n",
    "        current_val = seed\n",
    "        for current_map in maps:\n",
    "            current_val = current_map.get_target(current_val)\n",
    "        \n",
    "        location_map[seed] = current_val\n",
    "    \n",
    "    logger.debug(f\"Seeds->locations: {location_map}\")\n",
    "    return min(location_map.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [\"\"\"seeds: 79 14 55 13\n",
    "\n",
    "seed-to-soil map:\n",
    "50 98 2\n",
    "52 50 48\n",
    "\n",
    "soil-to-fertilizer map:\n",
    "0 15 37\n",
    "37 52 2\n",
    "39 0 15\n",
    "\n",
    "fertilizer-to-water map:\n",
    "49 53 8\n",
    "0 11 42\n",
    "42 0 7\n",
    "57 7 4\n",
    "\n",
    "water-to-light map:\n",
    "88 18 7\n",
    "18 25 70\n",
    "\n",
    "light-to-temperature map:\n",
    "45 77 23\n",
    "81 45 19\n",
    "68 64 13\n",
    "\n",
    "temperature-to-humidity map:\n",
    "0 69 1\n",
    "1 0 69\n",
    "\n",
    "humidity-to-location map:\n",
    "60 56 37\n",
    "56 93 4\"\"\"]\n",
    "sample_answers = [35]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "soln = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Day 5 Part 2\n",
    "\n",
    "_\"The values on the initial `seeds:` line come in pairs. Within each pair, the first value is the start of the range and the second value is the length of the range.\"_\n",
    "\n",
    "Uh oh, the values on the seeds line now describes ranges of seed numbers. In the real input, these numbers are huge. We're going to have way too many seeds to iterate over each seed.\n",
    "\n",
    "Instead of mapping a source seed to a target value, we need to map the entire interval. By the way... **I hate intervals.**\n",
    "\n",
    "I was struggling to get my head around this problem, so I created a sketch:\n",
    "\n",
    "![Fertiliser range mappings](https://aoc.just2good.co.uk/assets/images/fertiliser.png)\n",
    "\n",
    "Things to note:\n",
    "\n",
    "- We start with intervals that represent our seeds.\n",
    "- In each map, any given target range will always be the same length as the source range.\n",
    "- As we propagate our seed intervals through each map, the following rules will apply:\n",
    "  - If the interval sits outside a mapped range, the target interval will be the same.\n",
    "  - If the interval sits inside a mapped range, the target interval will be shifted by the appropriate delta.\n",
    "  - If the interval is both within and outside a mapped range, then we will need to split the interval. The part that is in the range will be moved by the appropriate delta.  That part that is out of the range will stay the same.\n",
    "  - An interval might span multiple ranges. In this scenario, the interval must be split accordingly which will create more intervals.\n",
    "  - Although it is possible to get intervals that are adjacent, a given source interval will never create overlapping intervals. Phew!\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- We can't map individual seed values, as there's just too many. Instead, we can map the intervals of seed values, and we only care about any boundaries. I.e. at the beginning and end of any mapping range, at the beginning and end of any seeds interval, and at any intersections.\n",
    "- I added a `map_intervals()` method to my `GardinerMap` class. The job of this method is to map the supplied intervals into a new set of intervals. It works like this:\n",
    "  - Take all the intervals supplied (which will initally be the seed intervals) as an input list.\n",
    "  - Then iterate through our mapping ranges, as we did before.\n",
    "  - For each range, determine the source start, source end, and destination start values, as we did before.\n",
    "  - Initialise `temp_intervals`. We use this to store intervals that we still need to map.\n",
    "  - Now, loop over each interval supplied as input. With each loop iteration, `pop` the current interval off the list.\n",
    "  - For each interval, determine the boundary conditions that make up the left, middle, and right parts of the interval, after intersecting with the current range. There may be no intersection, in which case only the `left` or `right` portion will be available.\n",
    "  - If we have a `left` or `right` portion, then these are not intersecting with the current range.  So add these to our `temp_intervals`, and we can test them with the next range.\n",
    "  - If we have a `mid` portion, then this portion is intersecting with the current range. If so, we need to _shift_ the range to obtain the new range, using exactly the same logic as we used for shifting individual values in Part 1. Note: once we've mapped an interval portion like this, that portion will not be mapped again by this GardinerMap.\n",
    "  - Now, at the end of the loop for this interval, continue to the next loop iteration, which will process the next interval with this range.\n",
    "  - Once we've processed all intervals for this range, re-populate `src_intervals` using the current intervals stored in `temp_intervals`. This allows us to test these intervals with the next range.\n",
    "  - Any intervals that are never mapped with a range will instead be passed on, as is.\n",
    "\n",
    "Back in our `solve_part2()` method, we iterate over all the `GardinerMap` instances, just as we did with Part 1. But this time, we're passing in the seed intervals into the `map_intervals` method of the first `GardinerMap`. This returns a new list of intervals, which we pass into the next `GardinerMap`. And so on.\n",
    "\n",
    "Eventually, we've left with the intervals generated by the _humidity-to-location_ map. This is a list of tuples, with each tuple containin the start and end values for that interval. So it's a trival matter to get the `min` of all the `start` values.\n",
    "\n",
    "And that's it!\n",
    "\n",
    "**That was a brutal day 5!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data):\n",
    "    seeds, maps = parse_data(data)\n",
    "    \n",
    "    # convert seeds to intervals, of format [(start, end), ...] where end is exclusive\n",
    "    # let's call them intervals, to avoid confusion with the ranges we stored in our GardinerMap\n",
    "    seed_intervals = [(seeds[i], seeds[i]+seeds[i+1]) for i in range(0, len(seeds), 2)]\n",
    "    logger.debug(f\"{seed_intervals=}\")\n",
    "    \n",
    "    current_intervals = seed_intervals\n",
    "    for current_map in maps:\n",
    "        current_intervals = current_map.map_intervals(current_intervals)\n",
    "        logger.debug(f\"Mapping to {current_map.dest_type}: {current_intervals}\")\n",
    "        \n",
    "    return min(start for start, _ in current_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [46]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "soln = solve_part2(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 6: Wait For It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"6\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 6 Part 1\n",
    "\n",
    "We're racing toy boats. There is a button that charges the boat; releasing the button sends off the boat. Charging causes the boat to go faster, but charging time counts towards overall race time.\n",
    "\n",
    "Our input shows multiple races as columns. Races have allotted durations, in ms. The record (best) distance for each race is recorded in mm. Our example has three races:\n",
    "\n",
    "```text\n",
    "Time:      7  15   30\n",
    "Distance:  9  40  200\n",
    "```\n",
    "\n",
    "Our boat starts at 0mm/ms. Charging of 1ms increases speed by 1mm/ms.\n",
    "\n",
    "**Determine the number of ways you could beat the record in each race. What do you get if you multiply these numbers together?**\n",
    "\n",
    "This one is pretty trivial. (Thank god. I needed a better day after yesterday!!)\n",
    "\n",
    "- Read in the race durations and distances from our input.\n",
    "- For each race, try hold times from 1 through to duration-1. (We won't go anywhere if we hold the button for 0ms, or for the total duration of the race.)\n",
    "- The distance we travel is given by: `d = (race_duration - hold_time) * hold_time`, since `hold_time` gives us our speed.\n",
    "- Every time our distance `d` is greater than the record distances from our input, store it as a win.\n",
    "- I decided to store each win as a dict of `{ hold_time, distance }` because I thought I might need it later. (I didn't!)\n",
    "- The length of this dict gives us the number of ways we won that particular race. So, multiply these dict lengths together to get the answer required for Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data):\n",
    "    durations = [int(x) for x in data[0].split(\":\")[1].split()]\n",
    "    distances = [int(x) for x in data[1].split(\":\")[1].split()]\n",
    "    logger.debug(f\"{durations=}\")\n",
    "    logger.debug(f\"{distances=}\")\n",
    "    \n",
    "    wins = defaultdict(dict) # { race_num: { hold_time: distance, hold_time: distance, ...} }\n",
    "    for i, duration in enumerate(durations): # e.g. 0, 7\n",
    "        for hold_time in range(1, duration): # e.g. 0-7\n",
    "            # d = (t - h) * h\n",
    "            d = (duration - hold_time) * hold_time\n",
    "            if d > distances[i]: # did we win?\n",
    "                wins[i][hold_time] = d\n",
    "    \n",
    "    ways_to_win_product = 1\n",
    "    for race_num, my_results in wins.items():\n",
    "        logger.debug(f\"{race_num=}, ways to win={len(my_results)}\")\n",
    "        ways_to_win_product *= len(my_results)\n",
    "        \n",
    "    return ways_to_win_product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"Time:      7  15   30\", \n",
    "                  \"Distance:  9  40  200\"]\n",
    "]\n",
    "sample_answers = [288]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed.\")\n",
    "\n",
    "soln = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 6 Part 2\n",
    "\n",
    "Uh oh, there's only one race and the spaces should be ignored. So we reinterpret the input like this:\n",
    "\n",
    "```text\n",
    "Time:      71530\n",
    "Distance:  940200\n",
    "```\n",
    "\n",
    "The input data is much larger. My race durations is millions of ms. So, my thought process was... _\"I don't think my Part 1 solution is going to scale. I'm going to need to do something smarter. But let's just try it and see.\"_\n",
    "\n",
    "**Part 1 Solution is Good Enough!!**\n",
    "\n",
    "I adapted Part 1 to parse the input as specified, and then removed the outer loop, since there's only one race. I also added a bit of optimisation to stop trying hold times if we previously won, but now we're not winning. This is because once the hold time stops producing wins, adding more time to the hold time will NEVER win again.\n",
    "\n",
    "And then I ran it... And it ran in under 3s!! That was a big surprise to me. Phew, nothing clever required!!\n",
    "\n",
    "**The Smart Way?**\n",
    "\n",
    "If we expand the formula for distance that we used in Part 1, we can see it is a quadratic problem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d &= (t - h) \\times h \\\\\n",
    "d &= th - h^2 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We can rearrange to find the roots for $h$:\n",
    "\n",
    "$$\n",
    "h^2 - th + d = 0\n",
    "$$\n",
    "\n",
    "Remember the quadratic formula?\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{For: } ah^2 + bh + c &= 0 \\\\\n",
    "\\notag \\\\\n",
    "h &= \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So we can use this to obtain the two roots, i.e. the hold times where our distance is equal to the record distance. We're given distance $d$ and one duration $t$ in our input. (And we know $a$ is `1`.) Using the sample data we were given:\n",
    "\n",
    "$$\n",
    "h^2 - 71530h + 940200 = 0 \\\\\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "b &= -71530 \\\\\n",
    "c &= 940200\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Substituting into the quadratic equation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h &= \\frac{71530 \\pm \\sqrt{-71530^2 - 4 \\times 940200}}{2} \\\\\n",
    "\\notag \\\\\n",
    "  &= \\frac{71530 \\pm 71503.71}{2} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So our two answers are approximately 13.15 and 71516.85. We need to find all the integer values between this range. So our answer is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "ans &= 71516 - 13 \\\\\n",
    "    &= 71503\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Which is the right answer!  So now, I'll code this as a solution!\n",
    "\n",
    "If we plot this quadratic, we can see the range of hold times for which we can beat the record:\n",
    "\n",
    "![Boat race quadratic plot](https://aoc.just2good.co.uk/assets/images/boat_race_quadratic.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve Part 2 the Naive Way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data):\n",
    "    _, durations_part = data[0].split(\":\")\n",
    "    _, distances_part = data[1].split(\":\")\n",
    "    race_duration = int(\"\".join(x for x in durations_part.split()))\n",
    "    distance = int(\"\".join(x for x in distances_part.split()))\n",
    "    logger.debug(f\"{race_duration=}, {distance=}\")\n",
    "    \n",
    "    wins = []\n",
    "    for hold_time in range(1, race_duration):\n",
    "        d = (race_duration - hold_time) * hold_time\n",
    "        if d > distance: # have we won against the record?\n",
    "            wins.append(d)\n",
    "        else: # we lost\n",
    "            if wins: # but we won before\n",
    "                break # so holding the button for longer will always lose.\n",
    "    \n",
    "    return len(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [71503]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed.\")\n",
    "\n",
    "soln = solve_part2(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve Part 2 as a Quadratic\n",
    "\n",
    "Two approaches...\n",
    "\n",
    "1. Use the quadratic formula.\n",
    "1. Use SymPy to calculate the roots of our quadratic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2_quadratic(data):\n",
    "    \"\"\" d = (t - h) * h\n",
    "          = th - h^2\n",
    "          \n",
    "        Where d is the distance to beat, t is the race duration, and h is the hold time.\n",
    "          \n",
    "        Therefore:\n",
    "        h^2 - th + d = 0\n",
    "    \"\"\"\n",
    "    race_duration = int(\"\".join(x for x in data[0].split(\":\")[1].split()))\n",
    "    distance = int(\"\".join(x for x in data[1].split(\":\")[1].split()))\n",
    "    logger.debug(f\"{race_duration=}, {distance=}\")\n",
    "    \n",
    "    # solve using quadratic        \n",
    "    discriminant = (-race_duration)**2 - (4 * distance) # bottom of our curve\n",
    "    h1 = int((-race_duration + math.sqrt(discriminant)) / 2) # max value\n",
    "    h2 = int((-race_duration - math.sqrt(discriminant)) / 2) # min value\n",
    "    \n",
    "    return abs(h1-h2)\n",
    "  \n",
    "def solve_part2_sympy_quadratic(data):\n",
    "    \"\"\" h^2 - th + d = 0 \"\"\"\n",
    "    race_duration = int(\"\".join(x for x in data[0].split(\":\")[1].split()))\n",
    "    distance = int(\"\".join(x for x in data[1].split(\":\")[1].split()))\n",
    "    logger.debug(f\"{race_duration=}, {distance=}\")  \n",
    "\n",
    "    # solve using quadratic with SymPy       \n",
    "    h, t = sympy.symbols(\"h t\", real=True)\n",
    "    expr = h**2 - race_duration*h + distance\n",
    "    equation = sympy.Eq(expr, 0)\n",
    "    solutions = sympy.solve(equation, h, dict=True)\n",
    "    answers = [solution[h].evalf() for solution in solutions] # there should be two\n",
    "    logger.debug(f\"Intersections={answers}\")\n",
    "    \n",
    "    # plot expr, with x axis across specified range\n",
    "    p = sympyplot(expr, (h, 0 - (race_duration * 0.1), 1.1* race_duration), \n",
    "             line_color='red', xlabel=\"Hold time\", ylabel=\"y\", show=False)\n",
    "    p[0].label = 'Equation: h^2 - {}h + {}'.format(race_duration, distance)\n",
    "    p.legend = True\n",
    "    p.show()\n",
    "    \n",
    "    return abs(int(answers[1])-int(answers[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [71503]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2_sympy_quadratic(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed.\")\n",
    "\n",
    "soln = solve_part2_quadratic(input_data)\n",
    "logger.info(f\"Part 2 soln with quadratic formula={soln}\")\n",
    "\n",
    "soln = solve_part2_sympy_quadratic(input_data)\n",
    "logger.info(f\"Part 2 soln with SymPy quadratic formula={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 7: Camel Cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"7\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 7 Part 1\n",
    "\n",
    "_\"Camel Cards is sort of similar to poker except it's designed to be easier to play while riding a camel.\"_\n",
    "\n",
    "Brilliant. I love these stories!!\n",
    "\n",
    "Anyway, we're paying a poker variant. Our input contains a list of five-card hands, along with bid amounts, e.g.\n",
    "\n",
    "```text\n",
    "32T3K 765\n",
    "T55J5 684\n",
    "KK677 28\n",
    "KTJJT 220\n",
    "QQQJA 483\n",
    "```\n",
    "\n",
    "**Find the rank of every hand in your set. What are the total winnings?**\n",
    "\n",
    "Basically, we're being asked to sort the hands from weakest to strongest.  Weakest is ranking 1, second weakest is rank 2, etc. Then, we determine winnings by multiplying rank by bid amount for each hand, and then adding them together to get total winnings.\n",
    "\n",
    "Rules for determining hand strengh:\n",
    "\n",
    "- Usual poker hand strengths with straights and flushes removed, i.e.\n",
    "  \n",
    "  1. 5-of-a-kind (\"FIVE\")\n",
    "  1. 4-of-a-kind (\"FOUR\")\n",
    "  1. full-house (\"FH\")\n",
    "  1. 3-of-a-kind (\"THREE\")\n",
    "  1. two pair (\"TP\")\n",
    "  1. one pair (\"OP\")\n",
    "  1. high card (\"HC\")\n",
    "\n",
    "- But unlike normal poker, hand strength tie-breakers are achieved by comparing the successive card strengths of cards in the hand, going from first to last.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- Create a `Hand` class:\n",
    "  - In this class, create a dict class attribute to store individual `card_scores`. I've assigned integer values so we can compare card strength.\n",
    "  - Create a dict class attribute to store `HAND_TYPE`. Again, I've assigned integer values so that we can compare hands.\n",
    "  - Initialise with a string that represents our five cards.\n",
    "  - I'm using the Python `collections.Counter` class to help me determine the `HAND_TYPE`. The `Counter` class counts members of any collection passed to it, including strings. It stores the counts as a dict, in the form `{character: count}`. Then we can use `most_common()` to convert this dict into an ordered list of tuples, ordered by count.\n",
    "  - From here, it's trivial to determine the hand type.  For example, if `most_common()` count is 5, then we have five-of-a-kind.  If it's 4, then we have four-of-a-kind, and so on. Once we've determined the hand type, store it in the `self._hand_type` instance variable.\n",
    "  - Lastly, I need to be able compare one `Hand` to another, according to the rules above. In Python, we can do this by implementing the `__lt__()` (_less than_) method. Here, my `__lt__()` works by first comparing the values of the hand types. If the two hands have the same type, it then compares the cards in each hand, from left to right. Note that I'm using the cool [zip](https://aoc.just2good.co.uk/python/zip) function to combine the two hands into a single iterable of tuples. This is a cool way to compare _items from thing1_ to _items from thing2_, where the two _things_ are the same size.\n",
    "\n",
    "- Now I read the input. For each line, I create a new `Hand` instance, and bid integer. I store these two values as a tuple, and add it to a list of `hands_and_bids`.\n",
    "- Then I sort the hands.  This is easy to do with `sorted()`, because of the work we already did in the `Hand` class. But because I want to sort the tuples of `(hand, bid)`, not just the hands, I need to tell the `sorted()` function that it should sort based on the first item in each tuple. \n",
    "  - This is easily done with a [lambda function](https://aoc.just2good.co.uk/python/functions#lambda-functions): \\\n",
    "  `hands_and_bids = sorted(hands_and_bids, key=lambda x: x[0])`\n",
    "  - The `key` needs to be a function. Here, that function is simply returning the first item in the tuple.\n",
    "\n",
    "- Once sorted, it's easy to loop through my hands from weakest to strongest, and multiply the rank by the bid amount.\n",
    "\n",
    "Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 7 Part 2\n",
    "\n",
    "Now `J` is a Joker. \n",
    "\n",
    "- It has the lowest score of any card, but it acts a wildcard.\n",
    "- When determine hand type, `J` becomes whatever results in the strongest hand type.\n",
    "- But when breaking ties, `J` is always considered as `J`, with low value.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- I tweaked my `Hand` class so that we can pass in an optional `joker` boolean attribute upon initialisation.\n",
    "- When `joker=True`:\n",
    "  - Update the value of `J` in our `card_scores`. Thus, our `__lt__()` will now work without modification.\n",
    "  - In `_determine_hand_type()` I've added an extra block that only runs if we're in _joker mode_. This block replaces any `J` in the hand with the card type that is most common. (If the card type that is most common is the `J` itself, then we instead identify the next most common card type.) We then recount the cards, and then perform the same logic as before to determine the hand type.\n",
    "- I've made `card_scores` an instance attribute, rather than a class attribute.  This is because if we leave it as a class attribute, then once we modify the value of `J`, we can no longer use it to solve Part 1. (E.g. if we were to run the solutions out of order.)\n",
    "\n",
    "So that's it!  Very little change required for Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hand:\n",
    "    # class attributes - never changes\n",
    "    HAND_TYPE   = { hand_type: val for val, hand_type \n",
    "                                   in enumerate(\"HC OP TP THREE FH FOUR FIVE\".split(), start=1)}\n",
    "    \n",
    "    def __init__(self, cards: str, joker=False) -> None:\n",
    "        \"\"\" Args:\n",
    "            cards (str): A str of five chars representating a hand\n",
    "            joker (bool, optional): Whether J is a Jack or a Joker. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.card_scores = { card: val for val, card in enumerate(\"23456789TJQKA\", start=2) }\n",
    "        \n",
    "        self.cards = cards\n",
    "        self._joker = joker\n",
    "        if self._joker:\n",
    "            self.card_scores[\"J\"] = 1 # update J value\n",
    "            \n",
    "        self._hand_type = self._determine_hand_type()\n",
    "        \n",
    "    def _determine_hand_type(self) -> str:\n",
    "        \"\"\" Determine HAND TYPE as str \"\"\"\n",
    "        cards = self.cards\n",
    "        ordered_counts = Counter(cards).most_common() # e.g. [('3', 2), ('2', 1), ('T', 1), ('K', 1)]\n",
    "    \n",
    "        # get the most common card\n",
    "        best, best_count = ordered_counts[0] # e.g. ('3', 2)\n",
    "        if best_count == 5:\n",
    "            return \"FIVE\"\n",
    "        \n",
    "        second_best, second_best_count = ordered_counts[1] # e.g. ('2', 1)\n",
    "        \n",
    "        if self._joker: # Part 2\n",
    "            if best == 'J': # get the next best card\n",
    "                best, best_count = second_best, second_best_count\n",
    "            \n",
    "            # convert all J into the best card\n",
    "            cards = cards.replace(\"J\", best)\n",
    "\n",
    "            ordered_counts = Counter(cards).most_common() # recount the hand\n",
    "            best, best_count = ordered_counts[0]\n",
    "            if best_count == 5:\n",
    "                return \"FIVE\"\n",
    "        \n",
    "            second_best, second_best_count = ordered_counts[1] # e.g. ('2', 1)    \n",
    "            \n",
    "        match best_count: # implement a switch-case\n",
    "            case 4:\n",
    "                return \"FOUR\"\n",
    "            case 3:\n",
    "                return \"FH\" if second_best_count == 2 else \"THREE\"\n",
    "            case 2:\n",
    "                return \"TP\" if second_best_count == 2 else \"OP\"\n",
    "            case _:\n",
    "                return \"HC\"\n",
    "    \n",
    "    def hand_value(self):\n",
    "        \"\"\" Return a score, based on hand type \"\"\"\n",
    "        return Hand.HAND_TYPE[self._hand_type]\n",
    "    \n",
    "    def __lt__(self, other: Hand):\n",
    "        \"\"\" Compare this hand with another hand. Winning hand has stronger hand type. \n",
    "        For hand type ties, winning hand is determined by highest card, \n",
    "        starting with the first card in the hand. \"\"\"\n",
    "        if self.hand_value() != other.hand_value():\n",
    "            return self.hand_value() < other.hand_value()\n",
    "        \n",
    "        assert self.hand_value() == other.hand_value(), \"Hand types are the same\"\n",
    "        for this_card, other_card in zip(self.cards, other.cards):\n",
    "            if this_card == other_card:\n",
    "                continue\n",
    "            \n",
    "            return self.card_scores[this_card] < self.card_scores[other_card]\n",
    "        \n",
    "        assert False, \"We should not get here\"\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"cards={self.cards}; hand_type={self._hand_type}\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(x={self.cards})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(data, joker=False):\n",
    "    \"\"\" For Part 2, we set joker=True \"\"\"\n",
    "    hands_and_bids = []\n",
    "    for line in data:\n",
    "        cards, bid = line.split()\n",
    "        hand = Hand(cards, joker)\n",
    "        hands_and_bids.append((hand, int(bid))) # [ (hand, bid), ... ]\n",
    "    \n",
    "    hands_and_bids = sorted(hands_and_bids, key=lambda x: x[0])\n",
    "            \n",
    "    return sum(rank*bid for rank, (hand, bid) in enumerate(hands_and_bids, start=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"32T3K 765\",\n",
    "                  \"T55J5 684\",\n",
    "                  \"KK677 28\",\n",
    "                  \"KTJJT 220\",\n",
    "                  \"QQQJA 483\"]\n",
    "                 ]\n",
    "sample_answers = [6440]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "soln = solve(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [5905]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input, joker=True), curr_ans) # test with sample data\n",
    "\n",
    "soln = solve(input_data, joker=True)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 8: Haunted Wasteland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"8\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 8 Part 1\n",
    "\n",
    "We're navigating a maze. The input looks like this:\n",
    "\n",
    "```\n",
    "RL\n",
    "\n",
    "AAA = (BBB, CCC)\n",
    "BBB = (DDD, EEE)\n",
    "CCC = (ZZZ, GGG)\n",
    "DDD = (DDD, DDD)\n",
    "EEE = (EEE, EEE)\n",
    "GGG = (GGG, GGG)\n",
    "ZZZ = (ZZZ, ZZZ)\n",
    "```\n",
    "\n",
    "- The top line is a list of left/right instructions. The instructions wrap indefinitely.\n",
    "- You start at `AAA`. If you follow a left instruction, you pick the next left node. If you follow a right instruction, you pick the next right node.\n",
    "\n",
    "**Starting at AAA, follow the left/right instructions. How many steps are required to reach ZZZ?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- First, parse the data. This is easy enough. \n",
    "  - I create a single string to represent the L/R instructions.\n",
    "  - I create a dict to map each source node to a pair of target nodes. The target nodes will be stored as a tuple, e.g. `(\"BBB\", \"CCC\")`\n",
    "\n",
    "- Set our starting node to \"AAA\".\n",
    "- Enter a while loop that ends when our current node is \"ZZZ\".\n",
    "- In the loop:\n",
    "  - Get the left and right target nodes for the current node.\n",
    "  - Retrieve the next instruction, and set the next node accordingly.\n",
    "\n",
    "Nice and easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_instructions(data):\n",
    "    instructions = data[0] # first line is our L/R instructions\n",
    "    nodes = {}\n",
    "    for line in data[2:]:\n",
    "        src_node, target_nodes = (x.strip() for x in line.split(\"=\"))\n",
    "        left_node, right_node = (x.strip() for x in target_nodes.split(\",\"))\n",
    "        left_node = left_node.replace(\"(\", \"\")\n",
    "        right_node = right_node.replace(\")\", \"\")\n",
    "        nodes[src_node] = (left_node, right_node)\n",
    "        \n",
    "    return instructions,nodes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(instructions: str, nodes: dict):\n",
    "    logger.debug(nodes)\n",
    "    instructions_cycler = cycle(instructions) # an infinite repeat of the instructions\n",
    "    \n",
    "    current_node = \"AAA\" # where we start\n",
    "    loop = 0\n",
    "    while current_node != \"ZZZ\": # our goal\n",
    "        instruction = next(instructions_cycler)\n",
    "        left, right = nodes[current_node]\n",
    "        current_node = left if instruction == \"L\" else right\n",
    "        loop += 1\n",
    "        if loop == 1000000: \n",
    "            assert False, \"I can't get out!\"\n",
    "    \n",
    "    return loop \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"RL\",\n",
    "                  \"\", \n",
    "                  \"AAA = (BBB, CCC)\",\n",
    "                  \"BBB = (DDD, EEE)\",\n",
    "                  \"CCC = (ZZZ, GGG)\",\n",
    "                  \"DDD = (DDD, DDD)\",\n",
    "                  \"EEE = (EEE, EEE)\",\n",
    "                  \"GGG = (GGG, GGG)\",\n",
    "                  \"ZZZ = (ZZZ, ZZZ)\"], \n",
    "                 [\"LLR\",\n",
    "                  \"\",\n",
    "                  \"AAA = (BBB, BBB)\",\n",
    "                  \"BBB = (AAA, ZZZ)\",\n",
    "                  \"ZZZ = (ZZZ, ZZZ)\"]\n",
    "]\n",
    "sample_answers = [2, 6]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_instructions = parse_instructions(curr_input)\n",
    "    validate(solve_part1(*curr_instructions), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "instructions = parse_instructions(input_data)\n",
    "soln = solve_part1(*instructions)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 8 Part 2\n",
    "\n",
    "Oh, _of course!_  The map is meant for ghosts!!\n",
    "\n",
    "We're told that we need to start by navigate away from ALL nodes that end with \"A\" in parallel. We only end when our navigation leads us to a set of nodes that all end with \"Z\" at the same time.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "First, I tried to solve this in a similar way to Part 1. I.e.\n",
    "\n",
    "- Find all the nodes that end in `A`.\n",
    "- Then, loop through the instructions in a cyclic way, as before. For each loop:\n",
    "  - Follow the instruction for our nodes, which gives us the next set of nodes to follow.\n",
    "  - Quit the loop, when all of our current nodes end with `Z`. Here I used the Python `all()` function, which returns `True` only if ALL of the members of the collection passed to it are themselves `True`.\n",
    "\n",
    "And this solution worked straight away for the test input. Yay!\n",
    "\n",
    "But when I ran it with the real input... Well, it was still running 30 minutes later. So this solution does not scale.  **I need a better approach!!** (Read on after the code below to see the better approach.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad solution...\n",
    "def solve_part2(instructions: str, nodes: dict[str, tuple]):\n",
    "    instructions_cycler = cycle(instructions)\n",
    "    \n",
    "    # all nodes that end with A\n",
    "    starting_nodes = [node for node in nodes if node.endswith(\"A\")]\n",
    "    current_nodes = starting_nodes\n",
    "    for i, instruction in enumerate(instructions_cycler):\n",
    "        next_nodes = []\n",
    "        for node in current_nodes: # go through each current node\n",
    "            left, right = nodes[node]\n",
    "            next_nodes.append(left if instruction == \"L\" else right)\n",
    "        \n",
    "        if all([node.endswith(\"Z\") for node in next_nodes]):\n",
    "            return i\n",
    "        \n",
    "        current_nodes = next_nodes\n",
    "        if i == 1000000000: \n",
    "            break\n",
    "    \n",
    "    assert False, \"I can't get out!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The better approach:**\n",
    "\n",
    "- We have multiple paths to follow in parallel. Work out how long it takes to get from `??A` to `??Z` for each path, i.e. for each starting node.\n",
    "- The time taken to reach `??Z` is different for different paths.\n",
    "- We need to determine the time at which all the `??Z` converge.\n",
    "- And here's a crucial observation! Whenever you get to `??Z` and then continue, the _next_ node is always the node that the original `??A` took you to. (I checked this in my actual input data.) This means that the time required to get from `??A` to `??Z` is the same as the time required to get from this `??Z` back to itself. I.e. **the cycle time is constant, for any given path, and the cycle always restarts with the same node.** (If this were not true, we would need Chinese Remainder Theorem to solve this problem.)\n",
    "- And because we now have repeating constant cycle times, it means we just need to find a way to line up all the cycle times, such that we can converge on `??Z`. We can do this using the **lowest common multiple (LCM) of these times.** The LCM of a set of integers is the smallest number that is exactly divisible by all those integers.\n",
    "\n",
    "So, to solve:\n",
    "\n",
    "- Obtain the number of loops required to get from `??A` to `??Z`, exactly as we did for Part 1. (But now our goal is `??Z` rather than `ZZZ`.)\n",
    "- Do this for each starting position. Store all the resulting loop counts in `loops`.\n",
    "- Then use `math.lcm(*loops)`. \n",
    "  - Note that Python has implemented the `math.lcm()` function since Python 3.9. Before then, you had to write your own!\n",
    "  - The `lcm()` function expects two or more integers as arguments.  So here I'm using the _splat_ operator (`*`) to unpack my list of loop values.\n",
    "\n",
    "And that's it!  The solution returns instantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2_lcm(instructions: str, nodes: dict[str, tuple]):\n",
    "    starting_nodes = [node for node in nodes if node.endswith(\"A\")]\n",
    "    loops = []\n",
    "    for node in starting_nodes:\n",
    "        # determine how many instructions before THIS node gets to \"??Z\"\n",
    "        instructions_cycler = cycle(instructions)\n",
    "        loop = 0\n",
    "        while not node.endswith(\"Z\"): # our goal\n",
    "            instruction = next(instructions_cycler)\n",
    "            left, right = nodes[node]\n",
    "            node = left if instruction == \"L\" else right\n",
    "            loop += 1\n",
    "            if loop == 1000000: \n",
    "                assert False, \"I can't get out!\"\n",
    "        \n",
    "        loops.append(loop)\n",
    "    \n",
    "    return math.lcm(*loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"LR\",\n",
    "                  \"\", \n",
    "                  \"11A = (11B, XXX)\", \n",
    "                  \"11B = (XXX, 11Z)\", \n",
    "                  \"11Z = (11B, XXX)\", \n",
    "                  \"22A = (22B, XXX)\", \n",
    "                  \"22B = (22C, 22C)\", \n",
    "                  \"22C = (22Z, 22Z)\", \n",
    "                  \"22Z = (22B, 22B)\", \n",
    "                  \"XXX = (XXX, XXX)\"]\n",
    "] \n",
    "sample_answers = [6]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_instructions = parse_instructions(curr_input)\n",
    "    validate(solve_part2_lcm(*curr_instructions), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2_lcm(*instructions)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 9: Mirage Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"9\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 9 Part 1\n",
    "\n",
    "We're using an _Oasis And Sand Instability Sensor (OASIS)_ to take readings of our environment.  The data looks like this:\n",
    "\n",
    "```text\n",
    "0 3 6 9 12 15\n",
    "1 3 6 10 15 21\n",
    "10 13 16 21 30 45\n",
    "```\n",
    "\n",
    "Each line is the history of a sequence representing a single attribute.\n",
    "\n",
    "Some of these sequences will be first-order arithmetic progressions, i.e. where the increment remains constant. E.g. 0, 3, 6, 9... \n",
    "\n",
    "```text\n",
    "0   3   6   9  12  15  18\n",
    "  3   3   3   3   3   3\n",
    "    0   0   0   0   0\n",
    "```\n",
    "\n",
    "The first order difference is always 3.\n",
    "\n",
    "Some will be second-order arithmetic progressions, i.e. where the increment changes by a constant amount. E.g. 1, 3, 6, 10... \n",
    "\n",
    "```\n",
    "1   3   6  10  15  21\n",
    "  2   3   4   5   6\n",
    "    1   1   1   1\n",
    "      0   0   0\n",
    "```\n",
    "\n",
    "The differences are 2, 3, 4, etc. The second-order difference is constant.\n",
    "\n",
    "Some will be nth-order progressions. E.g. 10, 13, 16, 21, 30... \n",
    "\n",
    "```\n",
    "10  13  16  21  30  45  68\n",
    "   3   3   5   9  15  23\n",
    "     0   2   4   6   8\n",
    "       2   2   2   2\n",
    "         0   0   0\n",
    "```\n",
    "\n",
    "Here, the third order diferences are constant.  \n",
    "\n",
    "We need to find the next value in each sequence.\n",
    "\n",
    "**Analyze your OASIS report and extrapolate the next value for each history. What is the sum of these extrapolated values?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "I used to _really_ hate [recursion](https://aoc.just2good.co.uk/python/recursion). But, having been forced to use it a bit in previous AoCs, I now simply _dislike_ recursion. I don't dislike the _idea_ of it. I dislike the fact that my tiny brain seems to struggle with exactly what needs to be returned for each case.\n",
    "\n",
    "Anyhoo... This problem is just begging for recursion!  If we were only ever seeing first-order and second-order progressions, then we could use simple math to work out the _n_ th term. But it was my guess that the real input could go pretty deep. So we need to be able to keep recursing to the next set of differences, until the differences remain constant.\n",
    "\n",
    "**[NumPy](https://aoc.just2good.co.uk/python/numpy)** for the win!! NumPy has built in capabilities to determine the differences between values in an array, and return these differences as a new array!  This saves me a few lines of code.\n",
    "\n",
    "So, here's my approach:\n",
    "\n",
    "- First, I parse the input data. I convert each line into a `list` of `int`. I.e. each list represents our _sequence_.\n",
    "- Then, for each _sequence_, I convert it to a one dimension NumPy ndarray and pass to my `recurse_diffs()` function. Note that I'm using a [list comprehension](https://aoc.just2good.co.uk/python/comprehensions) to do all of this, for each _sequence_. As a result, my `solve()` function is short, neat, and _Pythonic_.\n",
    "- Now, my recursive function, `recurse_diffs()`:\n",
    "  - Here, I first use the NumPy method `diff()` to return all the differences between terms, as a new `ndarray`. Note: without NumPy, we could also do: \\\n",
    "  `[y-x for x, y in zip(array, array[1:])]`\n",
    "  - Then I use `np.all(diffs == diffs[0])` to check if _all_ the values in this _diffs_ array are equal to the value of the first term in the array.  If so, then we've reached the point where the _diffs_ are constant.\n",
    "  - If not, then we recursively call this same function, but passing in the current set of _diffs_ as the new sequence.\n",
    "  - When we've reached the level of nesting where the diffs are constant, we add this diff to the current sequence of diffs, and use this to determine the next value in the diffs sequence.\n",
    "  - Then we bubble this same approach all the way back to the top. I.e. for each level of diffs, we obtain the next diff. And finally, at the very top, this gives us the next value in our sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sequences(data: list[str]):\n",
    "    return [[int(x) for x in line.split()] for line in data]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recurse_diffs(sequence: np.ndarray, forwards=True) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the next value in a numeric sequence based on the pattern of differences.\n",
    "\n",
    "    Recursively analyses the differences between consecutive elements of the sequence. \n",
    "    Recurses until the differences remain constant. \n",
    "    It then calculates the next value in the sequence based on this constant difference.\n",
    "\n",
    "    Parameters:\n",
    "        sequence (np.ndarray): A NumPy array representing the sequence.\n",
    "        forwards (bool, optional): A flag to determine the direction of progression.\n",
    "                                   If True (default), the function calculates the next value. \n",
    "                                   If False, it calculates the previous value in the sequence.\n",
    "\n",
    "    Returns:\n",
    "        int: The next (or previous) value in the sequence\n",
    "    \"\"\"\n",
    "    diffs = np.diff(sequence)\n",
    "    \n",
    "    op = operator.add if forwards else operator.sub\n",
    "    term = sequence[-1] if forwards else sequence[0]\n",
    "    \n",
    "    # Check if all the diffs are constant\n",
    "    # If they are, we've reached the deepest point in our recursion, and we know the constant diff\n",
    "    if np.all(diffs == diffs[0]):\n",
    "        next_val = op(term, diffs[0])\n",
    "    else: # if the diffs are not constant, then we need to recurse\n",
    "        diff = recurse_diffs(diffs, forwards)\n",
    "        next_val = op(term, diff)\n",
    "        \n",
    "    return int(next_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(sequences: list[list[int]], forwards=True):\n",
    "    next_vals = [recurse_diffs(np.array(sequence), forwards) for sequence in sequences]\n",
    "    return sum(next_vals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"0 3 6 9 12 15\", \n",
    "                  \"1 3 6 10 15 21\",\n",
    "                  \"10 13 16 21 30 45\"]\n",
    "                ]\n",
    "sample_answers = [114]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_sequences = parse_sequences(curr_input)\n",
    "    validate(solve(curr_sequences), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "sequences = parse_sequences(input_data)\n",
    "soln = solve(sequences)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 9 Part 2\n",
    "\n",
    "Now we need to find the previous result in each series!!\n",
    "\n",
    "It turns out that I can do this with a pretty trivial change to my `recurse_diffs()` function. I add a new parameter (`forwards`) that determines if we want to move forwards to the _next_ term in the sequence, or backwards to the previous term in the sequence.  Forwards is the default. If we set `forwards` to `False`, all we need to do is always return the _first term minus_ the diff, rather than the _last term plus_ the diff.\n",
    "\n",
    "Note this cool construct:\n",
    "\n",
    "```python\n",
    "    op = operator.add if forwards else operator.sub\n",
    "    term = sequence[-1] if forwards else sequence[0]\n",
    "```\n",
    "\n",
    "This is a neat way to set the `op` variable to be either the _add_ function, or the _substract_ (_sub_) function, depending on the value of `forwards`. And I use the same approach for setting the term we want to add the _diff_ to.  I.e. either the last term in the sequence, or the first.\n",
    "\n",
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [2]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_sequences = parse_sequences(curr_input)\n",
    "    validate(solve(curr_sequences, False), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve(sequences, False)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 10: Pipe Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"10\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 10 Part 1\n",
    "\n",
    "Well, I didn't enjoy today at all.  Part 1 was okay, but Part 2 took me WAY TOO MUCH time.\n",
    "\n",
    "So... We're standing in a field packed with pipes.  The pipes are arranged into a 2D grid of tiles. The pipe forms one long continuous loop. E.g.\n",
    "\n",
    "```text\n",
    ".....\n",
    ".F-7.\n",
    ".|.|.\n",
    ".L-J.\n",
    ".....\n",
    "```\n",
    "\n",
    "Here, `-` represents a horizontal pipe, `|` is a vertical pipe, `F` is a top-left join, `7` is a top-right join, `L` is a bottom left-join, and `J` is a bottom-right join.\n",
    "\n",
    "We start at `S`. In our main loop, every pipe will be connected to one or two neighbours. But there may also be pipes not connected to the loop:\n",
    "\n",
    "```text\n",
    "-L|F7\n",
    "7S-7|\n",
    "L|7||\n",
    "-L-J|\n",
    "L|-JF\n",
    "```\n",
    "\n",
    "**Find the single giant loop starting at S. How many steps along the loop does it take to get from the starting position to the point farthest from the starting position?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "This seems like a perfect case for a [BFS](https://aoc.just2good.co.uk/python/shortest_paths)!\n",
    "\n",
    "I will BFS from the start position. My BFS should result in two paths being explored at the same time: i.e. because from any position in a closed loop, we can always walk in two directions.\n",
    "\n",
    "![BFS closed loop](https://aoc.just2good.co.uk/assets/images/bfs_closed_loop.png)\n",
    "\n",
    "I'll make a `PipeGrid` class by extending my previous `Grid` class.  In this class:\n",
    "\n",
    "- `pipes_for_direction` show the valid pipes and connectors if we move from a given pipe in that direction.\n",
    "- `directions_for_pipe` gives the valid directions we can move from, starting from a given pipe/connector.\n",
    "- `valid_neighbours_for_loop_pipe()` does the following:\n",
    "  - Determines what sort pipe or connector is at our current point.\n",
    "  - Determines which directions we can move towards, from this pipe, using `directions_for_pipe`.\n",
    "  - Determines which pipes/connects are valid in those directions, using `pipes_for_direction`.\n",
    "  - Uses these to determine which adjacent tiles are valid next moves, to continue our pipe.\n",
    "\n",
    "Next, I determine the location of our start point.\n",
    "\n",
    "Then, I use the `pipe_bfs()` function to determine all the locations that make up our closed loop. \n",
    "  \n",
    "- This uses the standard BFS approach, performing a flood fill outwards from the start location.\n",
    "- It builds a breadcrumb trail called `came_from`. This is a dict which stores: \\\n",
    "    `{ point: (predecessor, step_count), ... }`\n",
    "- As we find successive pipes/connectors (which grow symmetrically, since we have a loop), we store the current furthest distance from the start position.\n",
    "\n",
    "Finally, we can return the furthest distance achieved.\n",
    "\n",
    "Not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "For a bit of fun, I decided to plot a heatmap that shows the journey from start to furthest, using [matplotlib](https://aoc.just2good.co.uk/python/matplotlib). I do this in my `plot_grid()` function.  It works by:\n",
    "\n",
    "- Creating a [NumPy](https://aoc.just2good.co.uk/python/numpy) grid to represent all step counts.\n",
    "- We initialise all grid points to -1.\n",
    "- We extract step counts from our `dict_from`. Thus, `S` has value 0, and each subsequent step will increment the count by 1.\n",
    "- I've made the starting location black, and the final (furthest) location white.\n",
    "\n",
    "For our small square test grid, the result looks like this:\n",
    "\n",
    "![Pipe loop small square](https://aoc.just2good.co.uk/assets/images/pipeloop_1.png)\n",
    "\n",
    "And with my real data:\n",
    "\n",
    "![Pipe loop real](https://aoc.just2good.co.uk/assets/images/pipeloop_real.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipeGrid(Grid):\n",
    "    \"\"\" A 2D grid that contains tiles. Tiles may be empty, or have pipe parts. \"\"\"\n",
    "    \n",
    "    pipes_for_direction = { # if in the pipe loop, these are valid pipe parts in each direction\n",
    "        \"N\": {\"|\", \"F\", \"7\"},\n",
    "        \"E\": {\"-\", \"J\", \"7\"},\n",
    "        \"S\": {\"|\", \"L\", \"J\"},\n",
    "        \"W\": {\"-\", \"F\", \"L\"}\n",
    "    }\n",
    "    \n",
    "    directions_for_pipe = { # for a given pipe part, these are valid next directions\n",
    "        \"|\": [\"N\", \"S\"], # vertical pipe\n",
    "        \"-\": [\"E\", \"W\"], # horizontal pipe\n",
    "        \"L\": [\"N\", \"E\"], # bottom-left join\n",
    "        \"J\": [\"N\", \"W\"], # bottom-right join\n",
    "        \"7\": [\"S\", \"W\"], # top-right join\n",
    "        \"F\": [\"S\", \"E\"], # top-left join\n",
    "        \"S\": [\"N\", \"E\", \"S\", \"W\"] # start location\n",
    "    }\n",
    "    \n",
    "    def infer_start_type(self, loop: list[Point]) -> str:\n",
    "        \"\"\" Determine the pipe type of the start point. It does this by determining\n",
    "        the two connected pipes in the loop. From their directions and types, we know what the S\n",
    "        must be.\n",
    "        \"\"\"\n",
    "        start_posn = len(loop) // 2\n",
    "        assert self.value_at_point(loop[start_posn]) == \"S\", \"Middle of the loop should be start\"\n",
    "        \n",
    "        dirs = set()\n",
    "        for vector in [loop[start_posn+1] - loop[start_posn], loop[start_posn-1] - loop[start_posn]]:\n",
    "            (x, y) = vector.x, vector.y\n",
    "            match (x, y):\n",
    "                case Vectors.N.value:\n",
    "                    dirs.add(\"N\")\n",
    "                case Vectors.E.value:\n",
    "                    dirs.add(\"E\")\n",
    "                case Vectors.S.value:\n",
    "                    dirs.add(\"S\")\n",
    "                case Vectors.W.value:\n",
    "                    dirs.add(\"W\")\n",
    "                case _:\n",
    "                    assert False, \"Invalid direction\"\n",
    "        \n",
    "        for pipe_type, directions in PipeGrid.directions_for_pipe.items():\n",
    "            if dirs == set(directions):\n",
    "                return pipe_type\n",
    "        \n",
    "        assert False, \"No valid pipe type found\"                    \n",
    "        \n",
    "    def valid_neighbours_for_loop_pipe(self, point: Point) -> list[Point]:\n",
    "        \"\"\" Get valid neighbours, given a current location that is the main pipe loop. \"\"\"\n",
    "        valid = []\n",
    "        this_pipe = self.value_at_point(point)\n",
    "        assert this_pipe in PipeGrid.directions_for_pipe, \"Point must be a pipe or S\"   \n",
    "        \n",
    "        allowed_directions = PipeGrid.directions_for_pipe[this_pipe]\n",
    "        allowed_vectors = [Vectors[direction] for direction in allowed_directions]\n",
    "\n",
    "        assert allowed_vectors, \"We must be allowed to move in at least one direction\"\n",
    "        \n",
    "        for direction, neighbour_pt in zip(allowed_directions, point.get_specific_neighbours(allowed_vectors)):\n",
    "            neighbour_val = self.value_at_point(neighbour_pt)\n",
    "            if neighbour_val in PipeGrid.pipes_for_direction[direction]:\n",
    "                valid.append(neighbour_pt)\n",
    "                \n",
    "        return valid\n",
    "                    \n",
    "    def valid_neighbours_for_non_loop(self, point: Point, loop: list[Point]) -> list[Point]:\n",
    "        valid = []\n",
    "        for neighbour in point.neighbours(include_diagonals=False):\n",
    "            if self.valid_location(neighbour) and neighbour not in loop:\n",
    "                valid.append(neighbour)\n",
    "\n",
    "        return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grid(data) -> PipeGrid:\n",
    "    return PipeGrid(data)\n",
    "    \n",
    "def pipe_bfs(grid: PipeGrid, start: Point) -> tuple[int, Point, dict[Point, tuple]]:\n",
    "    \"\"\" Use BFS to establish the points that make up the closed loop, \n",
    "    and determine the furthest location in the loop.\n",
    "\n",
    "    Args:\n",
    "        grid (PipeGrid): 2D grid containing a closed loop\n",
    "        start (Point): Starting point\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, Point, dict[Point, tuple]]: max distance, furthest point, came_from\n",
    "    \"\"\"\n",
    "    assert grid.value_at_point(start) == \"S\", \"We have found the start\"    \n",
    "    frontier = deque()\n",
    "    frontier.append((start, 0)) # store (posn, step_count)\n",
    "    came_from = {}\n",
    "    came_from[start] = (None, 0) # store (posn, step_count)\n",
    "    \n",
    "    max_steps, furthest = 0, start\n",
    "    \n",
    "    # keep going until the frontier is empty; i.e. when we've explored all the valid nodes\n",
    "    # the frontier will expand in two directions as we build the loop\n",
    "    while frontier:\n",
    "        current, step_count = frontier.popleft()  # pop the first item off the FIFO queue\n",
    "        \n",
    "        if step_count > max_steps:\n",
    "            max_steps = step_count\n",
    "            furthest = current\n",
    "       \n",
    "        for neighbour in grid.valid_neighbours_for_loop_pipe(current):\n",
    "            steps_to_neighbour = step_count + 1\n",
    "            if neighbour not in came_from:\n",
    "                frontier.append((neighbour, steps_to_neighbour))\n",
    "                came_from[neighbour] = (current, steps_to_neighbour)\n",
    "    \n",
    "    return max_steps, furthest, came_from\n",
    "\n",
    "def plot_grid(grid: PipeGrid, max_steps: int, came_from: dict[Point, tuple]):\n",
    "    steps_array = np.full((grid.height, grid.width), fill_value=-1, dtype=int)\n",
    "    \n",
    "    # Fill the steps_array with steps taken for each point\n",
    "    for point, (predecessor, steps) in came_from.items():\n",
    "        steps_array[point.y, point.x] = steps\n",
    "   \n",
    "    plt.imshow(steps_array, cmap=\"viridis\", origin=\"lower\", interpolation=\"none\", \n",
    "               extent=[0, grid.width, 0, grid.height])\n",
    "\n",
    "    # Highlight squares where steps is max (white color)\n",
    "    for point, (predecessor, steps) in came_from.items():\n",
    "        if steps == 0:\n",
    "            rect = Rectangle((point.x, point.y), 1, 1, fill=True, color='black')\n",
    "            plt.gca().add_patch(rect)\n",
    "        \n",
    "        if steps == max_steps:\n",
    "            rect = Rectangle((point.x, point.y), 1, 1, fill=True, color='white')\n",
    "            plt.gca().add_patch(rect)\n",
    "            \n",
    "    plt.colorbar(label=\"Steps Taken\")\n",
    "    plt.grid(True, which=\"both\", color=\"black\", linewidth=0)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.title('Number of Steps Taken for Each Point')\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.show()    \n",
    "    \n",
    "def solve_part1(grid: PipeGrid, show_plot=True) -> tuple[int, Point, dict[Point, tuple]]:\n",
    "    \"\"\" Returns:\n",
    "        tuple[int, Point, dict[Point, tuple]]: max distance, furthest point, came from\n",
    "    \"\"\"\n",
    "    start = next(point for point in grid.all_points()\n",
    "                if grid.value_at_point(point) == \"S\")\n",
    "    max_steps, furthest, came_from = pipe_bfs(grid, start)\n",
    "    if show_plot:\n",
    "        plot_grid(grid, max_steps, came_from)\n",
    "        \n",
    "    return max_steps, furthest, came_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = [[\"..F7.\",\n",
    "                  \".FJ|.\",\n",
    "                  \"SJ.L7\",\n",
    "                  \"|F--J\",\n",
    "                  \"LJ...\"]\n",
    "]\n",
    "sample_answers = [8]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_grid = parse_grid(curr_input)\n",
    "    curr_soln, curr_furthest, curr_from = solve_part1(curr_grid)\n",
    "    validate(curr_soln, curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "grid = parse_grid(input_data)\n",
    "soln, furthest, came_from = solve_part1(grid)\n",
    "logger.info(f\"Part 1: steps to farthest point={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 10 Part 2\n",
    "\n",
    "**How many tiles are enclosed by the loop?**\n",
    "\n",
    "OMG. This was pretty tough for me. But it turns out there's quite a few easy ways to do this!\n",
    "\n",
    "We're given a sample loop which contains pockets of _inside_ tiles:\n",
    "\n",
    "```text\n",
    "...........\n",
    ".S-------7.\n",
    ".|F-----7|.\n",
    ".||OOOOO||.\n",
    ".||OOOOO||.\n",
    ".|L-7OF-J|.\n",
    ".|II|O|II|.\n",
    ".L--JOL--J.\n",
    ".....O.....\n",
    "```\n",
    "\n",
    "Let's look at this same map, using the visualisation from Part 1. This makes it easy to see which tiles are _inside_ and which tiles are _outside_:\n",
    "\n",
    "![Finding loops, sample data](https://aoc.just2good.co.uk/assets/images/finding_loops_sample.png)\n",
    "\n",
    "The big challenge is that our pipe loop can have pipes adjacent to each other, forming pockets. Furthermore, our adjacent pipes might form a pocket with a channel to the outside.  So, in the next example, the channel to the outside is now made up of adjacent pipes.  But it _still counts_ as a channel to the outside:\n",
    "\n",
    "```text\n",
    "..........\n",
    ".S------7.\n",
    ".|F----7|.\n",
    ".||OOOO||.\n",
    ".||OOOO||.\n",
    ".|L-7F-J|.\n",
    ".|II||II|.\n",
    ".L--JL--J.\n",
    "..........\n",
    "```\n",
    "\n",
    "In the example above, the central pocket has tiles labelled as _outside_ (O), because they have a channel to the outside.\n",
    "\n",
    "![Adjacent channels](https://aoc.just2good.co.uk/assets/images/finding_loops_sample_2.png)\n",
    "\n",
    "Determining which pockets are inside or outside is tricky!\n",
    "\n",
    "**My solution**:\n",
    "\n",
    "Start by taking the `came_from` breadcrumbs dict from Part 1, and converting it into a complete path of Points that make up our loop.\n",
    "\n",
    "- Using the dict from farthest point out to starting point, I build a path of one half of the loop. Remember, the dict maps each point to its predecessor; it only goes one way so we have to go from furthest, and move all the way back to the start.\n",
    "- But I still need to determine the path through the other half of the loop. I do this by determining: the _neighbour_ of the point that is furthest out, that is also part of our overall loop, that is not in the `first_half` path that we've already created, and which is also a valid move from the point that is furthest out. This gives me the _other_ point that is connected to the furthest out point in our loop.  And from here, I can now build a path from this point back to the start.\n",
    "- Then I join these two paths together, thus creating the closed loop.\n",
    "\n",
    "![Closed loop two halves](https://aoc.just2good.co.uk/assets/images/closed_loop_two_halves.png)\n",
    "\n",
    "Next, I determine all the _regions_ (pockets) i.e. contiguous points of tiles. \n",
    "\n",
    "- These are pockets of tiles that are either _inside_ or _outside_.\n",
    "- Again, I do this with a BFS _floodfill_, for all points that are not part of the main loop. \n",
    "- We expand a given point until we've fully flood-filled its associated region. Of course, each time we flood fill from a point, we eliminate a bunch of points that we need to flood fill form.\n",
    "- Interesting observation: I pass my main loop to my `get_tile_regions()` function in the form of a `list[Point]`. During our flood fill, we have to check if we've reached a point in the loop. However, it took minutes when I used a list. If I converted by loop points to a `set`, it **runs in seconds**!  Which just goes to show how much more efficient Python is at checking whether something is a member of `set`, vs checking membership of a `list`.`  \n",
    "\n",
    "Finally, I return these regions.\n",
    "\n",
    "For each region, I can now arbitrarily pick any point in that region and see if that point is enclosed by the main loop. In the end, I cheated a little, and made use of `matplotlib` `contains_points()` to determine which regions are contained by the loop. Where any region has any point that is contained by the loop, we can conclude that all the points that region are contained by the loop. So we can add all these points to the list of included points.\n",
    "\n",
    "And that's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "I decied to plot the loops and _internal_ tiles. It's an interesting example of superimposing a scatter graph on a line graph.\n",
    "\n",
    "```python\n",
    "    plt.plot(loop_x_values, loop_y_values, \n",
    "             marker='o', linestyle='-', color=\"blue\", label=\"Loop\")\n",
    "        \n",
    "    plt.scatter(inside_x_values, inside_y_values, \n",
    "                marker='x', color=\"red\", label=\"Inside\")\n",
    "```\n",
    "\n",
    "Here's one of the sample grids:\n",
    "\n",
    "![Finding loops, sample data](https://aoc.just2good.co.uk/assets/images/contained_points.png)\n",
    "\n",
    "And with real data:\n",
    "\n",
    "![Finding loops, real data](https://aoc.just2good.co.uk/assets/images/contained_points_real.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_path(path: list[Point], inside: set[Point]):\n",
    "    # Extract x and y values from the path\n",
    "    loop_x_values = [point.x for point in path]\n",
    "    loop_y_values = [point.y for point in path]\n",
    "    \n",
    "    # Extract x and y values from the inside set\n",
    "    inside_x_values = [point.x for point in inside]\n",
    "    inside_y_values = [point.y for point in inside]\n",
    "\n",
    "    # Plot the line and scatter graphs\n",
    "    plt.plot(loop_x_values, loop_y_values, \n",
    "             marker=MarkerStyle('o'), linestyle='-', color=\"blue\", label=\"Loop\")\n",
    "        \n",
    "    plt.scatter(inside_x_values, inside_y_values, \n",
    "                marker=MarkerStyle('x'), color=\"red\", label=\"Inside\")\n",
    "    \n",
    "    plt.title('Path Plot')\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tile_regions(grid: PipeGrid, loop_path: list[Point]) -> list[set]:\n",
    "    \"\"\" Determine the regions (pockets) that enclose non-main-loop tiles.\n",
    "\n",
    "    Returns a list of multiple sets, where each set is a region.\n",
    "    The regions may be internal or external to the main loop.\n",
    "    \"\"\"\n",
    "    regions = [] # list of sets, where each set is a tile region\n",
    "    explored = set()\n",
    "    \n",
    "    logger.debug(\"Getting non_loop_tiles..\")\n",
    "    non_loop_tiles = set()\n",
    "    loop_path_set = set(loop_path) # converting to set makes a huge difference to performance\n",
    "    for point in tqdm(grid.all_points()): # if it's slow, we can watch the progress bar\n",
    "        if point not in loop_path_set:\n",
    "            non_loop_tiles.add(point)\n",
    "    \n",
    "    logger.debug(f\"Retrieved {len(non_loop_tiles)} non_loop_tiles\")\n",
    "    \n",
    "    # now let's BFS each region of non_loop_tiles\n",
    "    for tile in non_loop_tiles:\n",
    "        if tile in explored: # we've seen this before\n",
    "            continue\n",
    "        \n",
    "        frontier = deque()\n",
    "        frontier.append(tile)\n",
    "        this_region_explored = set()\n",
    "        this_region_explored.add(tile)\n",
    "        \n",
    "        # keep going until the frontier is empty; i.e. when we've explored all the valid nodes\n",
    "        while frontier:   \n",
    "            current = frontier.popleft()  # pop the first item off the FIFO queue\n",
    "            \n",
    "            if current in this_region_explored: # this massively improves performance\n",
    "                continue\n",
    "            \n",
    "            for neighbour in grid.valid_neighbours_for_non_loop(current, loop_path):\n",
    "                frontier.append(neighbour)\n",
    "                this_region_explored.add(neighbour)\n",
    "                    \n",
    "        explored.update(this_region_explored)\n",
    "        regions.append(this_region_explored)\n",
    "\n",
    "    return regions\n",
    "\n",
    "def get_loop_path(grid: PipeGrid, furthest: Point, came_from: dict[Point, tuple]) -> list[Point]:\n",
    "    \"\"\" Build the closed path of the main pipe loop. Returns as a list of points. \n",
    "    \n",
    "    This function constructs the loop path starting from the 'furthest' point, which is\n",
    "    the most distant point from the start in the grid, back to the starting point. The path\n",
    "    is constructed in two halves and then combined.\n",
    "    \n",
    "    Note, the start point is therefore the point in the middle.\n",
    "    \"\"\"\n",
    "    start = next(point for point in grid.all_points()\n",
    "            if grid.value_at_point(point) == \"S\")\n",
    "\n",
    "    # Build the first half of the path from the furthest point back to the start\n",
    "    path_first_half = []\n",
    "    current = furthest\n",
    "    while current != start:\n",
    "        path_first_half.append(current)\n",
    "        current = came_from[current][0] # Get the predecessor of the current point\n",
    "    \n",
    "    path_first_half.append(start)\n",
    "    \n",
    "    # Now we need the second half of the loop\n",
    "    # Find the point with a valid pipe, that is:\n",
    "    # adjacent to 'furthest', in 'came_from', but not in the first half of the path\n",
    "    join_candidates = [neighbour for neighbour in furthest.neighbours(include_diagonals=False)\n",
    "                       if neighbour in came_from \n",
    "                       and neighbour not in path_first_half\n",
    "                       and neighbour in grid.valid_neighbours_for_loop_pipe(furthest)]\n",
    "    current = join_candidates[0]\n",
    "    path_second_half = []\n",
    "    while current != start:\n",
    "        path_second_half.append(current)\n",
    "        current = came_from[current][0]\n",
    "    path_second_half.reverse() # the second half needs to continue from the first half\n",
    "    path_second_half.append(furthest) \n",
    "    \n",
    "    path = path_first_half + path_second_half\n",
    "    # logger.debug(f\"{path=}\")\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(grid: PipeGrid, furthest: Point, came_from: dict[Point, tuple]):\n",
    "    \"\"\" Determine number of tiles (which can be empty or non-loop pipe components) that are internal\n",
    "    to the main loop. \"\"\"\n",
    "    \n",
    "    loop_path = get_loop_path(grid, furthest, came_from) # get complete enclosed main loop\n",
    "    logger.debug(f\"Loop path has length {len(loop_path)}.\")\n",
    "    pltp = pltpath.Path(np.array([(point.x, point.y) for point in loop_path])) # convert to matplotlib.Path\n",
    "\n",
    "    regions = get_tile_regions(grid, loop_path) # determine all internal / external regions of tiles\n",
    "    logger.debug(\"Regions retrieved.\")\n",
    "\n",
    "    inside = set()\n",
    "    # Now let's work out if each region is inside or outside.\n",
    "    # We only need to look at one point from each region.\n",
    "    for region in regions:\n",
    "        a_point = next(iter(region)) # pick an arbitrary point in the region\n",
    "        x, y = float(a_point.x), float(a_point.y)\n",
    "        if pltp.contains_points([(x, y)]): \n",
    "            inside.update(region)\n",
    "    \n",
    "    logger.debug(f\"Inside={inside}\")\n",
    "    plot_path(loop_path, inside) # let's visualise it\n",
    "    \n",
    "    return len(inside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_inputs = []\n",
    "\n",
    "sample_inputs.append(\"\"\"...........\n",
    ".S-------7.\n",
    ".|F-----7|.\n",
    ".||.....||.\n",
    ".||.....||.\n",
    ".|L-7.F-J|.\n",
    ".|..|.|..|.\n",
    ".L--J.L--J.\n",
    "...........\"\"\")\n",
    "\n",
    "sample_inputs.append(\"\"\"..........\n",
    ".S------7.\n",
    ".|F----7|.\n",
    ".||....||.\n",
    ".||....||.\n",
    ".|L-7F-J|.\n",
    ".|..||..|.\n",
    ".L--JL--J.\n",
    "..........\n",
    "\"\"\")\n",
    "\n",
    "sample_inputs.append(\"\"\".F----7F7F7F7F-7....\n",
    ".|F--7||||||||FJ....\n",
    ".||.FJ||||||||L7....\n",
    "FJL7L7LJLJ||LJ.L-7..\n",
    "L--J.L7...LJS7F-7L7.\n",
    "....F-J..F7FJ|L7L7L7\n",
    "....L7.F7||L7|.L7L7|\n",
    ".....|FJLJ|FJ|F7|.LJ\n",
    "....FJL-7.||.||||...\n",
    "....L---J.LJ.LJLJ...\n",
    "\"\"\")\n",
    "\n",
    "sample_inputs.append(\"\"\"FF7FSF7F7F7F7F7F---7\n",
    "L|LJ||||||||||||F--J\n",
    "FL-7LJLJ||||||LJL-77\n",
    "F--JF--7||LJLJ7F7FJ-\n",
    "L---JF-JLJ.||-FJLJJ7\n",
    "|F|F-JF---7F7-L7L|7|\n",
    "|FFJF7L7F-JF7|JL---7\n",
    "7-L-JL7||F7|L7F-7F7|\n",
    "L.L7LFJ|||||FJL7||LJ\n",
    "L7JLJL-JLJLJL--JLJ.L\n",
    "\"\"\")\n",
    "\n",
    "sample_answers = [4, 4, 8, 10]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_grid = parse_grid(curr_input.splitlines())\n",
    "    curr_pt1_soln, curr_furthest, curr_from = solve_part1(curr_grid, show_plot=False)\n",
    "    curr_pt2_soln = solve_part2(curr_grid, curr_furthest, curr_from)\n",
    "    validate(curr_pt2_soln, curr_ans) # test with sample data\n",
    "    logger.debug(\"Test passed\")\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2(grid, furthest, came_from)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Solutions and Useful Resources\n",
    "\n",
    "There are a few other ways to solve this problem. \n",
    "\n",
    "- **Use the [Shoelace formula](https://en.wikipedia.org/wiki/Shoelace_formula) in conjunction with [Pick's theorem](https://en.wikipedia.org/wiki/Pick%27s_theorem)** to determine all the interior points of any polygon. This is probably the simplest and fastest solution.\n",
    "\n",
    "- **Horizontal [ray casting algorithm](https://www.youtube.com/watch?v=RSXM9bgqxJM):**\n",
    "  - Extend a virtual ray from left to right, for each row.\n",
    "  - Count how many times it intersects with the polygon. If the number of intersections is odd, then the line is inside the polygon. If it is even, it is outside of the polygon.\n",
    "  - If we have a row edge of type L---J or F---7, then this should not toggle.\n",
    "  - Because we care about the type of pipe we're hitting, we need to determine what sort of pipe `S` is. We can do this by assessing the two adjcaent loop pipe components.\n",
    "\n",
    "- **Diagonal ray casting.**\n",
    "  - Works the same way as horizontal, but avoids the need for handling the special case of \"walking along\" a channel.\n",
    "  - Cast a diagonal line from EVERY empty (`.`) point in the grid.\n",
    "  - If the line crosses an odd number of intersections, then the point is inside the loop.\n",
    "  - If the line is moving diagonal down+right, then it will be blocked by all pipe types, except for `7` and `L`.\n",
    "  - Again, we need to determine the pipe type of `S`.\n",
    "\n",
    "- You can **scale-up the entire grid by 3**. \n",
    "  - Every square is replaced by a 3x3 group of squares. \n",
    "  - The result is that loops that were adjacent now have a channel between them. \n",
    "  - We can represent the resulting expanded grid as either containing spaces of wall. \n",
    "  - Now we can flood fill from the outside.\n",
    "\n",
    "```text\n",
    ".|.\n",
    "-J.\n",
    "...\n",
    "```\n",
    "\n",
    "Would become:\n",
    "\n",
    "```text\n",
    "....#....\n",
    "....#....\n",
    "....#....\n",
    "....#....\n",
    "#####....\n",
    ".........\n",
    ".........\n",
    ".........\n",
    ".........\n",
    "```\n",
    "\n",
    "- **[Non-zero winding rule](https://en.wikipedia.org/wiki/Nonzero-rule)** to determine whether a point falls within an enclosed curve. This approach requires tracking the current direction.\n",
    "\n",
    "- This [Reddit post](https://www.reddit.com/r/adventofcode/comments/18fgddy/2023_day_10_part_2_using_a_rendering_algorithm_to/) from `tomi901` provides a **nice visual** to explain how to determine whether a point is in or out.\n",
    "\n",
    "Additionally:\n",
    "\n",
    "- For simplicity, we could have easily replaced all non-loop pipe components with `.`. (Since we already know which points make up the loop itself.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shoelace_area(polygon: list[Point]) -> int:\n",
    "    \"\"\" Use Shoelace formula to determine total area of a polygon:\n",
    "    A = 1/2 sum(x * (y+1 - y-1)) \"\"\"\n",
    "    total = 0\n",
    "    for i, (point) in enumerate(polygon):\n",
    "        next_index = (i+1) % len(polygon)\n",
    "        prev_index = i-1\n",
    "        total += point.x*(polygon[next_index].y - polygon[prev_index].y)\n",
    "        \n",
    "    return abs(total) // 2\n",
    "\n",
    "def interior_points(area: int, boundary_points: int):\n",
    "    \"\"\" Use Pick's Theorem to determine total number of internal integer points, \n",
    "    given a polygon area and number of boundary points. \"\"\"\n",
    "    return area - (boundary_points // 2) + 1  \n",
    "\n",
    "def remove_junk_parts(grid, loop_points) -> PipeGrid:\n",
    "    \"\"\" Replaces any pipe components with `.`, if the component is not part of the main loop. \"\"\"\n",
    "    return PipeGrid([\"\".join(char if Point(x,y) in loop_points else \".\" for x, char in enumerate(row))\n",
    "                                                                        for y, row in enumerate(grid.array)])\n",
    "    \n",
    "def plot_grid(grid):\n",
    "    \"\"\" Take a 2D grid with spaces and #, and plot visually \"\"\"\n",
    "    num_grid = [[1 if cell == '#' else 0 for cell in row] for row in grid]\n",
    "    num_array = np.array(num_grid)     # Convert to a NumPy array\n",
    "    \n",
    "    plt.figure(figsize=(10, 10), dpi=100)\n",
    "    plt.imshow(num_array, cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d10_with_shoelace_and_picks(grid: PipeGrid, furthest: Point, came_from: dict[Point, tuple]):\n",
    "    \"\"\" Determine number of tiles (which can be empty or non-loop pipe components) that are internal\n",
    "    to the main loop.  Here we calculate the total polygon area with Shoelace formula\n",
    "    and then determine the number of internal integer points with Pick's Theorem. \"\"\"\n",
    "    loop_path = get_loop_path(grid, furthest, came_from) # get complete enclosed main loop\n",
    "    area = shoelace_area(loop_path)\n",
    "    tiles = interior_points(area, len(loop_path))\n",
    "    \n",
    "    return tiles\n",
    "\n",
    "def d10_with_horizontal_ray_casting(grid: PipeGrid, furthest: Point, came_from: dict[Point, tuple]):\n",
    "    \"\"\" Determine number of tiles (which can be empty or non-loop pipe components) that are internal\n",
    "    to the main loop. Use ray casting lines. Ray cast from every point in the array.\n",
    "    When a ray intersects the edge of the polygon, we're inside. When they intersect again, we're outside. \n",
    "    So odd intersections means the point is inside. \"\"\"\n",
    "    loop_path = get_loop_path(grid, furthest, came_from) # get complete enclosed main loop\n",
    "    loop_points = set(loop_path)\n",
    "    cleaned_grid = remove_junk_parts(grid, loop_points)    \n",
    "    # logger.debug(f\"\\n{cleaned_grid}\")\n",
    "\n",
    "    # update \"S\" to be actual pipe type\n",
    "    start_type = grid.infer_start_type(loop_path)\n",
    "    cleaned_grid.set_value_at_point(loop_path[len(loop_path)//2], start_type)\n",
    "    \n",
    "    inside = 0\n",
    "    # sweep each row, from left to right. Flip when we hit an edge.\n",
    "    for row in cleaned_grid.array:\n",
    "        row = re.sub(\"L-*J|F-*7\", \"\", row) # ignore horizontal edge with no turn\n",
    "        internal_point = False\n",
    "        for char in row: # test each point in the row\n",
    "            if char in \"|FL\": # toggle wall and left edges only\n",
    "                internal_point = not internal_point # invert\n",
    "            if internal_point and char == \".\":\n",
    "                inside += 1\n",
    "            \n",
    "    return inside\n",
    "\n",
    "def d10_with_diagonal_ray_casting(grid: PipeGrid, furthest: Point, came_from: dict[Point, tuple]):\n",
    "    \"\"\" Determine number of tiles (which can be empty or non-loop pipe components) that are internal\n",
    "    to the main loop. Use ray casting lines, moving down and right. Ray cast from every point in the array.\n",
    "    When a ray intersects the edge of the polygon, we're inside. When they intersect again, we're outside. \n",
    "    So odd intersections means the point is inside. \"\"\"\n",
    "    loop_path = get_loop_path(grid, furthest, came_from) # get complete enclosed main loop\n",
    "    loop_points = set(loop_path)\n",
    "    cleaned_grid = remove_junk_parts(grid, loop_points)\n",
    "    \n",
    "    # update \"S\" to be actual pipe type\n",
    "    start_type = grid.infer_start_type(loop_path)\n",
    "    cleaned_grid.set_value_at_point(loop_path[len(loop_path)//2], start_type)\n",
    "        \n",
    "    inside = 0\n",
    "    # sweep diagonally, in the right/down direction\n",
    "    for y, row in enumerate(cleaned_grid.array):\n",
    "        for x, char in enumerate(row): # test a particular (x,y) and cast from here\n",
    "            if char != \".\": # we want to check whether a `.` is in or out; thus we can only cast from a `.`\n",
    "                continue\n",
    "            internal_point = False\n",
    "            next_y, next_x = y+1, x+1 # keep moving down and right\n",
    "            while next_y < cleaned_grid.height and next_x < cleaned_grid.width:\n",
    "                if cleaned_grid.value_at_point(Point(next_x, next_y)) in \"|-FJ\": # we can pass L and 7\n",
    "                    internal_point = not internal_point\n",
    "                \n",
    "                next_y += 1\n",
    "                next_x += 1\n",
    "            \n",
    "            if internal_point: # once we reach the end of the grid, decide whether the initial point was in or out\n",
    "                inside += 1\n",
    "            \n",
    "    return inside\n",
    "\n",
    "def d10_with_scale_up(grid: PipeGrid, furthest: Point, came_from: dict[Point, tuple]):\n",
    "    \"\"\" Determine number of tiles (which can be empty or non-loop pipe components) that are internal\n",
    "    to the main loop. We will scale-up the entire grid by 3x. So each point becomes 3x3. \n",
    "    We no longer need to care about pipe types, since each point can be represented by a wall. \"\"\"\n",
    "    loop_path = get_loop_path(grid, furthest, came_from) # get complete enclosed main loop\n",
    "    loop_points = set(loop_path)\n",
    "    cleaned_grid = remove_junk_parts(grid, loop_points)\n",
    "    # logger.debug(f\"\\n{cleaned_grid}\")\n",
    "    \n",
    "    # update \"S\" to be actual pipe type\n",
    "    start_type = grid.infer_start_type(loop_path)\n",
    "    cleaned_grid.set_value_at_point(loop_path[len(loop_path)//2], start_type)\n",
    "    \n",
    "    expanded_grid = [] # build grid that is 3x larger\n",
    "    for row in cleaned_grid.array:\n",
    "        new_rows = [[] for _ in range(3)] # create 3 empty rows\n",
    "        for char in row:\n",
    "            subgrid = [[\" \"]*3 for _ in range(3)] # create empty 3x3\n",
    "            # change parts of the 3x3 into wall\n",
    "            if char != \".\": # middle element must be a wall\n",
    "                subgrid[1][1] = \"#\"\n",
    "                if \"N\" in PipeGrid.directions_for_pipe[char]: # e.g. |\n",
    "                    subgrid[0][1] = \"#\"\n",
    "                if \"E\" in PipeGrid.directions_for_pipe[char]: # e.g. -\n",
    "                    subgrid[1][2] = \"#\"\n",
    "                if \"S\" in PipeGrid.directions_for_pipe[char]: # e.g. 7\n",
    "                    subgrid[2][1] = \"#\"\n",
    "                if \"W\" in PipeGrid.directions_for_pipe[char]: # e.g. 7\n",
    "                    subgrid[1][0] = \"#\"\n",
    "            \n",
    "            for i in range(3): # build out the triple-row horizontally, by appending 3x3 at a time\n",
    "                new_rows[i] += subgrid[i]\n",
    "        \n",
    "        for new_row in new_rows: # add the triple row\n",
    "            expanded_grid.append(\"\".join(new_row))\n",
    "    \n",
    "    # Visualise the expanded grid\n",
    "    # logger.debug(f\"\\n\" + \"\\n\".join(expanded_grid))\n",
    "    plot_grid(expanded_grid)\n",
    "    \n",
    "    # now we can flood fill from the outside\n",
    "    start_locn = (0, 0)\n",
    "    assert cleaned_grid.value_at_point(Point(*start_locn)) == \".\", \"Top left should be empty and outside\"\n",
    "    \n",
    "    outside = {start_locn}\n",
    "    queue = deque([start_locn])\n",
    "    while queue:\n",
    "        y, x = queue.popleft()\n",
    "        for dx, dy in VectorDicts.DIRS.values(): # for N, E, S, W\n",
    "            next_y, next_x = y + dy, x + dx\n",
    "            if (next_y, next_x) in outside: # already seen\n",
    "                continue\n",
    "            \n",
    "            if 0 <= next_y < len(expanded_grid) and 0 <= next_x < len(expanded_grid[0]):\n",
    "                if expanded_grid[next_y][next_x] != \"#\": # stop at a wall\n",
    "                    outside.add((next_y, next_x))\n",
    "                    queue.append((next_y, next_x))\n",
    "                    \n",
    "    # inside = (all - (loop points + outside points))\n",
    "    inside = 0\n",
    "    for y, row in enumerate(cleaned_grid.array):\n",
    "        for x, char in enumerate(row):\n",
    "            if Point(x, y) in loop_points:\n",
    "                continue\n",
    "            # convert normal grid to expanded grid. Add 1 because we want the one in the middle.\n",
    "            if (y*3+1, x*3+1) in outside: \n",
    "                continue\n",
    "            inside += 1\n",
    "\n",
    "    return inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"\\\n",
    "...........\n",
    ".S-------7.\n",
    ".|F-----7|.\n",
    ".||.....||.\n",
    ".||.....||.\n",
    ".|L-7.F-J|.\n",
    ".|..|.|..|.\n",
    ".L--J.L--J.\n",
    "...........\"\"\")\n",
    "\n",
    "sample_inputs.append(\"\"\"\\\n",
    "..........\n",
    ".S------7.\n",
    ".|F----7|.\n",
    ".||....||.\n",
    ".||....||.\n",
    ".|L-7F-J|.\n",
    ".|..||..|.\n",
    ".L--JL--J.\n",
    "..........\n",
    "\"\"\")\n",
    "\n",
    "sample_inputs.append(\"\"\"\\\n",
    ".F----7F7F7F7F-7....\n",
    ".|F--7||||||||FJ....\n",
    ".||.FJ||||||||L7....\n",
    "FJL7L7LJLJ||LJ.L-7..\n",
    "L--J.L7...LJS7F-7L7.\n",
    "....F-J..F7FJ|L7L7L7\n",
    "....L7.F7||L7|.L7L7|\n",
    ".....|FJLJ|FJ|F7|.LJ\n",
    "....FJL-7.||.||||...\n",
    "....L---J.LJ.LJLJ...\n",
    "\"\"\")\n",
    "\n",
    "sample_inputs.append(\"\"\"\\\n",
    "FF7FSF7F7F7F7F7F---7\n",
    "L|LJ||||||||||||F--J\n",
    "FL-7LJLJ||||||LJL-77\n",
    "F--JF--7||LJLJ7F7FJ-\n",
    "L---JF-JLJ.||-FJLJJ7\n",
    "|F|F-JF---7F7-L7L|7|\n",
    "|FFJF7L7F-JF7|JL---7\n",
    "7-L-JL7||F7|L7F-7F7|\n",
    "L.L7LFJ|||||FJL7||LJ\n",
    "L7JLJL-JLJLJL--JLJ.L\n",
    "\"\"\")\n",
    "\n",
    "sample_answers = [4, 4, 8, 10]\n",
    "\n",
    "def test_and_run_with_solve(solve:func):\n",
    "    for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "        curr_grid = parse_grid(curr_input.splitlines())\n",
    "        curr_pt1_soln, curr_furthest, curr_from = solve_part1(curr_grid, show_plot=False)\n",
    "        curr_pt2_soln = func(curr_grid, curr_furthest, curr_from)\n",
    "        validate(curr_pt2_soln, curr_ans) # test with sample data\n",
    "        logger.debug(\"Test passed\")\n",
    "\n",
    "    logger.info(\"All tests passed!\")\n",
    "\n",
    "    grid = parse_grid(input_data)\n",
    "    soln = func(grid, furthest, came_from)\n",
    "    logger.info(f\"Part 2 soln={soln}\")\n",
    "    \n",
    "# Shoelace + Pick's Theorem\n",
    "for func in (d10_with_shoelace_and_picks, \n",
    "             d10_with_horizontal_ray_casting,\n",
    "             d10_with_diagonal_ray_casting,\n",
    "             d10_with_scale_up):\n",
    "    logger.info(f\"Solving with {func.__name__}() ...\")\n",
    "    test_and_run_with_solve(func)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 11: Cosmic Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"11\" # replace with actual number (without leading digit)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 11 Part 1\n",
    "\n",
    "We have a 2D grid that represents galaxies `#` and empty space.  We need to get the shortest path between every pair of galaxies. But any rows or columns with no galaxies need to be duplicated!\n",
    "\n",
    "So this:\n",
    "\n",
    "```text\n",
    "...#......\n",
    ".......#..\n",
    "#.........\n",
    "..........\n",
    "......#...\n",
    ".#........\n",
    ".........#\n",
    "..........\n",
    ".......#..\n",
    "#...#.....\n",
    "```\n",
    "\n",
    "Becomes this:\n",
    "\n",
    "```text\n",
    "....#........\n",
    ".........#...\n",
    "#............\n",
    ".............\n",
    ".............\n",
    "........#....\n",
    ".#...........\n",
    "............#\n",
    ".............\n",
    ".............\n",
    ".........#...\n",
    "#....#.......\n",
    "```\n",
    "\n",
    "**My solution**:\n",
    "\n",
    "Grow the input according to the duplication rule:\n",
    "\n",
    "- Turn the 2D grid into a NumPy array, as this is faster for manipulating grids of data. I've converted all `#` to a `1` and all `.` to a `0`. This makes subsequent operations a bit easier. \n",
    "- For rows: iterate through each row check if a row contains `1`. If not, insert an extra row. I've parameterised this so that we can add an arbitrary number of rows. But for Part 1, we only ever need to add one additional row wherever a row contains no galaxies.\n",
    "- For cols: do the same, but transpose the array first, using the `ndarray transpose()` method. After transposing, each column now exists as a row.  So I can use the same function as I used for rows. After inserting the rows, we must transpose back.\n",
    "\n",
    "Now we work out where the galaxies are:\n",
    "\n",
    "- Determine all the locations in the array where the value is 1.\n",
    "\n",
    "```python\n",
    "    # Determine all locations with a galaxy and convert to Points\n",
    "    y, x = np.where(array == 1) # remember that 1 = #\n",
    "    hash_points = [Point(x, y) for (y, x) in zip(y, x)]\n",
    "```\n",
    "\n",
    "- For each of these locations return the list of `y` values and the list of `x` values.\n",
    "- Then I use [zip](https://aoc.just2good.co.uk/python/zip) to combine the two lists, to get a single list of `(y,x)` tuples. Then I use a [list comprehension](https://aoc.just2good.co.uk/python/comprehensions) to create a `Point` from each of these tuples.\n",
    "\n",
    "Next, get all the pairs of points using [`itertools.combinations()`](https://aoc.just2good.co.uk/python/perms_combos).\n",
    "\n",
    "- The `combinations()` function returns all unique combinations of points, but doesn't care about sequence. \n",
    "- I.e. if we have A -> B, it will not also return B -> A. \n",
    "- The instructions are pretty clear here: _\"Only count each pair once; order within the pair doesn't matter.\"_\n",
    "\n",
    "Now I find the  _Manhattan distance_ for each pair:\n",
    "\n",
    "- The Manhattan distance is the sum of horizontal and vertical distance, which is exactly what we want. \n",
    "- My `Point` class already knows how to do this.\n",
    "- Sum up the distances.\n",
    "\n",
    "Easy enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_rows(array: np.ndarray, insert_number) -> np.ndarray:\n",
    "    new_rows = []\n",
    "    for row in array:\n",
    "        new_rows.append(row) # add back the original row\n",
    "        if 1 not in row:\n",
    "            new_rows.extend(row for _ in range(insert_number))\n",
    "\n",
    "    return np.array(new_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(data, part=1, expansion_value:int=2):\n",
    "    # parse data into a NumPy array, swapping # for 1 and . for 0.\n",
    "    array_data = [[1 if char == '#' else 0 for char in line] for line in data]\n",
    "    array = np.array(array_data)\n",
    "    \n",
    "    insertions = expansion_value - 1 # we need to insert n-1 rows, given an expansion value\n",
    "    if part==1:\n",
    "        array = insert_rows(array, insertions) # triple rows\n",
    "        array = insert_rows(array.transpose(), insertions).transpose() # triple columns\n",
    "    \n",
    "    if part==2:\n",
    "        # work out which rows and cols have no #\n",
    "        empty_rows = np.where(np.all(array == 0, axis=1))[0]\n",
    "        empty_cols = np.where(np.all(array == 0, axis=0))[0]\n",
    "        \n",
    "    # Determine all locations with a galaxy and convert to Points\n",
    "    y, x = np.where(array == 1) # remember that 1 = #\n",
    "    hash_points = [Point(x, y) for (y, x) in zip(y, x)]\n",
    "\n",
    "    # get all unique pairs of points\n",
    "    point_dists = {} # store distances as { (point_a, point_b): distance, ... }\n",
    "    for (point_a, point_b) in combinations(hash_points, 2): # all combinations of 2 points   \n",
    "        point_dists[(point_a, point_b)] = point_a.manhattan_distance_from(point_b)\n",
    "        \n",
    "        if part == 2:\n",
    "            # Add insertion value for every row and every column that was empty, \n",
    "            # for each row and column that is between our two points\n",
    "            # Either point could be on the one to the left/right, above/below. So deterine min/max values.\n",
    "            for col in empty_cols:\n",
    "                if min(point_a.x, point_b.x) < col < max(point_a.x, point_b.x): # if this column between points\n",
    "                    point_dists[(point_a, point_b)] += insertions\n",
    "                    \n",
    "            for row in empty_rows:\n",
    "                if min(point_a.y, point_b.y) < row < max(point_a.y, point_b.y): # if this row between points\n",
    "                    point_dists[(point_a, point_b)] += insertions            \n",
    "            \n",
    "    sum_dists = sum(point_dists.values())\n",
    "    logger.debug(f\"{sum_dists=}\")\n",
    "    return sum_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_1 = \"\"\"...#......\n",
    ".......#..\n",
    "#.........\n",
    "..........\n",
    "......#...\n",
    ".#........\n",
    ".........#\n",
    "..........\n",
    ".......#..\n",
    "#...#.....\n",
    "\"\"\"\n",
    "sample_inputs = [sample_1.splitlines()]\n",
    "sample_answers = [374]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 11 Part 2\n",
    "\n",
    "Ah, if we're inserting millions of rows and columns, this isn't going to scale!! If we try creating a NumPy array where there are hundreds of millions of rows and hundreds of millions of columns, we'll run out of memory fast! We need to do something smarter.\n",
    "\n",
    "**My solution**:\n",
    "\n",
    "Rather than adding all the required extra rows and columns and then calculating distance, I will:\n",
    "\n",
    "- Determine the y values of all rows that are empty.\n",
    "- Determine the x values of all columns that are empty.\n",
    "- Calculate the distances between points in the original map.\n",
    "  - Using Manhattan distance, as before.\n",
    "  - Then determine if any of our empty rows and empty cols are between our two points. Note that we need to determine the min-x/min-y and max-x/max-y for each point, since we don't know which point will be on the left or above.\n",
    "  - For each empty row / empty column between our points, simply add the required expansion increment.\n",
    "  - Note that the instructions say _`each empty row should be replaced with 1000000 empty rows, and each empty column should be replaced with 1000000 empty columns.`_ **Replacing** is the key word.  So, for each empty row/column, we actually need to add n-1 to the Manhattan distance.\n",
    "\n",
    "When calculating if our rows or columns are empty, this code is interesting:\n",
    "\n",
    "```python\n",
    "        empty_rows = np.where(np.all(array == 0, axis=1))[0]\n",
    "        empty_cols = np.where(np.all(array == 0, axis=0))[0]\n",
    "```\n",
    "\n",
    "This works by saying:\n",
    "\n",
    " _\"Along the specified axis (either rows or columns), check if ALL the values in the array or 0. If they are, return the index values of those rows / columns.\"_\n",
    "\n",
    " So, not too bad in the end. A lot less painful than yesterday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [8410]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input, part=1, expansion_value=100), curr_ans) # test with sample data, part 1 approach\n",
    "    validate(solve(curr_input, part=2, expansion_value=100), curr_ans) # test with sample data, part 2 approach\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve(input_data, part=2, expansion_value=1000000)\n",
    "logger.info(f\"Soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Solution\n",
    "\n",
    "This solution is very efficient.\n",
    "\n",
    "- Read in the character values into a list of lists.\n",
    "- Create a cumulative sum array, to store the cumulative integer index of each row.\n",
    "  - Thus, each row index gives us the cumulative sum of all indexes up to (and including) this row.\n",
    "  - For each row, if it contains a galaxy, add 1 to the sum.\n",
    "  - Else, add the expansion value.\n",
    "- Now transpose with `zip(*grid)` and then build a col cumulative sum array in the same way.\n",
    "- Now we can get the vertical distance between any pair of galaxies by obtaining the difference\n",
    "  of the cumulative sums of their row indexes.\n",
    "- And we can get the horizontal distance between any pair of galaxies by obtaining the \n",
    "  differnce of the cumulative sums of their col indexes.\n",
    "\n",
    "This approach is efficient because to obtain a vertical or horizontal distance between two points, \n",
    "we only need to retrieve two index values from the array, and subtract one from the other. The cumulative sum array is ideal when we need to frequently obtain the sum of a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_with_cumulative_sum(data, expansion_value:int=2):\n",
    "    grid = [[char for char in line] for line in data]\n",
    "    # logger.debug(\"\\n\" + \"\\n\".join(\"\".join(line) for line in grid))\n",
    "    \n",
    "    row_sum_array = [0] # cumulative sum of row indexes\n",
    "    col_sum_array = [0] # cumulative sum of col indexes\n",
    "    for row in grid:\n",
    "        add_val = expansion_value if all(char == \".\" for char in row) else 1\n",
    "        row_sum_array.append(row_sum_array[-1] + add_val)\n",
    "        \n",
    "    cols = list(zip(*grid)) # transpose\n",
    "    for col in cols:\n",
    "        add_val = expansion_value if all(char == \".\" for char in col) else 1\n",
    "        col_sum_array.append(col_sum_array[-1] + add_val)\n",
    "    \n",
    "    hash_points = set()\n",
    "    for y, row in enumerate(grid):\n",
    "        for x, char in enumerate(row):\n",
    "            if char == \"#\":\n",
    "                hash_points.add((x,y))\n",
    "    \n",
    "    shortest_paths = []\n",
    "    for ((ax, ay), (bx, by)) in combinations(hash_points, 2): # all combinations of 2 points\n",
    "        shortest_paths.append(abs(row_sum_array[by]-row_sum_array[ay]) + \n",
    "                              abs(col_sum_array[bx]-col_sum_array[ax]))\n",
    "    \n",
    "    return sum(shortest_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"\"\"\\\n",
    "...#......\n",
    ".......#..\n",
    "#.........\n",
    "..........\n",
    "......#...\n",
    ".#........\n",
    ".........#\n",
    "..........\n",
    ".......#..\n",
    "#...#.....\n",
    "\"\"\"\n",
    "\n",
    "validate(solve_with_cumulative_sum(sample_input.splitlines(), expansion_value=2), 374) \n",
    "validate(solve_with_cumulative_sum(sample_input.splitlines(), expansion_value=100), 8410) \n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "logger.info(f\"Part 1 with cumulative sum={solve_with_cumulative_sum(input_data, expansion_value=2)}\")\n",
    "logger.info(f\"Part 2 with cumulative sum={solve_with_cumulative_sum(input_data, expansion_value=1000000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 12: Hot Springs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"12\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 12 Part 1\n",
    "\n",
    "The springs have fallen into disrepair. Our input is the condition records of which springs are damaged. But the input data is also damaged! We need to repair the damaged records.\n",
    "\n",
    "```text\n",
    "???.### 1,1,3\n",
    ".??..??...?##. 1,1,3\n",
    "?#?#?#?#?#?#?#? 1,3,1,6\n",
    "????.#...#... 4,1,1\n",
    "????.######..#####. 1,6,5\n",
    "?###???????? 3,2,1\n",
    "```\n",
    "\n",
    "Springs are arranged in rows. For each row, the condition record shows:\n",
    "\n",
    "- operational (`.`), damaged (`#`), or unknown (`?`) springs\n",
    "- followed by counts of contiguous groups of damaged springs, which accounts for every damaged spring\n",
    "- groups are always separated by at least one operational spring\n",
    "\n",
    "**For each row, count all of the different arrangements of operational and broken springs that meet the given criteria. What is the sum of those counts?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "We need to find all substitutions for `?` that result in valid records. For each `?` we can substitute either `.` or `#`. We need to substitute for every `?` in the record, and the substitutions are only valid if they match the constraints given by the counts of damanaged springs. \n",
    "\n",
    "For example, this record:\n",
    "\n",
    "```text\n",
    "?###???????? 3,2,1\n",
    "```\n",
    "\n",
    "Has 10 possible arrangements:\n",
    "\n",
    "```text\n",
    ".###.##.#...\n",
    ".###.##..#..\n",
    ".###.##...#.\n",
    ".###.##....#\n",
    ".###..##.#..\n",
    ".###..##..#.\n",
    ".###..##...#\n",
    ".###...##.#.\n",
    ".###...##..#\n",
    ".###....##.#\n",
    "```\n",
    "\n",
    "I start by parsing each row.  For each row, I create a `SpringRecord` [dataclass](https://aoc.just2good.co.uk/python/classes#dataclass). In this class:\n",
    "\n",
    "- The `damaged_groups` tuple defines the required lengths of contiguous blocks of `#`.\n",
    "- The count of possible valid arrangements is calculated by the [recursive](https://aoc.just2good.co.uk/python/recursion) `get_arrangements_count()` method.\n",
    "\n",
    "It works like this:\n",
    "\n",
    "- `char_idx` stores the current position in the record string. At each step, we decide what the character at this position could be: either `#` or `.`.\n",
    "- The `damaged_groups` tuple defines the required lengths of contiguous blocks of `#`. \n",
    "- The current group index (`curr_group_idx`) indicates which group (block of `#`) we are currently trying to complete.\n",
    "- `curr_group_len` keeps track of how many `#` characters have been consecutively placed in the current group.\n",
    "\n",
    "Together, these three pieces of information provide a complete snapshot of our progress through the string at any given recursion level. We know exactly where we are in the string, which group we are trying to fill, and how much of that group we have filled so far.\n",
    "\n",
    "- **Base case**\n",
    "  - When `char_idx` equals the length of the record, it means we've reached the end of the record.\n",
    "  - If `curr_group_idx` equals the length of `damaged_groups` and `curr_group_len` is `0`, all groups have been completed correctly, and this arrangement is valid. Return `1`, so we can add it to the count of valid arrangements.\n",
    "  - If `curr_group_idx` is the last group and `curr_group_len` equals the length of this last group, this is also a valid arrangement. Return `1`.\n",
    "  - In all other cases, the arrangement is invalid, so return `0`.\n",
    "\n",
    "- **Recursion**\n",
    "  - The function iterates over each character (using `char_idx`) in the record. For each character position, it considers both possible states (`.` and `#`).\n",
    "  - We check if the character at `char_idx` in the record is either the same as the state being considered, or a `?`. I.e. because if the current character in the record is a `.`, then `.` is the only valid `char` to try. If the current character in the record is a `#`, then `#` is the only valid `char` to try. If the current character in the record is a `?`, then we can try substituting either `.` or `#`. Thus, whenever we encounter `?`, our recursion branches.\n",
    "  - If the considered state is `.`, then there are two cases: 1) `curr_group_len` is `0`, which means we are  not currently counting a group of `#`, so we can safely proceed to the next character. And 2) `curr_group_len` equals the length of the current group in `damaged_groups`, meaning we've completed a group and can proceed to the next group.\n",
    "  - If the considered state is `#`, then we're extending (or beginning) the current group. We increment `curr_group_len` and proceed to the next character.\n",
    "\n",
    "  **Why store only these three state values, rather than the entire arrangement string?**\n",
    "  \n",
    "  Storing the entire arrangement string at each step of the recursion would significantly increase the solution space requirement. For a string of length `n`, there would potentially be 2**n different arrangements (each `?` can be either `#` or `.`), and each arrangement would need to be stored in memory. For larger strings, this just gets too big. (And would not work for Part 2, when we get to it!)\n",
    "\n",
    "  Instead, this approach stores just three integer values, which is sufficient to continue the recursion without storing the entire arrangement. It's important to understand that the state values (current character index, current group index, and length of the current group) do not represent a complete arrangement in themselves. Instead, they represent a _state_ within the process of building an arrangement. \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class SpringsRecord():\n",
    "    record: str\n",
    "    damaged_groups: tuple\n",
    "    \n",
    "    @cache # to cache, our SpringRecord must be immutable - hence frozen\n",
    "    def get_arrangements_count(self, char_idx: int, curr_group_idx: int, curr_group_len: int):\n",
    "        \"\"\" Determine how many arrangements are possible by recursion.\n",
    "        Move through the record one char at a time. \n",
    "        Each recursive call depth adds one to the char_idx.\n",
    "        Whenever we reach the end of the record, check if it has completed all the groups.\n",
    "        Our inputs represent a minimal state that defines where we are in the record, \n",
    "        which group we're processing, and the length the current group.\n",
    "\n",
    "        Args:\n",
    "            char_idx (int): current position in the record\n",
    "            curr_group_idx (int): current group in damaged_groups\n",
    "            curr_group_len (int): current length of damaged group\n",
    "\n",
    "        Returns:\n",
    "            int: count of arrangements\n",
    "        \"\"\"\n",
    "        # base case - check if we have a complete and valid arrangement\n",
    "        if char_idx == len(self.record): # if we're at the end, so all chars have been processed\n",
    "            if curr_group_idx == len(self.damaged_groups) and curr_group_len == 0:\n",
    "                # we've reached the end and we've processed the required number of damanged groups\n",
    "                return 1 # valid arrangement\n",
    "            elif curr_group_idx == len(self.damaged_groups) - 1 and self.damaged_groups[curr_group_idx] == curr_group_len:\n",
    "                # we're on the last char of the last group, and the group is complete\n",
    "                return 1 # valid arrangement\n",
    "            else: # we have not completed all groups, or current group length is too long\n",
    "                return 0 # invalid\n",
    "\n",
    "        # if we're here, we haven't reached the end of the record, so we need to recurse\n",
    "        # Whether the current char is any of [?.#], we still need to determine if a group has ended or started, and recurse to next char\n",
    "        count = 0\n",
    "        if self.record[char_idx] in \".?\": # Recurse with a .\n",
    "            # We need to be either extending the operational section or adding a . after a damaged group\n",
    "            if curr_group_len == 0: # we're not in a damaged group, so we must be extending\n",
    "                count += self.get_arrangements_count(char_idx+1, curr_group_idx, 0)\n",
    "            elif (curr_group_idx < len(self.damaged_groups) and \n",
    "                  curr_group_len == self.damaged_groups[curr_group_idx]):\n",
    "                # we're adding a . after a #, so the group is now complete; move on to next group\n",
    "                count += self.get_arrangements_count(char_idx+1, curr_group_idx+1, 0)\n",
    "            else: # Invalid configuration, which is one of:\n",
    "                # Either: curr_group_len is too long\n",
    "                # Or: we're adding . before completing a group\n",
    "                pass\n",
    "        \n",
    "        if self.record[char_idx] in \"#?\": # Recurse with a #\n",
    "            # we're adding a # so extend the current damaged group, or start a new damaged group\n",
    "            count += self.get_arrangements_count(char_idx+1, curr_group_idx, curr_group_len+1)\n",
    "        \n",
    "        return count          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_records(data) -> list[SpringsRecord]:\n",
    "    spring_records = []\n",
    "    for line in data:\n",
    "        record_part, groups_part = line.split()\n",
    "        spring_records.append(SpringsRecord(record_part, tuple([int(x) for x in groups_part.split(\",\")])))\n",
    "        \n",
    "    return spring_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Part 2\n",
    "def unfold(record: SpringsRecord, replica_count:int=5) -> SpringsRecord:\n",
    "    \"\"\" \n",
    "    Replace record with n copies of the record, separated by ? \n",
    "    Replace the damaged groups with a version that is n*current \n",
    "    \"\"\"\n",
    "    new_rec = \"?\".join(replica_count*[record.record])\n",
    "    new_groups = replica_count*record.damaged_groups\n",
    "    return SpringsRecord(new_rec, new_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(records: list[SpringsRecord], part:int=1):\n",
    "    counts = 0\n",
    "    for record in records:\n",
    "        if part==2:\n",
    "            record = unfold(record)\n",
    "        count = record.get_arrangements_count(0, 0, 0)\n",
    "        if logger.getEffectiveLevel() == logging.DEBUG: # avoid wasted compute effort\n",
    "            logger.debug(f\"{record=}, {count=}\")\n",
    "        counts += count\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"\\\n",
    "???.### 1,1,3\n",
    ".??..??...?##. 1,1,3\n",
    "?#?#?#?#?#?#?#? 1,3,1,6\n",
    "????.#...#... 4,1,1\n",
    "????.######..#####. 1,6,5\n",
    "?###???????? 3,2,1\"\"\")\n",
    "sample_answers = [21]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_records = parse_records(curr_input.splitlines())\n",
    "    validate(solve(curr_records), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "records = parse_records(input_data)\n",
    "soln = solve(records)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 12 Part 2\n",
    "\n",
    "Now we have to replace each record string with a record string that is five copies of the original, separated by `?`. So this:\n",
    "\n",
    "```text\n",
    ".# 1\n",
    "```\n",
    "\n",
    "Becomes\n",
    "\n",
    "```text\n",
    ".#?.#?.#?.#?.# 1,1,1,1,1\n",
    "```\n",
    "\n",
    "And similarly, we have to replace the damaged springs groups with a new list of groups, that is five copies of the original.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- Go through each `SpringRecord` we created before, and replicate the two fields as required.\n",
    "\n",
    "  - For the `damaged_groups`, we can simply multiply them by 5 to create the new tuple. Python is clever like that!\n",
    "  - For the `record` strings, we have to first convert each string to a `list` containing our string. Then we can multiply the list five times to result in a list with five members. Then we can use Python's `join()` method, to join the five string members together, putting `?` in between.\n",
    "\n",
    "- Then, we can run the same code as for Part 1, but with a couple of tweaks.\n",
    "\n",
    "  - I've added `@cache` to the `get_arrangements_count()` method.  This caches any results for any states that we have seen before. And this means that for every recursion, we will have cached all the states that led to this recursion. This approach is called _memoization_. This works because our state is minimal (there are far fewer combinations of [char position, group number, group length] than there are of [record combinations]), and this means we can effectively cache each state with each recursed arrangement count.\n",
    "  - To make this work, I have to ensure that my _state_ is _hashable_. I.e. this means that all the parameters I pass to the `get_arrangements_count()` method must themselves be hashable. In my original Part 1, my `SpringRecord` class was not hashable, because my class was mutable. I fixed this by 1) changing my `damaged_groups` from a `list` to a `tuple`, and 2) by adding `(frozen=True)` to my `@dataclass` decorator. This is sufficient to make my `SpringRecord` hashable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [525152]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_records = parse_records(curr_input.splitlines())\n",
    "    validate(solve(curr_records, part=2), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve(records, part=2)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 13: Point of Incidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"13\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 13 Part 1\n",
    "\n",
    "We're given several 2D patterns of ash (`.`) and rocks (`#`) as input.  Each pattern will have a line of symmetry (reflection), meaning that a pair of columns or a pair of rows will be identical.  The line of symmetry will not necessarily be in the middle of the pattern, so we can ignore rows / columns that are not reflected.\n",
    "\n",
    "The input takes the form of many blocks, e.g.\n",
    "\n",
    "```text\n",
    "#.##..##.\n",
    "..#.##.#.\n",
    "##......#\n",
    "##......#\n",
    "..#.##.#.\n",
    "..##..##.\n",
    "#.#.##.#.\n",
    "\n",
    "#...##..#\n",
    "#....#..#\n",
    "..##..###\n",
    "#####.##.\n",
    "#####.##.\n",
    "..##..###\n",
    "#....#..#\n",
    "```\n",
    "\n",
    "_\"To find the reflection in each pattern, you need to find a perfect reflection across **either a horizontal line between two rows or across a vertical line between two columns.**\"_\n",
    "\n",
    "We're asked to:\n",
    "\n",
    "- Add up the number of columns to the left of each vertical line of reflection\n",
    "- Add 100 multiplied by the number of rows above each horizontal line of reflection\n",
    "\n",
    "**What is the total number?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "First, parse the input data.\n",
    "\n",
    "- Split into blocks at each empty line, which we can do with `data.split(\"\\n\\n\")`.\n",
    "- Then, for each block, split into a list of lines, and for each line, convert to a list that contains `1` in place of `#` and `0` in place of `.`.  (I could have built my array using `#` and `.` instead of `1` and `0`. But I converted just in case it made life easier later.)\n",
    "- Turn each block into a 2D [NumPy](https://aoc.just2good.co.uk/python/numpy) array, because it's easier to work with rows and columns in NumPy.\n",
    "\n",
    "Now we can iterate through each array, and find either the vertical or horizontal symmetry line. I do this with a function called `find_symmetry()`, which takes the `pattern ndarray` and the `axis` as a parameter.  In a 2D NumPy array, `axis==0` for rows, and `==1` for columns.\n",
    "\n",
    "- If we want columns, we transpose the array. This turns columns into rows, so that we can then use the same symmetry checking logic in both cases.\n",
    "- Then, in my original part 1, I compared the rows (or transposed columns) like this: \n",
    "\n",
    "```python\n",
    "np.array_equal(array[row_num], array[row_num + 1])\n",
    "```\n",
    "\n",
    "- This returns `True` if row `n` is identical to row `n+1`.\n",
    "- If the rows are identical, I then determine how many rows remain, after the symmetry line.\n",
    "- I then iterate through all remaining rows outwards from the symmetry line. For example, if the symmetry line were between rows 5 and 6, then the next pair of rows would be 4 and 7, then 3 and 8, and so on.\n",
    "- If we get to either end of the array and all the pairs contain identical rows, then we simply return the index of the current row, plus 1. We add `1` because we want the row count to include the row that was immediately left of / above the symmetry line. Thus, if the row index was `3`, then we would want the row count to be the count of rows: `0, 1, 2, 3`, i.e. `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_patterns(data) -> list[np.ndarray]:\n",
    "    pattern_blocks = data.split(\"\\n\\n\")\n",
    "    return [np.array([[1 if char == '#' else 0 for char in line] \n",
    "                         for line in block.splitlines()]) for block in pattern_blocks]\n",
    "\n",
    "def find_symmetry(array: np.ndarray, axis: int, diffs_required: int=0) -> int:\n",
    "    \"\"\" Find the line of symmetry, and return the count of rows or columns before the line of symmetry.\n",
    "       \n",
    "    Args:\n",
    "        array (np.ndarray): The 2D array\n",
    "        axis (int): 0 for rows, 1 for cols\n",
    "        diffs_required (int): used for part 2, to allow n differences between rows. Default=0.\n",
    "        \n",
    "    Returns 0 if no symmetry.\n",
    "    \"\"\"\n",
    "    if axis == 1: # we want columns, so transpose the array\n",
    "        array = array.T\n",
    "    \n",
    "    for row_num in range(len(array) - 1):\n",
    "        # Compare current row with the next row\n",
    "        diffs = np.sum(array[row_num] != array[row_num+1])\n",
    "        if diffs <= diffs_required: # if no diffs, then this row is identical to the next row\n",
    "            rows_after = len(array) - (row_num + 2)\n",
    "            for i in range(rows_after): # check symmetry of each remaining row\n",
    "                if row_num-i == 0: # we've run out of rows in the first half, or symmetry at first row\n",
    "                    break\n",
    "                # For Part 1, this is sufficient:\n",
    "                # if not np.array_equal(array[row_num-1-i], array[row_num+2+i]):\n",
    "                diffs += np.sum(array[row_num-1-i] != array[row_num+2+i])\n",
    "                if diffs > diffs_required:\n",
    "                    break\n",
    "                \n",
    "            if diffs == diffs_required:\n",
    "                return row_num + 1 # the row count before the line of symmetry, which is equivalent to row index+1\n",
    "            else:\n",
    "                continue # move on to next row\n",
    "    \n",
    "    return 0 # no symmetry found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(patterns: list, part:int=1):\n",
    "    diffs_required = 0 if part==1 else 1\n",
    "    \n",
    "    rows_above_symmetry = 0\n",
    "    cols_left_of_symmetry = 0\n",
    "    for pattern in patterns:\n",
    "        rows_above_symmetry += find_symmetry(pattern, axis=0, diffs_required=diffs_required) \n",
    "        cols_left_of_symmetry += find_symmetry(pattern, axis=1, diffs_required=diffs_required)\n",
    "    \n",
    "    answer = cols_left_of_symmetry + (100*rows_above_symmetry)\n",
    "    logger.debug(f\"{rows_above_symmetry=}, {cols_left_of_symmetry=}, {answer=}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "\n",
    "# Samples from problem\n",
    "sample_inputs.append(\"\"\"#.##..##.\n",
    "..#.##.#.\n",
    "##......#\n",
    "##......#\n",
    "..#.##.#.\n",
    "..##..##.\n",
    "#.#.##.#.\n",
    "\n",
    "#...##..#\n",
    "#....#..#\n",
    "..##..###\n",
    "#####.##.\n",
    "#####.##.\n",
    "..##..###\n",
    "#....#..#\"\"\")\n",
    "\n",
    "# moving symmetry to top half and left half\n",
    "sample_inputs.append(\"\"\".##..##.#\n",
    ".#.##.#..\n",
    "#......##\n",
    "#......##\n",
    ".#.##.#..\n",
    ".##..##..\n",
    ".#.##.#.#\n",
    "\n",
    "#....#..#\n",
    "..##..###\n",
    "#####.##.\n",
    "#####.##.\n",
    "..##..###\n",
    "#....#..#\n",
    "#...##..#\"\"\")\n",
    "\n",
    "# testing symmetry at the edges\n",
    "sample_inputs.append(\"\"\"..##.#\n",
    "##.#..\n",
    "....##\n",
    "....##\n",
    "##.#..\n",
    "..##..\n",
    "##.#.#\n",
    "\n",
    "#####.##.\n",
    "#####.##.\n",
    "..##..###\n",
    "#....#..#\n",
    "#...##..#\"\"\")\n",
    "                     \n",
    "sample_answers = [405, 304, 101]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_patterns = parse_patterns(curr_input)\n",
    "    validate(solve(curr_patterns), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "patterns = parse_patterns(input_data)\n",
    "soln = solve(patterns)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 13 Part 2\n",
    "\n",
    "Every mirror has _exactly one_ smudge, which reverses a single `.` or `#` in our array.\n",
    "\n",
    "We're asked to change one smudge (`#`) in order to create a new line of reflection. I.e. for every 2D array, there will be a single `#` that we can change to a `.` that introduces a line of symmetry.  Note: this may or may not break the original line of symmetry.\n",
    "\n",
    "**In each pattern, fix the smudge and find the different line of reflection. What number do you get after summarizing the new reflection line in each pattern in your notes?**\n",
    "\n",
    "The crucial observation here is that we must calculate our answer based _only_ on the _new_ line of symmetry.  The existing line should be ignored.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "We could substitute each `#` for `.`, and repeat our symmetry check for each substitution.  But that would be pretty slow.\n",
    "\n",
    "Better solution: look for a line of symmetry where there is exactly one mismatch. I.e. because if exactly one mirror needs to change, then we know that this mirror will be the only mismatch for the second line of symmetry.\n",
    "\n",
    "This requires very little change to the code:\n",
    "\n",
    "- I've added a `part` parameter to the `solve` function.\n",
    "- When `part` is set to `2`, we set a variable called `diffs_required` to `1`; otherwise, we set it to `0`.\n",
    "- We call `find_symmetry()` like we did for Part 1, but this time we supply the `diffs_required` parameter.\n",
    "\n",
    "The `find_symmetry()` function has been updated. Instead of this:\n",
    "\n",
    "```python\n",
    "    if np.array_equal(array[row_num], array[row_num + 1]):\n",
    "```\n",
    "\n",
    "We now do this:\n",
    "\n",
    "```python\n",
    "    diffs = np.sum(array[row_num] != array[row_num+1])\n",
    "    if diffs <= diffs_required:\n",
    "```\n",
    "\n",
    "Here `np.sum()` calculates the sum of all the elements that differ between the two supplied rows.\n",
    "\n",
    "When `diffs_required` is `0`, the result is the same as Part 1.  But when `diffs_required` is `1`, this now checks for symmetry, starting with 0 or 1 differences. Then, as we check each pair of rows, we keep track of the number of differences so far.\n",
    "\n",
    "- If `diffs` exceeds `1`, then we're bust.\n",
    "- If `diffs` is exactly `1` after completing our symmetry check, then the requirement for Part 2 has been satisfied, and we return the row / column index, as before.\n",
    "\n",
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "\n",
    "# Samples from problem\n",
    "sample_inputs.append(\"\"\"#.##..##.\n",
    "..#.##.#.\n",
    "##......#\n",
    "##......#\n",
    "..#.##.#.\n",
    "..##..##.\n",
    "#.#.##.#.\n",
    "\n",
    "#...##..#\n",
    "#....#..#\n",
    "..##..###\n",
    "#####.##.\n",
    "#####.##.\n",
    "..##..###\n",
    "#....#..#\"\"\")\n",
    "\n",
    "sample_answers = [400]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_patterns = parse_patterns(curr_input)    \n",
    "    validate(solve(curr_patterns, part=2), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve(patterns, part=2)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation - Using Binary Values\n",
    "\n",
    "In my solution, I already turned my grid in to a binary grid, i.e. with `1` and `0`. So I can  significantly improve the performance of row comparisons by simply comparing the binary value of each row. \n",
    "\n",
    "- Any unique row has have a unique binary value.\n",
    "- So turn all the rows into a list of binary values, and the columns into a list of binary values.\n",
    "- If we find a pair of rows that are identical, we've got a potential reflection line.\n",
    "- Take all the numbers before the line of symmetry (ignoring the symmetry row we've already found), and call it the first block. And do the second after the line of symmetry, and call it the second block.\n",
    "- Now reverse the order of the first block.\n",
    "- Now we can iterate through the zip of the two blocks.  The zip finishes when either block has run out of elements. Compare each pair, and check if they are the same.\n",
    "\n",
    "For Part 2, we can then:\n",
    "\n",
    "- `XOR` the two rows. `XOR` returns a `1` if the bit values are different, otherwise `0`. Thus, we can use this value to determine how many bits did not match.\n",
    "- Turn the result of the XOR into a string using the `bin()` function. Then count how many `1` are in the string.  This gives us our count of differences.\n",
    "- We want to return the row indexes, ONLY if we have exactly one difference overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_symmetry_bin(array: np.ndarray, axis: int=0, diffs_required=0):\n",
    "    \"\"\" Find the line of symmetry, and return the count of rows or columns before the line of symmetry.\n",
    "       \n",
    "    Args:\n",
    "        array (np.ndarray): The 2D array\n",
    "        axis (int): 0 for rows, 1 for cols\n",
    "        diffs_required (int): used for part 2, to allow n differences between rows. Default=0.\n",
    "        \n",
    "    Returns 0 if no symmetry.\n",
    "    \"\"\"\n",
    "    if axis == 1: # we want columns, so transpose the array\n",
    "        array = array.T\n",
    "    \n",
    "    # turn 2D grid into binary sums of each row\n",
    "    # e.g. [358, 90, 385, 385, 90, 102, 346]\n",
    "    rows = [int(''.join(str(bit) for bit in row), 2) for row in array]\n",
    "    logger.debug(rows)\n",
    "\n",
    "    for i in range(1, len(rows)):\n",
    "        diffs_found = 0\n",
    "        xor_val = bin(rows[i] ^ rows[i - 1]) # get XOR, which is only 1 where bits are different\n",
    "        diffs_found += xor_val.count(\"1\") # count how many bits were different\n",
    "        if diffs_found <= diffs_required: # for Part 1, we could just do 'if rows[i] == rows[i - 1]:'\n",
    "            first_block = rows[0:i-1][::-1]\n",
    "            second_block = rows[i+1:]\n",
    "\n",
    "            # zip for as many items are present in the shortest block\n",
    "            for left, right in zip(first_block, second_block):\n",
    "                xor_val = bin(left ^ right)\n",
    "                diffs_found += xor_val.count(\"1\")\n",
    "                if diffs_found > diffs_required: # too many diffs, so we've gone bust\n",
    "                    break\n",
    "\n",
    "            if diffs_found == diffs_required: # we need EXACTLY this many diffs before returning\n",
    "                return i\n",
    "       \n",
    "    return 0 # no symmetry found\n",
    "\n",
    "def solve_with_bin(patterns: list, diffs_required:int=0):\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    rows_above_symmetry = 0\n",
    "    cols_left_of_symmetry = 0\n",
    "    for pattern in patterns:\n",
    "        rows_above_symmetry += find_symmetry_bin(pattern, axis=0, diffs_required=diffs_required) \n",
    "        cols_left_of_symmetry += find_symmetry_bin(pattern, axis=1, diffs_required=diffs_required)\n",
    "    \n",
    "    answer = cols_left_of_symmetry + (100*rows_above_symmetry)\n",
    "    logger.debug(f\"{rows_above_symmetry=}, {cols_left_of_symmetry=}, {answer=}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_input = \"\"\"\\\n",
    "#.##..##.\n",
    "..#.##.#.\n",
    "##......#\n",
    "##......#\n",
    "..#.##.#.\n",
    "..##..##.\n",
    "#.#.##.#.\n",
    "\n",
    "#...##..#\n",
    "#....#..#\n",
    "..##..###\n",
    "#####.##.\n",
    "#####.##.\n",
    "..##..###\n",
    "#....#..#\"\"\"\n",
    "\n",
    "patterns = parse_patterns(input_data)\n",
    "\n",
    "curr_patterns = parse_patterns(sample_input)\n",
    "validate(solve_with_bin(curr_patterns), 405) # Part 1\n",
    "validate(solve_with_bin(curr_patterns, diffs_required=1), 400) # Part 2\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "logger.info(f\"Part 1 soln={solve_with_bin(patterns)}\")\n",
    "logger.info(f\"Part 2 soln={solve_with_bin(patterns, diffs_required=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 14: Parabolic Reflector Dish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"14\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 14 Part 1\n",
    "\n",
    "We have rocks attached to pulleys which are used to focus the mirrors in our reflector dish. The rocks are located in a metal grid:\n",
    "\n",
    "- Rounded rocks (`O`) which can roll when the platform is tilted.\n",
    "- Cube rocks (`#`) whcih stay in place.\n",
    "- Empty spaces (`.`).\n",
    "\n",
    "E.g.\n",
    "\n",
    "```text\n",
    "O....#....\n",
    "O.OO#....#\n",
    ".....##...\n",
    "OO.#O....O\n",
    ".O.....O#.\n",
    "O.#..O.#.#\n",
    "..O..#O..O\n",
    ".......O..\n",
    "#....###..\n",
    "#OO..#....\n",
    "```\n",
    "\n",
    "When tilted north such that all round rolls roll to the top, total load is calculated as: the number of rows from a `O` to the south edge of the platform, inclusive of its own row. `#` don't contribute to load.\n",
    "\n",
    "**Tilt the platform so that the rounded rocks all roll north. Afterward, what is the total load on the north support beams?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- Read in the data and convert to a NumPy grid. This will make life much easier if we have to do any sort of flipping or rotating of our grid later!\n",
    "- Keep track of `highest_occupied` position in a dictionary of `{ col: row }`. Initially, we will set the highest occopied position of each column to be `-1`, to represent the row index above the start of our grid.  (Since our first grid row will be row number `0`.)\n",
    "- Now, iterate through each row, starting at the top.\n",
    "- For each row, iterate through each element.\n",
    "  - If the current element is `O` (i.e. a round rock), then we need to try to roll it to the top.\n",
    "  - The highest row we can roll it to will be `highest_occupied[col_num]`, plus `1`. Store this is our `target_row`.\n",
    "  - If our `target_row` is different to our current row, then we can move this stone. Set the current stone position to empty, and then place the stone in this column in the `target_row`. Then, update our `highest_occupied` value for this column.\n",
    "  - Finally, check if the element we're on is not empty. It may not be empty if it contains a `#`, or if we were unable to roll a stone stored here. If it's not empty, update `highest_occupied` for this columnn to be this row.\n",
    "- Once this outer loop finishes, we've finished our tilt. Now we just need to calculate the `total_load`:\n",
    "  - First, I use `np.sum()` to retrieve the counts of `O` in each row. We return these as a single dimension array of counts.\n",
    "  - Then, for each value in this list of counts, I simply multiply the value by the difference between the height of our grid, and the current count index position (which represents the row number).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input(data) -> np.ndarray:\n",
    "    return np.array([list(row) for row in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tilt_north(grid: np.ndarray) -> np.ndarray:\n",
    "    # { col_num: row_num } Initialise to -1, i.e. the row above the grid\n",
    "    highest_occupied = { col_num: -1 for col_num, _ in enumerate(grid[0]) }\n",
    "    \n",
    "    for row_num, row in enumerate(grid):\n",
    "        for col_num, val in enumerate(row):\n",
    "            if val == \"O\": # roll this rock up to highest occupied + 1\n",
    "                target_row = highest_occupied[col_num] + 1\n",
    "                if target_row != row_num: # we need to move this rock\n",
    "                    grid[target_row,col_num] = \"O\" # move it to this row\n",
    "                    grid[row_num,col_num] = \".\" # and set the current row empty\n",
    "                    highest_occupied[col_num] = target_row\n",
    "\n",
    "            # val may have changed\n",
    "            if grid[row_num,col_num] != \".\": # I.e. if it is a O or a #\n",
    "                highest_occupied[col_num] = row_num\n",
    "\n",
    "    return grid    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(grid: np.ndarray):\n",
    "    logger.debug(f\"\\n{grid}\")\n",
    "\n",
    "    grid = tilt_north(grid)\n",
    "    row_counts = np.sum(grid==\"O\", axis=1)\n",
    "    total_load = sum((grid.shape[0]-row)*val for (row, val) in enumerate(row_counts))\n",
    "\n",
    "    logger.debug(f\"{total_load=}\")\n",
    "    return total_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"O....#....\n",
    "O.OO#....#\n",
    ".....##...\n",
    "OO.#O....O\n",
    ".O.....O#.\n",
    "O.#..O.#.#\n",
    "..O..#O..O\n",
    ".......O..\n",
    "#....###..\n",
    "#OO..#....\"\"\")\n",
    "sample_answers = [136]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_grid = parse_input(curr_input.splitlines())\n",
    "    validate(solve_part1(curr_grid), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "grid = parse_input(input_data)\n",
    "soln = solve_part1(grid)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 14 Part 2\n",
    "\n",
    "Now we need to move the rocks to the edges. \n",
    "\n",
    "The platform performs tilt cycles.  Each cycle tilts the platform: N, W, S, E.\n",
    "We're asked to perform 1000000000 cycles!!\n",
    "\n",
    "Well, I can't just scale up the above. That's too many cycles. I need to be a bit more clever!\n",
    "\n",
    "My guess is that after a number of cycles, we'll end up with a loop of repeating cycles. I need to find out when I've seen this cycle before.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- Each cycle requires us to tilt N, W, S, E.\n",
    "  - We already know how to tilt north. So one way we can tilt west is to rotate our grid by 90 degrees clockwise - which results in the original west now pointing north - and then tilting north again. And from there, if we rotate 90 degrees clockwise again and tilt north, this is equivalent to performing a tilt south. Repeat again from E. Then rotate one more time to bring us back to facing north.\n",
    "  - [NumPy](https://aoc.just2good.co.uk/python/numpy) already provides a convenient method to rotate our grid: `np.rot90(grid, -1)`. Note: `-1` means clockwise. If we omit this, then the grid would rotate counterclockwise.\n",
    "- As I mentioned before, it's my expectation that after a number of initial cycles, the cycles will settle into a repeating pattern, where we will see the same configuration over and over again, every n cycles. So I need to keep track of all the post-cycle grid configurations I've' seen so far, until the point we see a grid configuration we've seen before.\n",
    "  - I can do this by storing each grid configuration in a dictionary _cache_ (along with the current cycle number as the value).\n",
    "  - However, the grid itself is not hashable, so I can convert it into a `tuple` (which is immutable) before storing it in the dictionary.\n",
    "  - Then, every time I perform a cycle, I check whether the resulting grid configuration is in the _cache_. If I haven't seen it before, I add this configuration to the _cache_. If I have seen it before, then I've found my first repeat.\n",
    "  - Having found the repeat, I can determine how many cycles are required between each repeat of configuration. I store this as `repeat_len`.\n",
    "\n",
    "Check this example:\n",
    "\n",
    "![Repeating configurations](https://aoc.just2good.co.uk/assets/images/repeating_configs.png)\n",
    "\n",
    "Here, the blue circles represent the grid configuration at the end of each cycle, where we haven't yet seen a repeat.  But eventually, we find that the green-yellow-red circles repeat.  So here we have a repeat length of 3.\n",
    "\n",
    "So, to find the nth configuration (which could be 999, 1000000, 1000000000, or whatever), then we:\n",
    "\n",
    "- Determine how many cycles are required to get to n from our current cycle position. I store this as `remaining_cycles`. In our example, our current cycle position is just after the second green, since the second green represents the first detection of a repeated config.\n",
    "- Divide `remaining_cycles` by the repeat length and obtain the remainder. This remainder gives us the number of cycles we need to execute from here, in order to create the configuration that will be the same as the configuration at n. I call this `additional_cycles`.\n",
    "- For example, `additional_cycles` were equal to 0, then it means that the nth grid would be a green. If it were equal to 1, then the nth grid would be a yellow. If it were 2, then it would be a red.\n",
    "\n",
    "For my real data, my logging output shows:\n",
    "\n",
    "```text\n",
    "Current iteration 131; last seen at 80; repeat_len=51\n",
    "additional_cycles=28\n",
    "```\n",
    "\n",
    "So, in total, I've only had to perform the cycle `131+28` times!  Which is a lot fewer than `1000000000`!!\n",
    "\n",
    "**Footnote:**\n",
    "\n",
    "When I was looking at this problem, it was immediately apparent to me that repeating cycles were likely. But you may be thinking... _\"Why would you think that?  Why did this solution occur to you?\"_\n",
    "\n",
    "Well, I guess it's partly intuition around what is likely to happen.  But perhaps more significantly... I've done a few AoCs now. And this isn't the first time we've seen a repeating cycle problem. So I think it's fair to say that a bit of AoC experience will help you quickly make an educated guess about what you need to do.  And if you haven't done AoC before (or similar), then this conclusion is less likely to be obvious.\n",
    "\n",
    "But don't sweat it.  This is where practice makes you better next time.\n",
    "\n",
    "(And I know that at my very best, I'm only mediocre!!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tilt_cycle(grid):\n",
    "    \"\"\" \n",
    "    Tilt N, S, E and W. \n",
    "    Do this by tiling north, and then rotate 90 degrees CW.\n",
    "    Repeat tile+rotate three times.  This brings us back to original orientation (north).\n",
    "    \"\"\"\n",
    "    for _ in range(4): # N, W, S, E\n",
    "        grid = tilt_north(grid)\n",
    "        grid = np.rot90(grid, -1) # clockwise\n",
    "        \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(grid: np.ndarray):\n",
    "    logger.debug(f\"\\n{grid}\")\n",
    "    required_cycles = 1000000000\n",
    "    repeat_len = 1\n",
    "\n",
    "    grid_cache = {}\n",
    "    for i in range(required_cycles):\n",
    "        grid = tilt_cycle(grid)\n",
    "\n",
    "        grid_top = tuple(map(tuple, grid)) # cache the grid as a tuple\n",
    "        if grid_top in grid_cache:\n",
    "            prev_iteration = grid_cache[grid_top]\n",
    "            repeat_len = i - prev_iteration\n",
    "            logger.debug(f\"Current iteration {i}; last seen at {prev_iteration}; {repeat_len=}\")\n",
    "            break\n",
    "\n",
    "        grid_cache[grid_top] = i\n",
    "\n",
    "    remaining_cycles = required_cycles - (i+1)\n",
    "    additional_cycles = remaining_cycles % repeat_len\n",
    "    logger.debug(f\"{additional_cycles=}\")\n",
    "\n",
    "    for i in range(additional_cycles):\n",
    "        grid = tilt_cycle(grid)\n",
    "\n",
    "    logger.debug(f\"\\n{grid}\")\n",
    "    row_counts = np.sum(grid==\"O\", axis=1)\n",
    "    total_load = sum((grid.shape[0]-row)*val for (row, val) in enumerate(row_counts))\n",
    "\n",
    "    logger.debug(f\"{total_load=}\")\n",
    "    return total_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [64]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_grid = parse_input(curr_input.splitlines())\n",
    "    validate(solve_part2(curr_grid), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "grid = parse_input(input_data)\n",
    "soln = solve_part2(grid)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 15: Lens Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"15\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 15 Part 1\n",
    "\n",
    "We need to turn a bunch of strings into hashes using a special _HASH algorithm_. Our sample data is a set of comma-delimited strings:\n",
    "\n",
    "`rn=1,cm-,qp=3,cm=2,qp-,pc=4,ot=9,ab=5,pc-,pc=6,ot=7`\n",
    "\n",
    "For each string, our hash will generate a single number in the range 0 to 255. It works like this:\n",
    "\n",
    "- Start with a current value of 0.\n",
    "- Determine the ASCII code for the current character of the string.\n",
    "- Increase the current value by the ASCII code you just determined.\n",
    "- Set the current value to itself multiplied by 17.\n",
    "- Set the current value to the remainder of dividing itself by 256.\n",
    "- Repeat for each character in the string.\n",
    "\n",
    "We need to do this for a collection of strings, and compute the sum.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "Part 1 is a very trivial problem. I've just written a simple function that executes the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hash(step_str: str) -> int:\n",
    "    curr_val = 0\n",
    "    for char in step_str:\n",
    "        curr_val = (curr_val + ord(char)) * 17 % 256\n",
    "        \n",
    "    return curr_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data):\n",
    "    strings = data[0].split(\",\")\n",
    "    logger.debug(f\"{strings}\")\n",
    "    \n",
    "    hashes = [compute_hash(step) for step in strings]\n",
    "    return sum(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"rn=1,cm-,qp=3,cm=2,qp-,pc=4,ot=9,ab=5,pc-,pc=6,ot=7\")\n",
    "sample_answers = [1320]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 15 Part 2\n",
    "\n",
    "Well, it took me a while to read and understand the instructions!  But once I understood them and started writing code, this part wasn't too bad either.\n",
    "\n",
    "We have a series of 256 boxes, arranged in series.\n",
    "\n",
    "- The boxes have holes to allow light through, and contain _multiple lenses_ for focussing. \n",
    "- The lenses can be removed, moved and replaced.\n",
    "- We have a library of lenses to choose from. Lenses are organised by focal length, ranging from 1 through 9.\n",
    "\n",
    "We're given the _Holiday ASCII String Helper Manual Arrangement Procedure (HASHMAP)_, and we're told to apply the procedure to our initialisation sequence. (I.e. the same input data). This is how the boxes are loaded with the right lenses in the right order.\n",
    "\n",
    "The initialisation sequence is a series of comma-separated values.  Each value represents a step or operation. For each operation:\n",
    "\n",
    "- `abc` (the chars before the `-` or `=`) -> the lens label for this operation.\n",
    "- `compute_hash(label)` -> the intended box number. Recall that the hash returns a number 0-255.\n",
    "- If followed by `-`: \n",
    "  - Go to the relevant box and remove the lens with that label, if present. \n",
    "  - Then move any remaining lenses as far forward as possible without changing their order, filling any space made by the removed lens.\n",
    "- Or, if followed by `=n`:\n",
    "  - Insert the lens with focal length `n`.\n",
    "  - If there is already a lens in the box with this label, replace it _in situ_.\n",
    "  - If there is not already a lens in the box with this label, add this lens behind any other lenses in the box, as far forward as possible.\n",
    "\n",
    "At the end, we're asked to determine the focussing power of each box, where focussing power is the product of:\n",
    "\n",
    "- One plus the box number of the lens in question.\n",
    "- The slot number of the lens within the box: 1 for the first lens, 2 for the second lens, and so on.\n",
    "- The focal length of the lens.\n",
    "\n",
    "The solution answer is the sum of these powers.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- I start by defining a `Box` [dataclass](https://aoc.just2good.co.uk/python/classes#dataclass). The `Box` has attributes for:\n",
    "  - `num`\n",
    "  - `lenses` - a Python `list` containing our lenses, in the required order. Each lens is stored as a `(label, focus length)` tuple.\n",
    "  - `lenses_set` - this is possibly not necessary. But I wanted to make it efficient to determine if a given lens is in the `Box` without iterating through the `lenses` list of tuples. So I've added this set to do this check efficiently, before actually retrieving any lenses from the list. This set contains only the lens labels.\n",
    "\n",
    "- `focussing_power` - this is a pretty simple function that just follows the rules we've been given.\n",
    "- `get_lens_location(label)` - iterates through the `lenses` list, and checks if the supplied label matches the first term of the current tuple. If it does, we've found this lense.\n",
    "- `pop_lens(label)` - checks if this label is in our set. If it is, then use `get_lens_location(label)` to retrieve the index where this lens is stored, then pop that tuple from the list.  Note, when you `pop()` from a list `list` in Python, the remaining items automatically move up.  E.g. if we started with `[1, 2, 3, 4]` and we popped `3` (the item at index `2`), then the result would be `[1, 2, 4]`. Note: I also delete the label from our set.\n",
    "- `add_lens(label, f_length, location)` - this adds the lens to our `lenses` list, either at the end (if `location` is `None`) or at the specified location in the list. (And, of course, I need to add the lens label to our `lenses_set`.)\n",
    "\n",
    "I originally created a small bug here, because I was checking location like this:\n",
    "\n",
    "```python\n",
    "    if location:\n",
    "        self.lenses.insert(location, lens)\n",
    "    else:\n",
    "        self.lenses.append(lens)\n",
    "```\n",
    "\n",
    "My intent is to only insert at the specified location, if `location` is not `None`. But here, Python will return `False` if `location` is `None` or `0`. But `0` is a valid location, of course! So, to fix it:\n",
    "\n",
    "```python\n",
    "    if location is None:\n",
    "        self.lenses.insert(location, lens)\n",
    "    else:\n",
    "        self.lenses.append(lens)\n",
    "```\n",
    "\n",
    "So, to complete the solution:\n",
    "\n",
    "- Create our 256 boxes, initialised with empty lists of lenses.\n",
    "- Parse each step.\n",
    "- Get the lens label from the step.\n",
    "- Get the box number by hashing the step before the operation symbol.\n",
    "- If the operation is `-`, then simply call `pop_lens()` on the box.\n",
    "- If the operation is `=`, then check if this label already exists in the box. If it does, retrieve the current location and pop the lens at that location. Then, call `add_lens()` with the location we obtained, or `None` for the location.\n",
    "\n",
    "Finally, we can sum up all the box focussing powers by placing a [list comprehension](https://aoc.just2good.co.uk/python/comprehensions#aggregating-comprehensions) inside the `sum` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Box():\n",
    "    num: int\n",
    "    lenses:list[tuple[str,int]] = field(default_factory=list) # [ lens[label, focal_lengh], ... ]\n",
    "    lenses_set:set[str] = field(default_factory=set)\n",
    "    \n",
    "    def focusing_power(self) -> int:\n",
    "        \"\"\" Determine the focussing power of this box, calculated as\n",
    "        the sum over all lenses of:\n",
    "        box_num (1-indexed) * slot-num (1-indexed) * lens focal length\n",
    "        \"\"\"\n",
    "        pwr = 0\n",
    "        for slot, lens in enumerate(self.lenses):\n",
    "            pwr += (self.num+1) * (slot+1) * lens[1]\n",
    "            \n",
    "        return pwr\n",
    "    \n",
    "    def pop_lens(self, label: str) -> tuple[str, int] | None:\n",
    "        \"\"\" Pop the lens with the specified label. If the lens exists, return (location, lens) \"\"\"\n",
    "        if label in self.lenses_set:\n",
    "            self.lenses_set.remove(label)\n",
    "            return self.lenses.pop(self.get_lens_location(label))\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def get_lens_location(self, label:str) -> int:\n",
    "        for idx, (curr_label, curr_pwr) in enumerate(self.lenses):\n",
    "            if label == curr_label:\n",
    "                return idx\n",
    "            \n",
    "        raise KeyError(\"Lens not found\")\n",
    "    \n",
    "    def add_lens(self, label: str, f_length: int, location=None):\n",
    "        \"\"\" Add a lens at the specified location, or at the end. \"\"\"\n",
    "        lens = (label, f_length)\n",
    "        assert label not in self.lenses_set, \"There should be no lens with this label\"\n",
    "        self.lenses_set.add(label)        \n",
    "        if location is not None:\n",
    "            self.lenses.insert(location, lens)\n",
    "        else:\n",
    "            self.lenses.append(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data: str):\n",
    "    strings = data[0].split(\",\")\n",
    "    boxes = { box_num: Box(box_num) for box_num in range(256) } # create boxes, numbered 0-255\n",
    "    \n",
    "    for step in strings: # e.g. fnln-, spfc=9\n",
    "        operation = \"=\" if \"=\" in step else \"-\"\n",
    "        lens_label = step.split(operation)[0] # the lens label is everything before the op\n",
    "        curr_box = boxes[compute_hash(lens_label)] # the hash of the lens label gives the required box\n",
    "        if operation == \"-\": # remove lens\n",
    "            curr_box.pop_lens(lens_label)\n",
    "        else: # add lens of specified focal length\n",
    "            f_length = int(step[-1])\n",
    "            location = None\n",
    "            if lens_label in curr_box.lenses_set:\n",
    "                location = curr_box.get_lens_location(lens_label)\n",
    "                curr_box.pop_lens(lens_label)\n",
    "            \n",
    "            curr_box.add_lens(lens_label, f_length, location)\n",
    "    \n",
    "    return sum(box.focusing_power() for box in boxes.values())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [145]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 16: The Floor Will Be Lava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"16\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 16 Part 1\n",
    "\n",
    "We're in a giant cave! Here, our light beam is being focussed into a flat 2D grid containing:\n",
    "\n",
    "- Empty space (`.`) - light passes through these\n",
    "- Mirrors (`/` and `\\`) - these redirect the light through 90 degrees\n",
    "- Splitters (`|` and `-`) - light either passes through, or is split into two beams each at 90 degrees to the incident\n",
    "\n",
    "E.g.\n",
    "\n",
    "```text\n",
    ".|...\\....\n",
    "|.-.\\.....\n",
    ".....|-...\n",
    "........|.\n",
    "..........\n",
    ".........\\\n",
    "..../.\\\\..\n",
    ".-.-/..|..\n",
    ".|....-|.\\\n",
    "..//.|....\n",
    "```\n",
    "\n",
    "A tile is energised if at least one beam passes through it, reflects in it, or is split in it.\n",
    "\n",
    "**With the beam starting in the top-left heading right, how many tiles end up being energized?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "Okay, this problem wasn't too difficult, but I woke up with a terrible headache and made a complete mess of it.  Then I took some pills, had some coffee, cleared my head, and started again!\n",
    "\n",
    "I think the most sensible approach is to use a BFS to perform a _flood fill_, originating at our starting location, and only allowing adjacent locations that follow the rules we've been given.\n",
    "\n",
    "All the hard work is done in my `LightGrid` [class](https://aoc.just2good.co.uk/python/classes):\n",
    "\n",
    "- I build a dictionary, which maps combinations of (start location, start direction) to new directions, when we arrive at a mirror.\n",
    "- I build another dictionary which maps my N, E, S, W vectors to arrow characters, which is useful for printing the grid and debugging.\n",
    "- The `next_move(position, direction)` method:\n",
    "  - Expects a current location, and a current direction.\n",
    "  - Determines the one or two valid next moves, where a move is a combination of a new location, together with the direction we were facing when we arrived at that location.\n",
    "  - It first checks whether our current location is a `.`, or if we're arriving at the _pointy end_ of a splitter. In either case, the light can pass straight through to the next square in the same direction.\n",
    "  - It then checks if our current location is a mirror.  If it is, it uses the `MIRROR_DIRECTION_MAP` to determine the new direction. Then we add a unit in that direction to get the next location.\n",
    "  - It then checks if we've arrived at splitters. If we have, then we must be arriving perpendicular to the splitters. (Because we already checked for _pointy ends_ before.) If we arrive at a splitter, then we need to yield _two_ new next moves.\n",
    "\n",
    "- Next, the `bfs()` method!  If you need a recap of when to use a BFS and why it works, then check out my page [here](https://aoc.just2good.co.uk/python/shortest_paths). This is a pretty simple BFS implementation:\n",
    "  - I'm using a [deque](https://aoc.just2good.co.uk/python/lifo_fifo) for my first-in, first-out queue. Deques are very efficient for this.\n",
    "  - Then, for each current `(point, direction)` in the frontier, I expand out to valid neighbours. The neighbours are the next moves returned by `next_move()`, but additionally filtered on whether this move is in the grid or falls outside.\n",
    "  - Every time I retrieve a new next move in the BFS, I add it to `path_taken`, and also add next move to my `energised` dictionary. I can use this later to quickly count how many locations are energised. (Because the keys are unique locations.)\n",
    "\n",
    "Also of interest: in my `__str__()` method, I'm using [colorama](https://aoc.just2good.co.uk/python/colours) to draw a yellow path through the grid.\n",
    "\n",
    "Lastly, to determine how many tiles are energised, I just need to count the keys in my `energised` dictionary.\n",
    "\n",
    "That's all there is to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "I figured now would be a good time for an animation!  I've created an animated gif by creating frames of [Matplotlib plots](https://aoc.just2good.co.uk/python/matplotlib).\n",
    "\n",
    "![Sample data animation](https://aoc.just2good.co.uk/assets/images/sample_lava_floor.gif)\n",
    "\n",
    "Shift click...\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Z3gU67a6rPM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGrid(Grid):\n",
    "    \"\"\" Represents a 2D grid containing empty space (.), mirrors, and splitters (- and |).\n",
    "    Light passes through empty space. Light is refracted by 90 degrees at a mirror. \n",
    "    Light is split in the two orthogonal directions at a splitter, or allowed to pass through unchanged, \n",
    "    depending on orientation. \"\"\"\n",
    "    \n",
    "    MIRROR_DIRECTION_MAP = { # { (current char, current direction): new direction }\n",
    "        (\"/\", Vectors.E.value): Vectors.N.value,\n",
    "        (\"/\", Vectors.S.value): Vectors.W.value,\n",
    "        (\"/\", Vectors.W.value): Vectors.S.value,\n",
    "        (\"/\", Vectors.N.value): Vectors.E.value,\n",
    "        (\"\\\\\", Vectors.E.value): Vectors.S.value,\n",
    "        (\"\\\\\", Vectors.S.value): Vectors.E.value,\n",
    "        (\"\\\\\", Vectors.W.value): Vectors.N.value,\n",
    "        (\"\\\\\", Vectors.N.value): Vectors.W.value,\n",
    "    }\n",
    "    \n",
    "    VECTORS_TO_ARROWS = { # used for rendering a console representation\n",
    "        Vectors.N.value: \"^\",\n",
    "        Vectors.E.value: \">\",\n",
    "        Vectors.S.value: \"v\",\n",
    "        Vectors.W.value: \"<\",\n",
    "    }\n",
    "    \n",
    "    def __init__(self, grid_array: list, animating: bool = False, **kwargs) -> None:\n",
    "        \"\"\" Creates a grid that light beams pass through. \"\"\"\n",
    "       \n",
    "        # [ (posn, dirn), ... ]\n",
    "        # dirn is the direction we were facing when we arrived at this position\n",
    "        self.path_taken: list[tuple[Point, tuple[int,int]]] = []\n",
    "        self.energised = defaultdict(set) # { point: {dirn}, }\n",
    "        super().__init__(grid_array=grid_array, animating=animating, **kwargs)\n",
    "    \n",
    "    def animate_step(self, i):\n",
    "        \"\"\" Update the plot for the ith step in the animation. \"\"\"\n",
    "        if self._frame_index < len(self.path_taken):\n",
    "            if self._frame_index % 100 == 0:\n",
    "                logger.debug(f\"Rendering frame {self._frame_index}\")\n",
    "                \n",
    "            self._render_plot()\n",
    "            self._frame_index += 1\n",
    "        return []\n",
    "    \n",
    "    def create_animation(self, output_path='animation.mp4', fps=10):\n",
    "        \"\"\" Create the animation, by calling the animate_step() method to generate frames. \"\"\"\n",
    "        self._plot_info = self._setup_fig()  # Set up the figure for plotting\n",
    "        fig, axes, mkr_size = self._plot_info\n",
    "        \n",
    "        logger.debug(f\"Creating the animation. We have {len(self.path_taken)} frames to render.\")\n",
    "        # Creating the animation\n",
    "        anim = FuncAnimation(fig, self.animate_step, frames=len(self.path_taken), \n",
    "                             interval=1000/fps, blit=True)\n",
    "\n",
    "        # Save the animation\n",
    "        anim.save(output_path, writer='ffmpeg')\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\" Clear the path_taken and energised properties \"\"\"\n",
    "        self.path_taken = []\n",
    "        self.energised = defaultdict(set)\n",
    "    \n",
    "    def bfs(self, start:tuple[Point,tuple[int,int]]=(Point(0,0), Vectors.E.value)):\n",
    "        \"\"\" Perform a BFS to build the path that light takes through the grid.\n",
    "\n",
    "        Args:\n",
    "            start (tuple, optional): (point, vector value). Defaults to (Point(0,0), Vectors.E\n",
    "        \"\"\"\n",
    "        frontier = deque() # ideal for FIFO\n",
    "        frontier.append(start)\n",
    "        explored = set()\n",
    "        explored.add(start)\n",
    "\n",
    "        while frontier:\n",
    "            posn, dirn = frontier.popleft() # point, vector value\n",
    "            self.path_taken.append((posn, dirn))\n",
    "            self.energised[posn].add(dirn)       \n",
    "            \n",
    "            for neighbour in self.next_move(posn, dirn):\n",
    "                if self.valid_location(neighbour[0]): # is this next move in the grid?\n",
    "                    if neighbour not in explored:\n",
    "                        frontier.append(neighbour)\n",
    "                        explored.add(neighbour)\n",
    "    \n",
    "    def next_move(self, posn:Point, dirn:tuple):\n",
    "        \"\"\" Determine the next moves that are valid from here. Returns each move sequentially, as a generator.\n",
    "        For any given current (point, direction), we can move to 1 or 2 adjacent points. \n",
    "        If the current point is a mirror, the new direction will be different.\n",
    "        If the current point is a splitter, the new direction will be different if we've hit the splitter\n",
    "        from a perpendicular direction.\n",
    "\n",
    "        Args:\n",
    "            posn (Point): Our current location\n",
    "            dirn (Vector tuple): The direction we were facing when we landed at this point\n",
    "\n",
    "        Yields:\n",
    "            tuple: (next point, next direction)\n",
    "        \"\"\"\n",
    "\n",
    "        curr_val = self.value_at_point(posn) # where are we now\n",
    "\n",
    "        # First, check our \"pass through\" conditions...\n",
    "        if (curr_val == \".\" or (curr_val == \"|\" and dirn in (Vectors.N.value, Vectors.S.value))\n",
    "                            or (curr_val == \"-\" and dirn in (Vectors.E.value, Vectors.W.value))):\n",
    "            next_dirn = dirn \n",
    "            next_posn = posn + Point(*next_dirn)\n",
    "            yield (next_posn, dirn)            \n",
    "        elif curr_val in (\"/\", \"\\\\\"): # Now map directions if we're at a mirror\n",
    "            next_dirn = LightGrid.MIRROR_DIRECTION_MAP[(curr_val, dirn)]\n",
    "            next_posn = posn + Point(*next_dirn)\n",
    "            yield (next_posn, next_dirn)                      \n",
    "        elif curr_val ==  \"|\": # split at |, yielding two directions\n",
    "            assert dirn in (Vectors.E.value, Vectors.W.value), \"We must be going E or W\"\n",
    "            for next_dirn in (Vectors.N.value, Vectors.S.value):\n",
    "                next_posn = posn + Point(*next_dirn)\n",
    "                yield (next_posn, next_dirn)\n",
    "        else: # split at -, yielding two directions\n",
    "            assert curr_val == \"-\", \"We should be at -\"\n",
    "            assert dirn in (Vectors.N.value, Vectors.S.value), \"We must be going N or S\"\n",
    "            for next_dirn in (Vectors.E.value, Vectors.W.value):\n",
    "                next_posn = posn + Point(*next_dirn)\n",
    "                yield (next_posn, next_dirn)                    \n",
    "        \n",
    "    def __str__(self) -> str:\n",
    "        \"\"\" Generate a str representation of the grid, including the path_taken. \"\"\"\n",
    "        rows = []\n",
    "        for row_num, row in enumerate(self.array):\n",
    "            repr = []\n",
    "            for char_num, char in enumerate(row):\n",
    "                point = Point(char_num, row_num)\n",
    "                if point in self.energised:\n",
    "                    repr.append(Fore.YELLOW)\n",
    "                    if char not in (\"|\", \"-\", \"\\\\\", \"/\"):\n",
    "                        dirs_for_locn = len(self.energised[point])\n",
    "                        if dirs_for_locn == 1:\n",
    "                            dirn = list(self.energised[point])[0]\n",
    "                            repr.append(LightGrid.VECTORS_TO_ARROWS[dirn])                   \n",
    "                        else:\n",
    "                            repr.append(str(dirs_for_locn))\n",
    "                    else:\n",
    "                        repr.append(char)\n",
    "                    repr.append(Fore.RESET)\n",
    "                else:\n",
    "                    repr.append(char)\n",
    "            \n",
    "            rows.append(\"\".join(repr))\n",
    "        \n",
    "        return \"\\n\".join(rows)\n",
    "\n",
    "    def _setup_fig(self):\n",
    "        \"\"\" Initialise the plot \"\"\"      \n",
    "        my_dpi = 120\n",
    "        fig, axes = plt.subplots(figsize=(1024/my_dpi, 768/my_dpi), dpi=my_dpi, facecolor=\"white\") # set size in pixels\n",
    "\n",
    "        axes.get_xaxis().set_visible(True)\n",
    "        axes.get_yaxis().set_visible(True)\n",
    "        axes.invert_yaxis()\n",
    "        axes.set_aspect('equal') # set x and y to equal aspect\n",
    "        axes.set_facecolor('xkcd:black')\n",
    "        \n",
    "        min_x, max_x = -0.5, self.width - 0.5\n",
    "        min_y, max_y = -0.5, self.height - 0.5\n",
    "        axes.set_xlim(min_x, max_x)\n",
    "        axes.set_ylim(max_y, min_y)\n",
    "\n",
    "        # dynamically compute the marker size\n",
    "        fig.canvas.draw()\n",
    "        mkr_size = ((axes.get_window_extent().width / (max_x-min_x) * (45/fig.dpi)) ** 2)\n",
    "        return fig, axes, mkr_size\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\" Show the current plot \"\"\"\n",
    "        self._render_plot(frame_idx=-1)\n",
    "        plt.show()\n",
    "        \n",
    "    def _render_plot(self, frame_idx=None):\n",
    "        \"\"\" Add each new frame. Note that this method should draw should only draw up to a particular state. \n",
    "        If frame_idx is set, it will render the plot at that particular frame. Use -1 for final state. \"\"\"\n",
    "        fig, axes, mkr_size = self._plot_info\n",
    "        axes.clear() # Clear the axes to start fresh for each frame\n",
    "        axes.invert_yaxis()\n",
    "       \n",
    "        dir_sets = [set() for _ in range(4)]\n",
    "            \n",
    "        # Plot the path\n",
    "        # Only plot the path up to the current frame index\n",
    "        last_frame = frame_idx if frame_idx else (self._frame_index + 1)\n",
    "        for point, dirn in self.path_taken[:last_frame]:\n",
    "            for dir_set, arrow in zip(dir_sets, (\"^\", \">\", \"v\", \"<\")):\n",
    "                if LightGrid.VECTORS_TO_ARROWS[dirn] == arrow:\n",
    "                    dir_set.add(point)\n",
    "                    continue\n",
    "\n",
    "        for dir_set, arrow in zip(dir_sets, (\"^\", \">\", \"v\", \"<\")):\n",
    "            if dir_set:\n",
    "                dir_set_x, dir_set_y = zip(*((point.x, point.y) for point in dir_set))\n",
    "                axes.scatter(dir_set_x, dir_set_y, marker=arrow, s=mkr_size*0.3, color=\"white\")            \n",
    "                    \n",
    "        # Plot the infra\n",
    "        vert_splitters, horz_splitters, forw_mirrors, back_mirrors = set(), set(), set(), set()\n",
    "        infra_mappings = {\n",
    "            '|': vert_splitters, \n",
    "            '-': horz_splitters, \n",
    "            '/': forw_mirrors, \n",
    "            '\\\\': back_mirrors\n",
    "        }\n",
    "        \n",
    "        for row_num, row in enumerate(self.array):\n",
    "            for char_num, char in enumerate(row):\n",
    "                point = Point(char_num, row_num)\n",
    "                if char in infra_mappings:\n",
    "                    infra_mappings[char].add(point)        \n",
    "                    \n",
    "        for infra_type, marker in [(vert_splitters, r'$\\vert$'), \n",
    "                                   (horz_splitters, r'$-$'), \n",
    "                                   (forw_mirrors, r'$\\slash$'), \n",
    "                                   (back_mirrors, r'$\\backslash$')]:\n",
    "            \n",
    "            if infra_type: # check not empty\n",
    "                x, y = zip(*((point.x, point.y) for point in infra_type))\n",
    "                axes.scatter(x, y, marker=marker, s=mkr_size, color=\"xkcd:azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data, animate=False, out_name=\"animation.mp4\", fps=10):\n",
    "    grid = LightGrid(data, animating=True)\n",
    "    grid.bfs()\n",
    "    grid.plot()\n",
    "    if animate:\n",
    "        output_file = Path(locations.output_dir, out_name)\n",
    "        grid.create_animation(str(output_file), fps=fps)\n",
    "        \n",
    "    return len(grid.energised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(r\"\"\".|...\\....\n",
    "|.-.\\.....\n",
    ".....|-...\n",
    "........|.\n",
    "..........\n",
    ".........\\\n",
    "..../.\\\\..\n",
    ".-.-/..|..\n",
    ".|....-|.\\\n",
    "..//.|....\"\"\")\n",
    "sample_answers = [46]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input.splitlines(), animate=True, out_name=\"sample_lava_floor.mp4\"), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part1(input_data, animate=True, out_name=\"real_lava_floor.mp4\", fps=120)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 16 Part 2\n",
    "\n",
    "**Find the initial beam configuration that energizes the largest number of tiles; how many tiles are energized in that configuration?**\n",
    "\n",
    "We're told that valid starting configurations are:\n",
    "\n",
    "- All top row tiles, pointing south.\n",
    "- All bottom row tiles, pointing north.\n",
    "- All left edge tiles, pointing east.\n",
    "- All right edge tiles, pointing west.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "There's probably a much faster solution, but my approach was simply to:\n",
    "\n",
    "- Build a list of all the starting locations, and their associated directions.\n",
    "- Perform the same BFS as before, but using these starting locations, rather than the top left square.\n",
    "\n",
    "So this requires very little code, and it still runs in under 10 seconds.\n",
    "\n",
    "- I had already coded by BFS to accept a start location, and starting location, but defaulting to top-left, and east, respectively. So no changes required there.\n",
    "- The other thing I added to my `LightGrid` class is a `reset()` method, to clear the `path_taken` and `energised` variables with each configuration. That way, I don't have to create a new `LightGrid` with each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data):\n",
    "    grid = LightGrid(data)\n",
    "    \n",
    "    starts = [] # to store all possible starting locations and directions\n",
    "    for col in range(0, grid.width): \n",
    "        # points on top edge pointing down\n",
    "        starts.append((Point(col, 0), Vectors.S.value))\n",
    "        # points on bottom edge, pointing up\n",
    "        starts.append((Point(0, grid.height-1), Vectors.N.value))\n",
    "    \n",
    "    for row in range(0, grid.height):\n",
    "        # points on left edge, pointing right\n",
    "        starts.append((Point(0, row), Vectors.E.value))\n",
    "        # points on right edge, pointing left\n",
    "        starts.append((Point(grid.width-1, row), Vectors.W.value))\n",
    "    \n",
    "    energised = {} # { (start, direction), count_energised, ... }\n",
    "    for start in tqdm(starts): # let's view a progress bar\n",
    "        grid.bfs(start)\n",
    "        energised[start] = len(grid.energised)\n",
    "        grid.reset()\n",
    "    \n",
    "    return max(energised.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [51]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2(curr_input.splitlines())[1], curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2(input_data)[1]\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 17: Clumsy Crucible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"17\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 17 Part 1\n",
    "\n",
    "_\"To get Desert Island the machine parts it needs as soon as possible, you'll need to find the best way to get the crucible from the lava pool to the machine parts factory. To do this, you need to minimize heat loss while choosing a route that doesn't require the crucible to go in a straight line for too long.\"_\n",
    "\n",
    "We have a map that can be used to calculate heat loss at any particular city block.  E.g.\n",
    "\n",
    "```text\n",
    "2413432311323\n",
    "3215453535623\n",
    "3255245654254\n",
    "3446585845452\n",
    "4546657867536\n",
    "1438598798454\n",
    "4457876987766\n",
    "3637877979653\n",
    "4654967986887\n",
    "4564679986453\n",
    "1224686865563\n",
    "2546548887735\n",
    "4322674655533\n",
    "```\n",
    "\n",
    "Each digit represents the amount of heat loss if the crucible enters that block.\n",
    "\n",
    "We need to go from top-left to bottom-right. But here's the catch: we can only move a maximum of three blocks in a straight line, before we have to turn left or right. And we can't reverse.\n",
    "\n",
    "**Directing the crucible from the lava pool to the machine parts factory, but not moving more than three consecutive blocks in the same direction, what is the least heat loss it can incur?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "I found this one pretty tough.\n",
    "\n",
    "It was pretty obvious from the start that I needed either [Dijkstra’s or A* algorithm](https://aoc.just2good.co.uk/python/shortest_paths) to find the path with the lowest cumulative cost. Recall that Dijkstra’s algorithm is ideal for finding the shortest path through a graph where the edges have variable weigth. \n",
    "\n",
    "Alas, the actual implementation took me ages to get right.\n",
    "\n",
    "So... We need to find a path from a start to a goal, where the cost varies depending on the path taken. As usual for a Dijkstra, we need to use a [priority queue](https://aoc.just2good.co.uk/python/priority_queues). First, we need to define some way to represent the state of our journey through the map. I'll represent state in a tuple that stores `(cost, current position, direction, straight steps taken)`. The cost is the cumulative heat lost as we enter a vertex. It's important that `cost` comes first in the tuple, because our priority queue pops the next state based on the lowest value of the first item in the tuple.\n",
    "\n",
    "We keep popping next states until we reach the goal. To determine valid next states:\n",
    "\n",
    "- We create `dict` to store states that we've already seen. \n",
    "  - Here, we only want to store `(current position, direction, staight_steps)` because we may have entered a cycle and re-entered the same configuration.  In this case, we don't want to continue if the cost is higher, because this means we've entered a cycle.  I.e. we've repeated a previous state, but with a higher cost. But we can't include the cost in the state, because then the state would always be different if we see a cycle.\n",
    "  - We could use a `set` to store the seen states.  But by using a `dict`, we can also store the cumulative cost to reach this state. This way, we can ignore any previously seen states if the cumulative cost is not less than a cost we've already stored.\n",
    "- If we're in our initial position, we don't have a direction yet.  So we can try all adjacent squares, and with each, we set `straight steps` to 1.\n",
    "- If we're not on the first step, then our valid next moves are left, right, or straight on. We can't go backwards.\n",
    "  - We can move left relative to our current direction by adding the vector `Point(-dirn.y, dirn.x)`.\n",
    "  - We can move right relative to our current direction by adding the vector `Point(dirn.y, -dirn.x)`.\n",
    "  - We can only go straight if `straight steps` is less than `3`.\n",
    "- Now we've got our new position and new direction, we can determine the cost of the next move, and add that cost to our current cumulative cost.\n",
    "\n",
    "Finally, we return the total cumulative cost, when we reach the goal.\n",
    "\n",
    "And that's basically all there is to it for the Dijkstra solution!\n",
    "\n",
    "I was interested to see what would happen if I changed to an A* implementation. I did this by creating a `heuristic` variable that stores the combination of `path cost` with the _Manhattan distance_ between start and goal. The idea here is that by using the heuristic, we will favour paths that take us towards the goal. But when I added this, the solution took about twice as long to compute. Oh well... It was worth a try! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(grid: list[list[int]], start: Point, goal: Point, max_straight: int, pre_turn: int):\n",
    "    \"\"\" Determine the path with lowest cost from start to goal\n",
    "\n",
    "    Args:\n",
    "        grid (list[list[int]]): A 2D grid containing int values\n",
    "        start (Point): Starting point\n",
    "        goal (Point): Final point\n",
    "        max_straight (int): Max moves we can make in a straight line before turning\n",
    "        pre_turn (int): Max moves we must make before turning or stopping\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no solution\n",
    "\n",
    "    Returns:\n",
    "        int: best cost for path\n",
    "    \"\"\"\n",
    "    queue = [] # priority queue to store current state\n",
    "    \n",
    "    cost:int = 0 # cumulative cost of all moves, based on value of each entered location\n",
    "    current_posn:Point = start\n",
    "    dirn:Optional[Point] = None # Current direction. (None when we start.)\n",
    "    straight_steps:int = 0 # number of steps taken in one direction without turning\n",
    "    heapq.heappush(queue, (cost, current_posn, dirn, straight_steps)) # cost must come first for our priority queue\n",
    "    \n",
    "    # Store initial state in the dict, with the cheapest cost to this state.\n",
    "    # We want to track states we've seen as (current_posn, dirn, straight_steps)\n",
    "    # But we want to ignore total cost. Because we need to detect \n",
    "    # if we've entered a configuration we've seen before. E.g. into a cycle.\n",
    "    seen = { (current_posn, dirn, straight_steps): 0 } # { state: cumulative cost }\n",
    "    \n",
    "    while queue:\n",
    "        cost, current_posn, dirn, straight_steps = heapq.heappop(queue)\n",
    "        # logger.debug(f\"{cost=}, {current_posn=}, {dirn=}, {steps=}\")\n",
    "\n",
    "        if current_posn == goal and straight_steps >= pre_turn:\n",
    "            return cost\n",
    "        \n",
    "        next_states = [] # point, dirn, steps\n",
    "        if dirn is None: # we're at the start, so we can go in any direction\n",
    "            for dir_x, dir_y in ((0, 1), (1, 0), (0, 1), (-1, 0)):\n",
    "                dirn = Point(dir_x, dir_y)\n",
    "                neighbour = current_posn + dirn\n",
    "                next_states.append((neighbour, dirn, 1))\n",
    "        else:\n",
    "            if straight_steps >= pre_turn:  # we can turn left or right, relative to current direction\n",
    "                next_states.append(((current_posn + Point(-dirn.y, dirn.x)), Point(-dirn.y, dirn.x), 1)) # turn 90 degrees CCW (left)\n",
    "                next_states.append(((current_posn + Point(dirn.y, -dirn.x)), Point(dirn.y, -dirn.x), 1)) # turn 90 degrees CW (right)\n",
    "            \n",
    "            if straight_steps < max_straight: # we can move straight ahead. \n",
    "                next_states.append(((current_posn + dirn), dirn, straight_steps+1))\n",
    "            \n",
    "        for next_state in next_states:\n",
    "            neighbour, dirn, new_steps = next_state\n",
    "            if (0 <= neighbour.x < len(grid[0]) and 0 <= neighbour.y < len(grid)):\n",
    "                new_cost = cost + grid[neighbour.y][neighbour.x]\n",
    "                # enqueue only if it's a new state, or a previous state with better cost\n",
    "                if next_state not in seen or new_cost < seen[next_state]:\n",
    "                    seen[next_state] = new_cost\n",
    "                    heapq.heappush(queue, (new_cost, \n",
    "                                        neighbour, \n",
    "                                        dirn, \n",
    "                                        new_steps))\n",
    "    \n",
    "    raise ValueError(\"No solution found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data, max_straight=3, pre_turn=0):\n",
    "    grid = [list(map(int, line)) for line in data]\n",
    "\n",
    "    start = Point(0,0)\n",
    "    goal = Point(len(grid[0])-1, len(grid)-1)\n",
    "    cost = get_path(grid, start, goal, max_straight=max_straight, pre_turn=pre_turn)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"2413432311323\n",
    "3215453535623\n",
    "3255245654254\n",
    "3446585845452\n",
    "4546657867536\n",
    "1438598798454\n",
    "4457876987766\n",
    "3637877979653\n",
    "4654967986887\n",
    "4564679986453\n",
    "1224686865563\n",
    "2546548887735\n",
    "4322674655533\"\"\")\n",
    "sample_answers = [102]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "logger.info(\"Tests passed!\")\n",
    "soln = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 17 Part 2\n",
    "\n",
    "We're upgrading to _ultra crucibles_.  These:\n",
    "\n",
    "- Require a minimum of 4 blocks before it can turn or stop.\n",
    "- But can move 10 blocks before turning.\n",
    "\n",
    "**Directing the ultra crucible from the lava pool to the machine parts factory, what is the least heat loss it can incur?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- Change the `max_straight` value from 3 to 10. So I've parameterised this.\n",
    "- Add a new parameter called `pre_turn`, which determines the minimum of `straight_steps` before we're allowed to turn. So now I perform this test before turning left or right:\n",
    "\n",
    "```python\n",
    "if straight_steps >= pre_turn:\n",
    "```\n",
    "\n",
    "Finally, we also need to ensure that we've taken at least four straight steps when we arrive at the goal. So we simply amend our exit condition check like this:\n",
    "\n",
    "```python\n",
    "    if current_posn == goal and straight_steps >= pre_turn:\n",
    "        return cost\n",
    "```\n",
    "\n",
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data, max_straight=10, pre_turn=4):\n",
    "    grid = [list(map(int, line)) for line in data]\n",
    "\n",
    "    start = Point(0,0)\n",
    "    goal = Point(len(grid[0])-1, len(grid)-1)\n",
    "    cost = get_path(grid, start, goal, max_straight=max_straight, pre_turn=pre_turn)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs.append(\"\"\"111111111111\n",
    "999999999991\n",
    "999999999991\n",
    "999999999991\n",
    "999999999991\"\"\")\n",
    "\n",
    "sample_answers = [94, 71]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 18: Lavaduct Lagoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"18\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 18 Part 1\n",
    "\n",
    "We're going to dig a lagoon. We've been given a dig plan that looks like this:\n",
    "\n",
    "```text\n",
    "R 6 (#70c710)\n",
    "D 5 (#0dc571)\n",
    "L 2 (#5713f0)\n",
    "D 2 (#d2c081)\n",
    "R 2 (#59c680)\n",
    "D 2 (#411b91)\n",
    "L 5 (#8ceee2)\n",
    "U 2 (#caa173)\n",
    "L 1 (#1b58a2)\n",
    "U 2 (#caa171)\n",
    "R 2 (#7807d2)\n",
    "U 3 (#a77fa3)\n",
    "L 2 (#015232)\n",
    "U 2 (#7a21e3)\n",
    "```\n",
    "\n",
    "The digger starts in a 1 meter cube hole in the ground and can move the specified number of metres up, down, left or right. These directions are all in the same plane (as seen from above), so we're in a 2D grid. Having followed the instructions to dig a tunnel that represents the edge of the lagoon, we then dig out the interior.\n",
    "\n",
    "Each trench is also listed with the color that the edge of the trench should be painted as an RGB hexadecimal color code. But for part 1, we don't use this information.  So we only care about the first two parameters in each line.\n",
    "\n",
    "**If they follow their dig plan, how many cubic meters of lava could it hold?**\n",
    "\n",
    "**My (first) solution:**\n",
    "\n",
    "My first solution is a flood fill.  It works fine for Part 1.\n",
    "\n",
    "- First, parse the input data.  For each instruction in this part, we only care about the direction and distance in that direction.\n",
    "- Then, let's plot the perimeter path with `process_plan()`:\n",
    "  - Create a list to represent the squares of the perimeter - i.e. what we'll dig with the instructions.\n",
    "  - Start at `(0,0)` and add it to the list.\n",
    "  - For each instruction, iterate over the required number of squares. For each iteration, add the current direction to the current square. This gives us the new current square. Add it to the `perimeter_path` list.\n",
    "  - Whenever we reach a turn, identify the inside square that is bordered by the three squares of the turn, and flag it as an _interior candidate_. The way I identify the candidate square is to create a `2x2` square from the three squares of the turn, and then simply identify the square that is missing.\n",
    "  - This method returns `perimeter_path` and `interior_candidates` as a tuple.\n",
    "- Next, I create `perimeter_set` from my `perimeter_path`, because this is much more efficient when checking membership. (I'll be using this later for my BFS.)\n",
    "- Then I call `create_bounds()` to determine the top-left and bottom-right coordinates of all points in the perimeter. We will use this later to form the outer bounds of our BFS.\n",
    "- Now I'm ready to `flood_fill()`, to determine the interior volume of our perimeter.\n",
    "  - This is another [BFS flood fill](https://aoc.just2good.co.uk/python/shortest_paths).\n",
    "  - We create `interior` and `exterior` sets, to store points as we examine them.\n",
    "  - We iterate over candidate points from `interior_candidates`. If the candidate has already been placed in `interior` or `exterior`, then we can skip it.\n",
    "  - Otherwise, we set up a BFS to expand from this particular candidate. \n",
    "  - The BFS expands in each direction, until it reaches a perimeter point, or the boundary of the grid.\n",
    "  - With each point we find, we add it to a set called `region`.\n",
    "  - If our BFS touches the boundary of the grid, then we mark `region` as NOT `is_internal`. We then add the entire region to `exterior`, and continue to the next candidate.\n",
    "  - If our BFS never touches the boundary, then the `region` remains `is_internal`, and we add all the points from this region to `interior`.\n",
    "  - I return `interior`.\n",
    "  \n",
    "  Finally, I add up the counts of points from the `perimeter_set` to the `interior` set, and return this as the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "I'm plotting the `perimeter` and the `interior` points using [matplotlib](https://aoc.just2good.co.uk/python/matplotlib).\n",
    "\n",
    "Our sample data, after filling, is represented like this:\n",
    "\n",
    "```text\n",
    "#######\n",
    "#######\n",
    "#######\n",
    "..#####\n",
    "..#####\n",
    "#######\n",
    "#####..\n",
    "#######\n",
    ".######\n",
    ".######\n",
    "```\n",
    "\n",
    "After plotting, we end up with this:\n",
    "\n",
    "![Sample data lava lagoon](https://aoc.just2good.co.uk/assets/images/lava_lagoon_sample.png)\n",
    "\n",
    "And for my real data:\n",
    "\n",
    "![Real data lava lagoon](https://aoc.just2good.co.uk/assets/images/lava_lagoon_real.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_plan(data) -> list[tuple]:\n",
    "    \"\"\" Read the plan and convert to a light of instructions and edge lengths. E.g.\n",
    "    [('R', 6), ('D', 5), ('L', 2), ... \n",
    "    \"\"\"\n",
    "    plan = []\n",
    "    for line in data:\n",
    "        dirn, path_len, hex_code = line.split()\n",
    "        plan.append((dirn, int(path_len)))\n",
    "        \n",
    "    return plan\n",
    "\n",
    "def process_plan(plan: list) -> tuple[list, set]:\n",
    "    \"\"\" Determine perimeter path and interior candidates. \"\"\"\n",
    "    perimeter_path = []\n",
    "    current = (0,0)\n",
    "    perimeter_path.append(current)\n",
    "    interior_candidates = set()\n",
    "    \n",
    "    for instr_num, (dirn_char, path_len) in enumerate(plan):\n",
    "        # logger.debug(f\"[{instr_num}]: {(dirn_char, path_len, hex_code)}\")\n",
    "        dirn = VectorDicts.DIRS[dirn_char]\n",
    "        for step in range(path_len):\n",
    "            current = (current[0]+dirn[0], current[1]+dirn[1])\n",
    "            perimeter_path.append(current)\n",
    "            \n",
    "            if instr_num>0 and step==0: # turn executed\n",
    "                # After the turn, the last three moves will represent three squares in a 2x2 grid.\n",
    "                # The remaining square will represent an interior point\n",
    "                assert len(perimeter_path) >= 3, \"We must have at least three squares if we've made a turn\"\n",
    "                last_three = perimeter_path[-3:]\n",
    "            \n",
    "                for y in range(min(y for x, y in last_three), max(y for x, y in last_three)+1):\n",
    "                    for x in range(min(x for x, y in last_three), max(x for x, y in last_three)+1):\n",
    "                        if (x, y) not in perimeter_path:\n",
    "                            interior_candidates.add((x, y))\n",
    "                            break\n",
    "    \n",
    "    return perimeter_path, interior_candidates\n",
    "\n",
    "def get_bounds(perimeter_path: list[tuple[int,int]]) -> tuple[tuple, tuple]:\n",
    "    \"\"\" Given a path of points, determine the top left and bottom right coordinates. \"\"\"\n",
    "    min_x = min(perimeter_path, key=lambda p: p[0])[0]\n",
    "    max_x = max(perimeter_path, key=lambda p: p[0])[0]\n",
    "    min_y = min(perimeter_path, key=lambda p: p[1])[1]\n",
    "    max_y = max(perimeter_path, key=lambda p: p[1])[1]\n",
    "    \n",
    "    return ((min_x, min_y), (max_x, max_y)) # tl, br\n",
    "\n",
    "def flood_fill(perimeter_set, interior_candidates: set[tuple], bounds: tuple[tuple, tuple]) -> set[tuple]:\n",
    "    \"\"\" Perform a BFS flood fill, to determine the interior volume of our perimeter.\n",
    "\n",
    "    Args:\n",
    "        perimeter_set (_type_): all coordinates that make up the perimeter\n",
    "        interior_candidates (set[tuple]): interior candidates, that are the inside of a turn.\n",
    "        bounds (tuple[tuple, tuple]): top left, bottom right. Used to form the bounds of our BFS.\n",
    "\n",
    "    Returns:\n",
    "        set[tuple]: _description_\n",
    "    \"\"\"\n",
    "    min_x, min_y = bounds[0]\n",
    "    max_x, max_y = bounds[1]\n",
    "    exterior = set()\n",
    "    interior = set()\n",
    "\n",
    "    logger.debug(f\"Processing {len(interior_candidates)} interior candidates.\") \n",
    "    for point in interior_candidates: # try a BFS from each interior point\n",
    "        if point in interior or point in exterior:\n",
    "            continue # this point is in a region we've done already\n",
    "            \n",
    "        region = set()\n",
    "        queue = deque()\n",
    "        queue.append(point)\n",
    "        explored = set()\n",
    "        explored.add(point)\n",
    "        is_interior = True # assume interior point\n",
    "    \n",
    "        while queue and is_interior:\n",
    "            current = queue.popleft()\n",
    "            \n",
    "            if current in interior: # we've seen this interior point before\n",
    "                break\n",
    "            \n",
    "            if current not in perimeter_set: # we need to ignore points in the perimeter\n",
    "                region.add(current)\n",
    "            \n",
    "            neighbours = [(current[0]+dx, current[1]+dy) for dx,dy in (VectorDicts.DIRS.values())]\n",
    "            for neighbour in neighbours:\n",
    "                if min_x <= neighbour[0] <= max_x and min_y <= neighbour[1] <= max_y: # within bounds\n",
    "                    if neighbour not in perimeter_set: # this is a valid region point\n",
    "                        if neighbour not in explored:\n",
    "                            queue.append(neighbour)\n",
    "                            explored.add(neighbour)\n",
    "                else: # outside of bounds so mark the region as external\n",
    "                    is_interior = False\n",
    "                    break\n",
    "        \n",
    "        if is_interior:              \n",
    "            interior.update(region)\n",
    "            logger.debug(f\"Updated all_interior with {len(region)} points.\")\n",
    "        else:\n",
    "            exterior.update(region)\n",
    "            logger.debug(f\"Updated exterior with {len(region)} points.\")\n",
    "            \n",
    "    return interior \n",
    "\n",
    "def plot_path(path: list[tuple], inside: set[tuple]=set()):\n",
    "    # Extract x and y values from the path\n",
    "    loop_x_values = [point[0] for point in path]\n",
    "    loop_y_values = [point[1] for point in path]\n",
    "    \n",
    "    # Extract x and y values from the inside set\n",
    "    inside_x_values = [point[0] for point in inside]\n",
    "    inside_y_values = [point[1] for point in inside]\n",
    "\n",
    "    # Plot the line and scatter graphs\n",
    "    plt.plot(loop_x_values, loop_y_values, \n",
    "             marker=MarkerStyle('o'), linestyle='-', color=\"blue\", label=\"Loop\")\n",
    "        \n",
    "    plt.scatter(inside_x_values, inside_y_values, \n",
    "                marker=MarkerStyle('x'), color=\"red\", label=\"Inside\")\n",
    "    \n",
    "    plt.title('Path Plot')\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.gca().set_aspect('equal', adjustable='box') # set equal scale \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data) -> int:\n",
    "    plan = parse_plan(data)\n",
    "    perimeter_path, interior_candidates = process_plan(plan)\n",
    "    perimeter_set = set(perimeter_path)\n",
    "\n",
    "    bounds = get_bounds(perimeter_path)\n",
    "    all_interior = flood_fill(perimeter_set, interior_candidates, bounds)\n",
    "    plot_path(perimeter_path, all_interior)\n",
    "    \n",
    "    return len(perimeter_set) + len(all_interior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"R 6 (#70c710)\n",
    "D 5 (#0dc571)\n",
    "L 2 (#5713f0)\n",
    "D 2 (#d2c081)\n",
    "R 2 (#59c680)\n",
    "D 2 (#411b91)\n",
    "L 5 (#8ceee2)\n",
    "U 2 (#caa173)\n",
    "L 1 (#1b58a2)\n",
    "U 2 (#caa171)\n",
    "R 2 (#7807d2)\n",
    "U 3 (#a77fa3)\n",
    "L 2 (#015232)\n",
    "U 2 (#7a21e3)\"\"\")\n",
    "sample_answers = [62]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 18 Part 2\n",
    "\n",
    "Uh oh. The colour and instruction parameters need to be swapped!! We need to extract the correct instructions from the hex codes.\n",
    "\n",
    "For each hex code:\n",
    "\n",
    "- First five digits = hex value of distance in m\n",
    "- Last digit = direction, where `0 = R`, `1 = D`, `2 = L`, ` 3 = U`\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "Even with the sample data, we're told that our lagoon will hold `952408144115` cubic metres of lava.  So clearly we have far too many points to go storing them in sets.  Our Part 1 solution isn't going to scale! We need to do something smarter.\n",
    "\n",
    "We can use math! I'm going to use the **Shoelace Formula** with **Pick's Theorem**, just as I did in Day 10. \n",
    "\n",
    "Here are the key formulae:\n",
    "\n",
    "- [Shoelace Formula](https://en.wikipedia.org/wiki/Shoelace_formula) - to calculate the area of any polygon, given the _consecutive coordinates_ of its vertices. One great thing about this formula is that it even works for self-overlapping / self-intersecting polygons!\n",
    "\n",
    "$$\n",
    "A = \\frac{1}{2} \\left| \\sum_{i=1}^{n} x_i (y_{i+1} - y_{i-1}) \\right|\n",
    "$$\n",
    "\n",
    "- [Pick's Theorem](https://en.wikipedia.org/wiki/Pick%27s_theorem) - to calculate the number of interior points for an enclosed polygon.\n",
    "\n",
    "$$\n",
    "A = i + \\frac{b}{2} - 1\n",
    "$$\n",
    "\n",
    "However, our vertex coordinates are at the centre of 1x1 grid cells. Why? Because we start at the centre of a 1x1 cell, and then we dig the perimeter. So, from our sample input:\n",
    "\n",
    "```text\n",
    "#######\n",
    "#*****#\n",
    "###***#\n",
    "..#***#\n",
    "..#***#\n",
    "###*###\n",
    "#***#..\n",
    "##**###\n",
    ".#****#\n",
    ".######\n",
    "```\n",
    "\n",
    "Let's use Matplotlib to draw a more accurate representation of the dig plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_path(path: list[tuple], inside: set[tuple]=set()):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Function to add a 1x1 square with the point at its center\n",
    "    def add_square(x, y, colour, fill=False):\n",
    "        square = Rectangle((x - 0.5, y - 0.5), 1, 1, fill=fill, edgecolor=colour, facecolor=colour)\n",
    "        ax.add_patch(square)\n",
    "\n",
    "    # Plot each point in the path as a square\n",
    "    for point in path:\n",
    "        add_square(point[0], point[1], 'blue')\n",
    "\n",
    "    # Plot each point in the inside set as a square\n",
    "    for point in inside:\n",
    "        add_square(point[0], point[1], 'red', fill=True)\n",
    "\n",
    "    # Extract x and y values for vertices\n",
    "    path_x_values = [point[0] for point in path]\n",
    "    path_y_values = [point[1] for point in path]\n",
    "    inside_x_values = [point[0] for point in inside]\n",
    "    inside_y_values = [point[1] for point in inside]\n",
    "    \n",
    "    # Plot the actual vertex points\n",
    "    ax.scatter(path_x_values, path_y_values, color=\"blue\", zorder=5)\n",
    "    ax.scatter(inside_x_values, inside_y_values, color=\"blue\", zorder=5)\n",
    "    \n",
    "    # Set limits for x and y axis\n",
    "    all_x_values = [point[0] for point in path] + [point[0] for point in inside]\n",
    "    all_y_values = [point[1] for point in path] + [point[1] for point in inside]\n",
    "\n",
    "    ax.set_xlim(min(all_x_values) - 1, max(all_x_values) + 1)\n",
    "    ax.set_ylim(min(all_y_values) - 1, max(all_y_values) + 1)\n",
    "\n",
    "    plt.title('Path Plot')\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "sample_input = \"\"\"R 6 (#70c710)\n",
    "D 5 (#0dc571)\n",
    "L 2 (#5713f0)\n",
    "D 2 (#d2c081)\n",
    "R 2 (#59c680)\n",
    "D 2 (#411b91)\n",
    "L 5 (#8ceee2)\n",
    "U 2 (#caa173)\n",
    "L 1 (#1b58a2)\n",
    "U 2 (#caa171)\n",
    "R 2 (#7807d2)\n",
    "U 3 (#a77fa3)\n",
    "L 2 (#015232)\n",
    "U 2 (#7a21e3)\"\"\"\n",
    "\n",
    "plan = parse_plan(sample_input.splitlines())\n",
    "perimeter_path, interior_candidates = process_plan(plan)\n",
    "perimeter_set = set(perimeter_path)\n",
    "\n",
    "bounds = get_bounds(perimeter_path)\n",
    "all_interior = flood_fill(perimeter_set, interior_candidates, bounds)\n",
    "plot_path(perimeter_path, all_interior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like this:\n",
    "\n",
    "![Dig plan](https://aoc.just2good.co.uk/assets/images/lava_lagoon_1-by-1_squares.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here our vertex coordinates represent the top left of each cell. But in order to use the _Shoelace Formula_, we need our vertices to be on the _outside_ of each corner. We could adjust our vertices to achieve this, like this:\n",
    "\n",
    "![Dig plan](https://aoc.just2good.co.uk/assets/images/lava_lagoon_1-by-1_squares_corners.png)\n",
    "\n",
    "Or alternatively, we can combine the _Shoelace formula_ with _Pick's Theorem_ to determine the number of _internal coordinates bounded by our polygon_. Either should work!\n",
    "\n",
    "Here's is the approach that combines the _Shoelace formula_ with _Pick's Theorem_:\n",
    "\n",
    "1. Determine the area of the _overall_ polygon, using _Shoelace_.\n",
    "2. Determine the number of perimeter boundary points, which is also the perimeter volume. (Since our perimeter is made up of 1x1 squares.)\n",
    "3. Rearrange _Pick's Theorem_ to make use of this area and the number of boundary points, to determine the number of _internal integer coordinates_, i.e.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A &= i + \\frac{b}{2} - 1 \\\\\n",
    "\\\\\n",
    "i &= A - \\frac{b}{2} + 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "4. Add the perimeter volume to the number of internal integer coordinates, to get the overall polygon volume.\n",
    "\n",
    "To implement this:\n",
    "\n",
    "- I've created a new `parse_plan_hex(input_data)` function, which now uses the hex codes to obtain the instructions.\n",
    "- This time, I use a function `process_turns(plan)`, which returns all the successive coordinates of each vertex that makes up our polygon.\n",
    "- I pass the resulting list of vertices (my `polygon`) to a new function called `shoelace_area()`, which calculates the overall area of the polygon. This just implementes the equation above.\n",
    "- Next, I need to determine the number of boundary points in the perimeter. This happens to also be the total area of the perimeter, since the perimeter is made up of squares that are 1-by-1. I do this with a `perimeter_length(polygon)` function.\n",
    "- Then I calculate the interior points, using my `interior_points(area, boundary_points)`. Again, this just implements the equation above.\n",
    "- Finally, I add the `interior_points` to the `boundary_points` to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_plan_hex(data) -> list[tuple]:\n",
    "    \"\"\" \n",
    "    Parse input data and convert into a list of instructions like this:\n",
    "    [('R', 461937), ('D', 56407), ...]\n",
    "\n",
    "    For each input value (which looks like #70c710), convert the last digit into a direction,\n",
    "    and then convert the first five hex digits into a decimal value.\n",
    "    \"\"\"\n",
    "    dirs = { 0: \"R\", 1: \"D\", 2: \"L\", 3: \"U\" }\n",
    "    \n",
    "    plan:list[tuple] = []\n",
    "    for line in data:\n",
    "        instr = line[-7:-1]\n",
    "        dirn = dirs[int(instr[-1])]\n",
    "        path_len = int(instr[0:-1], base=16)\n",
    "        plan.append((dirn, int(path_len)))\n",
    "        \n",
    "    return plan\n",
    "\n",
    "def process_turns(plan) -> list[tuple[int,int]]:\n",
    "    \"\"\" \n",
    "    Process the plan instructions, and return the perimeter path, which contains only the vertices of the polygon. E.g.\n",
    "    [(0, 0), (461937, 0), (461937, 56407), (818608, 56407), ...]\n",
    "    \"\"\"\n",
    "    # initialise, from current\n",
    "    current = (0,0)\n",
    "    path:list[tuple[int,int]] = [current]\n",
    "\n",
    "    for dirn_char, path_len in plan:\n",
    "        dirn = VectorDicts.DIRS[dirn_char]\n",
    "        current = (current[0]+dirn[0]*path_len, current[1]+dirn[1]*path_len)\n",
    "        path.append(current)\n",
    "\n",
    "    return path\n",
    "\n",
    "def shoelace_area(polygon: list[tuple[int,int]]) -> int:\n",
    "    total = 0\n",
    "    for i, (x,y) in enumerate(polygon):\n",
    "        next_index = (i+1) % len(polygon)\n",
    "        prev_index = i-1\n",
    "        total += x*(polygon[next_index][1] - polygon[prev_index][1])\n",
    "        \n",
    "    return abs(total) // 2\n",
    "\n",
    "def perimeter_length(polygon: list[tuple[int,int]]) -> int:\n",
    "    \"\"\" The total length of all the edges of this polygon. This is equivalent to the number of boundary points. \"\"\"\n",
    "    total = 0\n",
    "    for i, (x,y) in enumerate(polygon):\n",
    "        next_index = (i+1) % len(polygon)\n",
    "        total += abs(polygon[next_index][0]-x) + abs(polygon[next_index][1]-y) # there will only ever be an x or y component\n",
    "        \n",
    "    return total\n",
    "\n",
    "def interior_points(area: int, boundary_points: int):\n",
    "    \"\"\" Determine the number of interior points using Pick's Theorem.\n",
    "\n",
    "    Args:\n",
    "        area (int): total polygon area\n",
    "        boundary_points (int): the number of boundary points\n",
    "    \"\"\"\n",
    "    return area - (boundary_points // 2) + 1\n",
    "\n",
    "def total_area(polygon: list[tuple[int,int]]) -> int:\n",
    "    \"\"\" Return total area of the polygon, including the perimeter.\n",
    "    Combine Shoelace formula and Pick's theorem to determine internal integer points of the polygon.\n",
    "    Then add the total number of points that make up the perimeter.\n",
    "\n",
    "    Args:\n",
    "        plan (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        int: _description_\n",
    "    \"\"\"\n",
    "    polygon_area = shoelace_area(polygon)\n",
    "    boundary_points = perimeter_length(polygon)\n",
    "    interior_area = interior_points(polygon_area, boundary_points)\n",
    "    return interior_area + boundary_points    \n",
    "\n",
    "def plot_path(perimeter: list[tuple]):\n",
    "    # Extract x and y values from the perimeter\n",
    "    perimeter_x_values = [point[0] for point in perimeter]\n",
    "    perimeter_y_values = [point[1] for point in perimeter]\n",
    "    \n",
    "    # Plot the perimeter as a line\n",
    "    plt.plot(perimeter_x_values, perimeter_y_values, \n",
    "             marker=MarkerStyle('o'), linestyle='-', color=\"blue\", label=\"Perimeter\")\n",
    "\n",
    "    # Fill the inside of the perimeter\n",
    "    plt.fill(perimeter_x_values, perimeter_y_values, color=\"red\", alpha=0.8)  # Adjust alpha for transparency\n",
    "\n",
    "    plt.title('Path Plot')\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.gca().invert_yaxis()  # Invert the y-axis\n",
    "    plt.gca().set_aspect('equal', adjustable='box')  # Set equal scale \n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(data) -> int:\n",
    "    plan = parse_plan(data)\n",
    "    perimeter_path = process_turns(plan)    \n",
    "    area = total_area(perimeter_path)\n",
    "    logger.info(f\"Part 1: {area=}\")\n",
    "    plot_path(perimeter_path)\n",
    "\n",
    "    plan = parse_plan_hex(data)\n",
    "    perimeter_path = process_turns(plan)    \n",
    "    area = total_area(perimeter_path)\n",
    "    logger.info(f\"Part 2: {area=}\") \n",
    "    plot_path(perimeter_path)       \n",
    "    \n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"\\\n",
    "R 6 (#70c710)\n",
    "D 5 (#0dc571)\n",
    "L 2 (#5713f0)\n",
    "D 2 (#d2c081)\n",
    "R 2 (#59c680)\n",
    "D 2 (#411b91)\n",
    "L 5 (#8ceee2)\n",
    "U 2 (#caa173)\n",
    "L 1 (#1b58a2)\n",
    "U 2 (#caa171)\n",
    "R 2 (#7807d2)\n",
    "U 3 (#a77fa3)\n",
    "L 2 (#015232)\n",
    "U 2 (#7a21e3)\"\"\")\n",
    "sample_answers = [952408144115]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "It's easy enough to plot the new lagoon for Part 2.  With my real data, it looks like this:\n",
    "\n",
    "![Dig plan - Part 2](https://aoc.just2good.co.uk/assets/images/lava_lagoon_real_pt2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Shapely Library \n",
    "\n",
    "I figured, let's try to do this with a standard Python library.  So I'm going to use `Shapely`.\n",
    "\n",
    "The approach is similar to before, but with a few minor changes:\n",
    "\n",
    "1. We create a `Polygon` object, by passing in the list of tuples of vertices. We determine the vertices just as we did before.\n",
    "1. Then, we can obtain the total polygon area using the `area` property of the `Polygon` object. So we don't need to use _Shoelace_ to calculate the area.\n",
    "1. We still need to determine the number of interior points and add them to the perimeter channel. So we still need to use `Pick's Theorem`. But we can obtain the perimeter length by simply using the `length` property of our `Polygon`; rather than by adding up the lengths of all the edges.\n",
    "1. Now we have our interior points, and we have have the volume of the perimeter. So just add them together to get the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_with_shapely(data) -> list[int]:\n",
    "    answers = []\n",
    "    \n",
    "    for part in (0, 1):\n",
    "        plan_processor = parse_plan if part == 0 else parse_plan_hex\n",
    "        perimeter_path = process_turns(plan_processor(data))\n",
    "        poly = Polygon(perimeter_path)\n",
    "        int_area = poly.area - (poly.length // 2) + 1 # Pick's Theorem\n",
    "        answers.append(int(int_area + poly.length))\n",
    "    \n",
    "    return answers\n",
    "\n",
    "sample_input = \"\"\"\\\n",
    "R 6 (#70c710)\n",
    "D 5 (#0dc571)\n",
    "L 2 (#5713f0)\n",
    "D 2 (#d2c081)\n",
    "R 2 (#59c680)\n",
    "D 2 (#411b91)\n",
    "L 5 (#8ceee2)\n",
    "U 2 (#caa173)\n",
    "L 1 (#1b58a2)\n",
    "U 2 (#caa171)\n",
    "R 2 (#7807d2)\n",
    "U 3 (#a77fa3)\n",
    "L 2 (#015232)\n",
    "U 2 (#7a21e3)\"\"\"\n",
    "\n",
    "answers = solve_with_shapely(sample_input.splitlines())\n",
    "validate(answers[0], 62)\n",
    "validate(answers[1], 952408144115)\n",
    "logger.info(\"Tests passed!\")\n",
    "logger.info(f\"Answers with Shapely={solve_with_shapely(input_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 19: Aplenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"19\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 19 Part 1\n",
    "\n",
    "We have a list of parts, and we are deciding which to accept, and which to reject.\n",
    "\n",
    "Our sample input looks like this:\n",
    "\n",
    "```text\n",
    "px{a<2006:qkq,m>2090:A,rfg}\n",
    "pv{a>1716:R,A}\n",
    "lnx{m>1548:A,A}\n",
    "rfg{s<537:gd,x>2440:R,A}\n",
    "qs{s>3448:A,lnx}\n",
    "qkq{x<1416:A,crn}\n",
    "crn{x>2662:A,R}\n",
    "in{s<1351:px,qqz}\n",
    "qqz{s>2770:qs,m<1801:hdj,R}\n",
    "gd{a>3333:R,R}\n",
    "hdj{m>838:A,pv}\n",
    "\n",
    "{x=787,m=2655,a=1222,s=2876}\n",
    "{x=1679,m=44,a=2067,s=496}\n",
    "{x=2036,m=264,a=79,s=2244}\n",
    "{x=2461,m=1339,a=466,s=291}\n",
    "{x=2127,m=1623,a=2188,s=1013}\n",
    "```\n",
    "\n",
    "It is composed of two blocks.  First, a list of workloads, and then a list of parts.\n",
    "\n",
    "Starting with the parts... Each row is a part. And each part has been rated using the `xmas` system...\n",
    "\n",
    "- x: Extremely cool looking\n",
    "- m: Musical (it makes a noise when you hit it)\n",
    "- a: Aerodynamic\n",
    "- s: Shiny\n",
    "\n",
    "The the parts are sent through a series of named workflows. (The first block.) Each workflow has a list of rules which determine where the part goes, based on the _first condition that is true_. So when a condition is true, a part is sent to the appropriate named workflow. Some of the conditions send the part to `A` or `R`, rather than to another workflow:\n",
    "\n",
    "- `A` = Accepted\n",
    "- `R` = Rejected\n",
    "\n",
    "One of the workflows is called `in`, and we always start with this workflow.  \n",
    "\n",
    "**Sort through all of the parts you've been given; what do you get if you add together all of the rating numbers for all of the parts that ultimately get accepted?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "I've created a `Workflow` class that represents each workflow in our input. I.e.\n",
    "\n",
    "- It contains a `name` for that workflow.\n",
    "- It contains a `list` of `tuples` that represent the `rules`. Each rule takes the form: `(condition, next)`, where:\n",
    "  - `condition` could be a comparison (like `a<2006`) or simply `True` (to represent the last rule which always applied if reached).\n",
    "  - `next` is one of: the next workflow, or `A` (accepted) or `R` (rejected).\n",
    "\n",
    "- Split the specified condition (the first member of the _rule_ tuple) into its three components. I.e. left (e.g. `a`), operator (e.g. `<`), and right (e.g. `2006`).\n",
    "- Finally, our `Workflow` has an `execute()` method. This is used to push a given `part` through the `Workflow`. This method iterates through each `rule` in `rules`. It evaluates each condition, passing the in the actual values from the `part`. \n",
    "  - If the condition evaluates to `True`, then we return the next `Workflow`/`A`/`R`.\n",
    "  - If the condition evaluates to `False` then we move on to the next rule in hte workflow.\n",
    "\n",
    "Finally, to solve for Part 1:\n",
    "\n",
    "- Parse the input. The first block represents the workflows, and the second block represents parts with their _\"xmas\"_ category values.\n",
    "  - Store the workflows in a dictionary, keyed by `Workflow name`.\n",
    "  - Store the parts in a list. Each _part_ is represented as a dictionary that maps the _\"xmas\"_ categories to their values.\n",
    "- Create an `accepted` list, where we store all parts that are accepted by our workflows.\n",
    "- Iterate over each part. For each:\n",
    "  - Pass it through all the workflows, starting with _\"in\"_.\n",
    "  - Continue looping through workflows until the next state returned is either `A` or `R`. In either case, we're done with this part. If `A`, then add this part to our list. If `R`, do nothing.\n",
    "  - Otherwise, the next state is the next `Workflow`.  So execute the next `Workflow`.\n",
    "\n",
    "- Finally, add up all the part values (which are all the dictionary values) of each part, which gives a single value per part. Then add up all _these_ values to give the final answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Workflow():\n",
    "    \"\"\" \n",
    "    A Workflow is a list of rules that we must apply to a part. \n",
    "    Each rule maps a condition to another workflow.\n",
    "    Rules are executed in order.  \n",
    "    We determine the next available workflow based on the first rule that evaluates to True.\n",
    "    \"\"\"\n",
    "    name: str # e.g. \"in\", \"qqz\"\n",
    "    \n",
    "    # [ {condition: next_flow }, ...]\n",
    "    # For any given condition (if not True or False), \n",
    "    # the first operand is a part category, and the second operand is an int value\n",
    "    rules: list[tuple[str, str]] # e.g. [('s<1351', 'px'), ('True', 'qqz')]\n",
    "    \n",
    "    ops = {\n",
    "        \">\": operator.gt,\n",
    "        \"<\": operator.lt,\n",
    "    }\n",
    "    \n",
    "    def execute(self, part: dict):\n",
    "        \"\"\" Execute rules in order, for this part. Return the matching next workflow. \"\"\"\n",
    "        for condition, next_flow in self.rules:\n",
    "            if condition[0] in \"xmas\":\n",
    "                left, op, right = condition[0], condition[1], int(condition[2:]) # e.g. a, <, 2006\n",
    "                part_val = part[left]\n",
    "                res = Workflow.ops[op](part_val, right)\n",
    "            else:\n",
    "                res = True # if this rule is not a comparison, then it is our \"fallthrough\" case\n",
    "            \n",
    "            if res:\n",
    "                return next_flow # we return once we reach the first condition that is True\n",
    "        \n",
    "        assert False, \"At least one condition must be True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_input(data: str) -> tuple[dict[str,Workflow], list[dict[str,int]]]:\n",
    "    \"\"\" The input contains two blocks: workflows, and parts.\n",
    "    Parse the workflows into a dict of Workflow objects.\n",
    "    Parse the parts into a list of part dictionaries. \n",
    "    Each part will contain K:V for each of the xmas attributes. \n",
    "    For example, a part with x=2, m=3, a=4, s=5 will be represented as\n",
    "    {'x': 2, 'm': 3, 'a': 4, 's': 5} \"\"\"\n",
    "    flow_lines, part_lines = [block.splitlines() for block in data.split(\"\\n\\n\")]\n",
    "    \n",
    "    workflows = {}\n",
    "    for flow_line in flow_lines:\n",
    "        flow_name, flow_rules = flow_line[:-1].split(\"{\")\n",
    "        flow_rules = [flow_rule for flow_rule in flow_rules.split(\",\")]\n",
    "        \n",
    "        new_rules = [] # Dua protocol\n",
    "        for rule in flow_rules:\n",
    "            if \":\" in rule:\n",
    "                new_rules.append(tuple(rule.split(\":\")))\n",
    "            else:  # final condition is always true\n",
    "                new_rules.append((\"True\", rule))\n",
    "                \n",
    "        workflows[flow_name] = Workflow(flow_name, new_rules)\n",
    "    \n",
    "    parts = [] # store [ { 'x': val, 'm': val, ... }, ... ]  \n",
    "    for part_line in part_lines:\n",
    "        components = part_line[1:-1].split(\",\") # strip off the leading and trailing brackets\n",
    "        cat_vals = { cat: int(cat_val) for cat, cat_val in \n",
    "                            (component.split(\"=\") for component in components) }\n",
    "        \n",
    "        parts.append(cat_vals)\n",
    "        \n",
    "    return workflows, parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data):\n",
    "    workflows, parts = parse_input(data)\n",
    "    \n",
    "    accepted:list[dict[str, int]] = [] # store accepted parts\n",
    "    for part in parts: \n",
    "        current_flow = workflows[\"in\"]\n",
    "        while True: # loop until next state is A or R, which means we're done with workflows for this part\n",
    "            workflow_out = current_flow.execute(part)\n",
    "            if workflow_out in (\"A\", \"R\"):\n",
    "                if workflow_out == \"A\":\n",
    "                    accepted.append(part)\n",
    "                break # we're done with this part\n",
    "            \n",
    "            current_flow = workflows[workflow_out] # otherwise, move on to the next workflow\n",
    "            \n",
    "    return sum(sum(part.values()) for part in accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"px{a<2006:qkq,m>2090:A,rfg}\n",
    "pv{a>1716:R,A}\n",
    "lnx{m>1548:A,A}\n",
    "rfg{s<537:gd,x>2440:R,A}\n",
    "qs{s>3448:A,lnx}\n",
    "qkq{x<1416:A,crn}\n",
    "crn{x>2662:A,R}\n",
    "in{s<1351:px,qqz}\n",
    "qqz{s>2770:qs,m<1801:hdj,R}\n",
    "gd{a>3333:R,R}\n",
    "hdj{m>838:A,pv}\n",
    "\n",
    "{x=787,m=2655,a=1222,s=2876}\n",
    "{x=1679,m=44,a=2067,s=496}\n",
    "{x=2036,m=264,a=79,s=2244}\n",
    "{x=2461,m=1339,a=466,s=291}\n",
    "{x=2127,m=1623,a=2188,s=1013}\"\"\")\n",
    "\n",
    "sample_answers = [19114]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 19 Part 2\n",
    "\n",
    "Rather than push parts through the workflows, we need to determine which combinations of category ratings will be accepted or rejected. So we no longer care about the parts in the input data.\n",
    "\n",
    "Each category rating can have an int value from 1 to 4000 (inclusive).\n",
    "\n",
    "**Consider only your list of workflows; the list of part ratings that the Elves wanted you to sort is no longer relevant. How many distinct combinations of ratings will be accepted by the Elves' workflows?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "I've defined a recursive function called `count_ranges()`. It's purpose is to count the total number of distinct accepted part ratings. It works like this:\n",
    "\n",
    "- It takes three parameters:\n",
    "  - A dictionary that maps _xmas_ categories to a range. The values are tuples representing the inclusive `(low, high)` values of each range. Initially, the ranges will be defined as 1-4000 for each category.\n",
    "  - The current workflow name; it might also be `A` for _accepted_ and `R` for _rejected_.\n",
    "  - The dictionary of all the workflows, which we simply use for mapping workflow name to actual `Workflow`.\n",
    "\n",
    "- Our base case:\n",
    "  - Returns `0` if the workflow is `R`.\n",
    "  - If the next workflow is `A`, then all the current ranges are accepted. So take the product of all _xmas_ ranges and return it.  E.g. if each of _xmas_ had a range from 901-1000 inclusive, then we would be accepting `100*100*100*100` distinct combinations of ratings.\n",
    "\n",
    "- Next, we set `total` to 0, which accumulates the count of accepted combinations as we process the rules and recursively process workflows.\n",
    "- Now we iterate through each rule `(condition, next_flow)` in the current workflow. Recall that we should exit once a condition evaluates to True, and move on to the associated next workflow. Moving onto the next workflow is achieved by calling the function recursively, with the next workflow and updated ranges.\n",
    "  - If the current condition is a comparison involving an _xmas_ category (e.g. `a<1716`), then we split the condition into three parts: _left_, _operator_ (i.e. `<` or `>`), and _right_. The left part will always be a _category_, and the right part will always be an integer value.\n",
    "  - Then we get the current range for the category component. I.e. from the ranges that were passed to the function, and using the category that was retrieved from the condition.\n",
    "  - The operator and right value are then used to split the current range into two parts: a subrange where the condition holds true, and a subrange where the condition does not hold.\n",
    "  - For each subrange, if the true subrange is not empty, this means that this range has met the condition, and we need to pass the range to the next workflow.\n",
    "  - If the true subrange is empty, this means there are no values within the current range of the category that satisfy the condition. So we need to move on to check the next condition in the `Workflow`.\n",
    "  - If the false subrange is empty, then there are no values left that do not meet the condition. Since the condition of the current rule encompasses all possible values in the range, there's no need to evaluate subsequent rules in the current workflow. Thus, we break from the loop. The function can safely proceed to process the true subrange (if not empty) in the next workflow or finalize the count if the decision is \"Accepted\" or \"Rejected\".\n",
    "- The final rule is always `True` and acts as the fallback condition for the `Workflow`. This condition will make a recursive call to the next workflow. \n",
    "\n",
    "![Splitting ranges through workflows](https://aoc.just2good.co.uk/assets/images/2019_range_splitting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ranges(ranges:dict[str, tuple[int,int]], workflow_name: str, workflows: dict[str, Workflow]) -> int:\n",
    "    \"\"\" Count total number of distinct accepted part ratings numbers.\n",
    "\n",
    "    Args:\n",
    "        ranges (dict[str, tuple[int,int]]): Range for a given category, as (low, high) inclusive.\n",
    "        E.g. ranges = {\"a\": (1, 10), \"b\": (100, 200)} means category \"a\" is 1-10 inclusive, and \"b\" is 100-200 inclusive.\n",
    "        workflow_name (str): Name of a workflow.\n",
    "        workflows (dict[str, Workflow]): dictionary of all our workflows, keyed by name\n",
    "    \"\"\"\n",
    "    logger.debug(f\"{workflow_name=}\")\n",
    "        \n",
    "    # Base case\n",
    "    if workflow_name == \"R\": # rejected count\n",
    "        return 0\n",
    "    \n",
    "    if workflow_name == \"A\": # accepted count\n",
    "        # product of all the ranges\n",
    "        logger.debug(f\"Accepted {ranges=}\")\n",
    "        return math.prod((high-low+1) for low, high in ranges.values())\n",
    "            \n",
    "    workflow = workflows[workflow_name]\n",
    "    total = 0\n",
    "    for condition, next_flow in workflow.rules: # process rules for this workflow\n",
    "        if condition[0] in \"xmas\": # We need to split the range into two segments\n",
    "            cat, op, right_val = condition[0], condition[1], int(condition[2:]) # e.g. a, <, 2006\n",
    "            low, high = ranges[cat] # current inclusive range for this category\n",
    "            \n",
    "            # split the current range based on this this condition\n",
    "            # E.g. with current range 1,4000:\n",
    "            #      a<1716 means split into 1-1715 (true), 1716-4000 (false)\n",
    "            #      a>1716 means split into 1-1716 (false), 1717-4000 (true)\n",
    "            \n",
    "            # True for condition: we're done with this workflow, so recurse to the next\n",
    "            true_for_condition = (low, right_val - 1) if op == \"<\" else (right_val + 1, high)\n",
    "            \n",
    "            # False for condition: we need to try the next rule in this workflow\n",
    "            false_for_condition = (right_val, high) if op == \"<\" else (low, right_val)\n",
    "            \n",
    "            # Check if the condition splits are NOT empty\n",
    "            # First, check if true component non-empty. \n",
    "            # These will need to be processed by subseqauent rules.\n",
    "            if true_for_condition[0] <= true_for_condition[1]: \n",
    "                # for any True in this range, we're done with this workflow, so recurse to next workflow\n",
    "                ranges_copy = dict(ranges) # make a copy of ranges\n",
    "                ranges_copy[cat] = true_for_condition # pass through the new, true range\n",
    "                total += count_ranges(ranges_copy, next_flow, workflows) \n",
    "            \n",
    "            if false_for_condition[0] <= false_for_condition[1]: # false half non-empty\n",
    "                # for any False in this range, we need to move to the next rule in THIS workflow\n",
    "                ranges = dict(ranges) \n",
    "                ranges[cat] = false_for_condition # pass through the new, false range\n",
    "            else: # false half is empty, so nothing left to do in this workflow\n",
    "                break\n",
    "        else: # we're at the \"True\" rule in our workflow\n",
    "            assert condition == \"True\", \"We must be at the final condition.\"\n",
    "            total += count_ranges(ranges, next_flow, workflows) # recurse to next workflow\n",
    "            \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data):\n",
    "    workflows, _ = parse_input(data)\n",
    "    \n",
    "    ranges = { cat: (1, 4000) for cat in \"xmas\" }\n",
    "    logger.debug(ranges)\n",
    "    accepted = count_ranges(ranges, workflow_name=\"in\", workflows=workflows)\n",
    "    \n",
    "    return accepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [167409079868000]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2(curr_input), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 20: Pulse Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"20\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 20 Part 1\n",
    "\n",
    "Modules communicate using pulses.  \n",
    "\n",
    "- There are two types of pulses: `high` and `low`. \n",
    "- For any given module, each pulse is sent to a list of destination modules. \n",
    "- _Flip-flop_ modules (prefix `%`) are either on or off.  \n",
    "  - They are initially off. \n",
    "  - If it receives a `low` pulse it flips. When it flips on, it sends a `high` pulse. When it flips off, it sends a `low` pulse.\n",
    "  - If it receives a `high`, it does nothing.\n",
    "- _Conjunction_ modules (prefix `&`) remember the type of pulse received from each of its inputs. They initially default to remembering `low` pulse. When it receives a pulse, it updates its memory state, and then:\n",
    "  - If all inputs remembered are `high`, it sends a `low`.\n",
    "  - Otherwise it sends a `high`.\n",
    "- There is one _broadcast_ module. When it receives a pulse, it sends the same pulse to all of its destinations.\n",
    "- There is a _button_ module. When pressed, it sends `low` to _broadcast_.\n",
    "- Modules are processed in the order they are sent. (So, breadth first, rather than depth first.)\n",
    "\n",
    "Sample configuration:\n",
    "\n",
    "```text\n",
    "broadcaster -> a, b, c\n",
    "%a -> b\n",
    "%b -> c\n",
    "%c -> inv\n",
    "&inv -> a\n",
    "```\n",
    "\n",
    "**Consult your module configuration; determine the number of low pulses and high pulses that would be sent after pushing the button 1000 times, waiting for all pulses to be fully handled after each push of the button. What do you get if you multiply the total number of low pulses sent by the total number of high pulses sent?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "This is perfect for some OO programming!\n",
    "\n",
    "I've implemented an abstract class called `BaseModule`. It is _abstract_ because it must be extended by concrete subclasses, and those subclasses must implement the abstract method `receive_pulse()`.\n",
    "\n",
    "The `BaseModule` is an abstract class (marked by `abc.ABC`) which means it cannot be instantiated directly. We must extend this `BaseModule` and implement all methods marked as `abstractmethod`. This `BaseModule` provides some default properties and behaviour that are common to the various specific module types:\n",
    "\n",
    "- It defines the module `name`.\n",
    "- It contains a list of `outputs`, which are stored as strings that can be used to retrieve the actual `Module`.\n",
    "- It contains a `ModuleManager` which coordinates communication between modules.\n",
    "- It has a `send_pulse(high: bool)` method. This calls the `manager.send_pulse()` method against each module in this module's `outputs` list. (This is how onward pulses are enqueued.) It also increments the relevant _sent counter_.\n",
    "- The `receive_pulse()` method is marked as _abstract_, meaning it MUST be implemented in any concrete subclasses. The job of the `receive_pulse()` method is to process the pulse received, update any internal state, and then call the `send_pulse()` method to enqueue the onward sending.\n",
    "\n",
    "Here, I've defined a subclass for each module type, i.e.\n",
    "\n",
    "- `FlipFlopModule` - This maintains internal state, and inverts its state (and sends a matching pulse) if it receives a low pulse.\n",
    "- `ConjunctionModule` - This remembers the last pulse received by each of its inputs, and stores these values in a dictionary. It sends a _low_ pulse if all of its remembered pulses are _high_. Essentially, it's a NAND gate.\n",
    "- `Broadcaster` - This forwards on any pulse received to all _outputs_.\n",
    "\n",
    "Then I defined a `ModuleManager` which acts as the central coordinator for the interactions between the modules of the system. \n",
    "\n",
    "- It is instantiated by passing in all of the modules of the system. \n",
    "  - The `ModuleManager` assigns itself as the manager to each module.\n",
    "  - It sets the `BroadcastModule`, since this unique for the system.\n",
    "  - It links all modules to any `ConjunctionModule` instances.\n",
    "\n",
    "- The fun stuff happens in the `push_button()` method. This simulates the initial kickoff of the cycle.\n",
    "  - It enqueues the initial sending of a pulse to the `BroadcastModule`, by placing the `(source, destination, pulse type)` on a queue implemented as a [deque](https://aoc.just2good.co.uk/python/lifo_fifo). This is what the `send_pulse()` method does.\n",
    "  - Then, it processes all pulses in the queue using a `while` loop. In the loop, we call `receive_pulse()` of the next input module from the queue. Calling `receive_pulse()` will generally enqueue further pulses.  So this loop will continue to iterate until there are no further downstream pulses to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModule(abc.ABC):\n",
    "    \"\"\" Base abstract Module. This must be subclassed. \"\"\"\n",
    "    name: str\n",
    "    outputs: list[str]\n",
    "    \n",
    "    # The manager is set when we pass a Module to the ModuleManager\n",
    "    manager: ModuleManager | None = None\n",
    "\n",
    "    def send_pulse(self, high: bool):\n",
    "        \"\"\" Sends the required pulse to all output modules \"\"\"\n",
    "        assert self.manager is not None # we must have a manager, before we can send pulses\n",
    "        for output_name in self.outputs:\n",
    "            self.manager.send_pulse(self, output_name, high)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def receive_pulse(self, sender_name: str, high: bool):\n",
    "        \"\"\" Must be implement in subclasses.\n",
    "        Handle receiving a pulse from another module. \n",
    "        The implementation should: \n",
    "        - Update any required internal state\n",
    "        - Determine the pulse value that should bs subsequently sent\n",
    "        - Call send_pulse()\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "@dataclass\n",
    "class FlipFlopModule(BaseModule):\n",
    "    \"\"\" Flips its own state only when it receives a low pulse. \n",
    "    When it flips state, it sends this state onto outputs. \"\"\"\n",
    "    state = False\n",
    "\n",
    "    def receive_pulse(self, sender_name: str, high: bool):\n",
    "        if not high:\n",
    "            self.state = not self.state\n",
    "            self.send_pulse(self.state)\n",
    "\n",
    "@dataclass\n",
    "class ConjunctionModule(BaseModule):\n",
    "    \"\"\" The ConjunctionModule implements NAND (Not AND) logic within our system. \n",
    "    It stores the last received state of all of its inputs (defaulting to False). \"\"\"\n",
    "    inputs: dict[str, bool] = field(default_factory=lambda: defaultdict(bool))\n",
    "    high_pulse_sent: int | None = None # record when the first High pulse was received\n",
    "\n",
    "    def add_input(self, input_name: str):\n",
    "        self.inputs[input_name] = False\n",
    "\n",
    "    def send_pulse(self, high: bool):\n",
    "        \"\"\" Override and chain. \n",
    "        If this is the first time we are sending a high pulse, \n",
    "        record the index of this pulse. This is required for Part 2. \"\"\"\n",
    "        \n",
    "        assert self.manager is not None\n",
    "        # If this module is sending a high pulse for the first time\n",
    "        if high and self.high_pulse_sent is None:\n",
    "            # Save the index of this high pulse\n",
    "            self.high_pulse_sent = self.manager.button_pushes\n",
    "\n",
    "        return super().send_pulse(high)\n",
    "\n",
    "    def receive_pulse(self, sender_name: str, high: bool):\n",
    "        \"\"\" Updates the last received state from this input. \n",
    "        Then sends Low ONLY IF ALL input states are High. Otherwise sends High. \"\"\"\n",
    "        self.inputs[sender_name] = high\n",
    "        self.send_pulse(not all(self.inputs.values()))\n",
    "\n",
    "class BroadcastModule(BaseModule):\n",
    "    \"\"\" Sends specified pulse to all outputs. \"\"\"\n",
    "    def receive_pulse(self, sender_name: str, high: bool):\n",
    "        self.send_pulse(high)\n",
    "\n",
    "class ModuleManager:\n",
    "    \"\"\" Perform actions on behalf of the modules.\n",
    "    Registers itself as the manager of all modules. \n",
    "    Provides the capability to push the button and run the entire flow. \"\"\"\n",
    "    \n",
    "    def __init__(self, modules: list[BaseModule]):\n",
    "        \"\"\" Initialise a ModuleManager by passing in a collection of Modules.\n",
    "        Each Module in the collection is associated with this Manager. \"\"\"\n",
    "        \n",
    "        # Store pulses to be processed, as (sender, output, pulse)\n",
    "        self._pulses: deque[tuple[str, str, bool]] = deque()\n",
    "        \n",
    "        self.high_pulses = 0 # total high pulses sent by the system\n",
    "        self.low_pulses = 0 # total low pulses sent by the system\n",
    "        self.button_pushes = 0 # total number of button pushes\n",
    "        \n",
    "        # Store all modules in the system by name, and map name to Module.\n",
    "        self.modules: dict[str, BaseModule] = {}\n",
    "        \n",
    "        # Mapping between module names and instances for conjunction modules\n",
    "        # So that we can determine input modules to the Conjunction modules\n",
    "        conjunctions: dict[str, ConjunctionModule] = {}\n",
    "\n",
    "        # Process each module\n",
    "        for module in modules:\n",
    "            # This Manager is the module's Manager\n",
    "            module.manager = self\n",
    "            self.modules[module.name] = module\n",
    "\n",
    "            if isinstance(module, BroadcastModule):\n",
    "                self._broadcaster = module\n",
    "                    \n",
    "            elif isinstance(module, ConjunctionModule):\n",
    "                conjunctions[module.name] = module\n",
    "\n",
    "        # Gather inputs of each conjunction module\n",
    "        for name, module in self.modules.items():\n",
    "            for output_name in module.outputs:\n",
    "                output = conjunctions.get(output_name, None)\n",
    "                if output is not None:\n",
    "                    output.add_input(name)\n",
    "\n",
    "    def push_button(self):\n",
    "        \"\"\" Push the button, which triggers the pulse cascade for the system. \"\"\"\n",
    "        self.button_pushes += 1\n",
    "        self.send_pulse(None, self._broadcaster.name, False)\n",
    "\n",
    "        # Pulses are sent in the order that they are requested.\n",
    "        # Note that each call to receive_pulse() will then call a send_pulse()\n",
    "        # which will enqueue the next pulse.\n",
    "        # So this loop continues until there are no remaining pulses to process.\n",
    "        while self._pulses:\n",
    "            sender_name, output_name, high = self._pulses.popleft()\n",
    "            self.modules[output_name].receive_pulse(sender_name, high)\n",
    "\n",
    "    def send_pulse(self, sender: BaseModule | None, output_name: str, high: bool):\n",
    "        \"\"\" Increments pulse counter and enqueues a pulse for processing. \"\"\"\n",
    "        if high:\n",
    "            self.high_pulses += 1\n",
    "        else:\n",
    "            self.low_pulses += 1\n",
    "\n",
    "        output = self.modules.get(output_name, None)\n",
    "        if output is None:\n",
    "            return\n",
    "        sender_name = \"\" if sender is None else sender.name\n",
    "        self._pulses.append((sender_name, output_name, high))\n",
    "\n",
    "    def get_module(self, name: str) -> BaseModule:\n",
    "        module = self.modules.get(name, None)\n",
    "        if module is None:\n",
    "            raise ValueError(f\"Module {name} does not exist\")\n",
    "        return module\n",
    "\n",
    "    def get_input_modules(self, output_name: str) -> list[BaseModule]:\n",
    "        return [module for module in self.modules.values()\n",
    "                       if output_name in module.outputs]\n",
    "\n",
    "def parse_modules(lines: list[str]) -> list[BaseModule]:\n",
    "    \"\"\" Process the configuration to determine all the input and output modules.\n",
    "    Then use these to create the ModuleManager. \"\"\"\n",
    "    modules: list[BaseModule] = []\n",
    "\n",
    "    for line in lines:\n",
    "        module_part, outputs_part = [part.strip() for part in line.split(\"->\")]\n",
    "        outputs = [part.strip() for part in outputs_part.split(\",\")]\n",
    "\n",
    "        if module_part == \"broadcaster\":\n",
    "            module = BroadcastModule(module_part, outputs)\n",
    "        else:\n",
    "            module_type, module_part = module_part[0], module_part[1:]\n",
    "            match module_type: # implement a switch-case\n",
    "                case \"%\":\n",
    "                    module = FlipFlopModule(module_part, outputs)\n",
    "                case \"&\":\n",
    "                    module = ConjunctionModule(module_part, outputs)\n",
    "                case _:\n",
    "                    assert False, f\"Unknown module type {module_type}\"\n",
    "\n",
    "        modules.append(module)\n",
    "\n",
    "    return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data: list[str]):\n",
    "    \"\"\" Count pulses after 1000 button pushes \"\"\"\n",
    "    modules = parse_modules(data)\n",
    "    manager = ModuleManager(modules)\n",
    "    \n",
    "    for _ in range(1000):\n",
    "        manager.push_button()\n",
    "        \n",
    "    # Multiply the number of low and high pulses sent\n",
    "    return manager.low_pulses * manager.high_pulses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"broadcaster -> a, b, c\n",
    "%a -> b\n",
    "%b -> c\n",
    "%c -> inv\n",
    "&inv -> a\"\"\")\n",
    "\n",
    "sample_inputs.append(\"\"\"broadcaster -> a\n",
    "%a -> inv, con\n",
    "&inv -> b\n",
    "%b -> con\n",
    "&con -> output\"\"\")\n",
    "\n",
    "sample_answers = [32000000, 11687500]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 20 Part 2\n",
    "\n",
    "In Part 1, it transpired that _rx_ was an output, but not an input. So we can conclude it's the last step. \n",
    "\n",
    "In Part 2, we're told that _rx_ turns on when a single low pulse is sent to _rx_.\n",
    "\n",
    "**Reset all modules to their default states. Waiting for all pulses to be fully handled after each button press, what is the fewest number of button presses required to deliver a single low pulse to the module named rx?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "First, I tried brute force. (A long shot, I know!) Push the button until a low pulse arrives at _rx_.\n",
    "\n",
    "Conclusion: this takes 30s to execute 1m button presses, and still doesn't exit.  So, the number could be huge, and this solution probably isn't viable. I need to be smarter!\n",
    "\n",
    "Plan B...\n",
    "\n",
    "I need to understand the inputs to _rx_ and try to understand what's going on.  I started by drawing a graph of the configuration. I used [NetworkX](https://aoc.just2good.co.uk/python/networkx) to do this. With `NetworkX`, it's trivial to build a graph that contains vertices (the string names of the modules) and the edges between the vertices.\n",
    "\n",
    "![Graph of module configuration from NetworkX](https://aoc.just2good.co.uk/assets/images/module_graph.png)\n",
    "\n",
    "Or, we can draw it with **GraphViz**, which produces a nicer looking graph. Here I've used some colours and shapes to make it clearer what's going on. The last module is in red. All conjunction modules are triangles, and all flipflip modules are rectangles:\n",
    "\n",
    "![Graph of module configuration from GraphViz](https://aoc.just2good.co.uk/assets/images/graphviz_module_graph.png)\n",
    "\n",
    "We can see that:\n",
    "\n",
    "- _rx_ has a single input, called _ns_. The module _ns_ is a `ConjunctionModule`. We need _ns_ to send a _low_ pulse to _rx_.\n",
    "- The module _ns_ itself has four inputs, which are themselves Conjunction modules. Our _ns_ will only send a _low_ pulse to _rx_ if all of its inputs are _high_.\n",
    "- Thus, we need the four Conjunction module inputs to _ns_ to each send a _high_ pulse.\n",
    "\n",
    "Now we can code a solution. We know that each of these inputs will periodically sent a High pulse to _ns_. We need to determine the button press that causes all four inputs to send High at the same time. We know we can't brute-force this answer.  But we can determine the number of button presses required for _each_ input to send a High. So if we take the lowest common multiple (LCM) of these four numbers, we will get the number where all four inputs coincide.\n",
    "\n",
    "So here's how we do it:\n",
    "\n",
    "- Retrieve the input of _rx_. Check that there is only one, and that it is a `ConjunctionModule`.\n",
    "- Now get the inputs to _that_ input. Check that these are also of type `ConjunctionModule`.\n",
    "- Push the button until each of these inputs has its `high_pulse_sent` set. If it is, it will be set to the number of button presses that was required to set this particular input to _High_. When this is true for all inputs, then we have the number of button presses that was required to for each to send a high pulse.\n",
    "- It is now trivial to get the LCM.  We just call the `math.lcm()` function, and pass in the four button press values.\n",
    "\n",
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nx_module_graph(modules: dict[str, BaseModule]):\n",
    "    \"\"\" Build a NetworkX graph from our modules \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "    for name, mod in modules.items():\n",
    "        graph.add_node(name)\n",
    "        for child in mod.outputs:\n",
    "            graph.add_edge(name, child)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def draw_nx_graph(rx_input_id, graph: nx.DiGraph):\n",
    "    \"\"\" Visualise our NetworkX graph \"\"\"\n",
    "    pos = nx.spring_layout(graph)\n",
    "    \n",
    "    # Find upstream nodes of rx_input_id\n",
    "    immediate_predecessors = set(graph.predecessors(rx_input_id))\n",
    "    \n",
    "    # Create a list of colors for each node\n",
    "    node_colors = []\n",
    "    for node in graph.nodes():\n",
    "        if node == \"rx\":\n",
    "            node_colors.append(\"red\")\n",
    "        elif node == rx_input_id:\n",
    "            node_colors.append(\"yellow\")\n",
    "        elif node in immediate_predecessors:\n",
    "            node_colors.append(\"green\")\n",
    "        else:\n",
    "            node_colors.append(\"lightblue\")\n",
    "                    \n",
    "    nx.draw(graph, pos=pos, node_color=node_colors, edge_color=\"grey\", width=1, with_labels=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    plt.show()\n",
    "    \n",
    "def build_and_draw_graphviz_module_graph(modules: dict[str, BaseModule], rx_input_id):\n",
    "    # we can use X11 colour names (https://graphviz.org/doc/info/colors.html#x11)\n",
    "    color_map = defaultdict(lambda: \"cadetblue1\", {\n",
    "        \"broadcaster\": \"orangered\", \n",
    "        \"rx\": \"red\",\n",
    "        rx_input_id: \"goldenrod\",\n",
    "        \"rx_input_predecessor\": \"green\"\n",
    "    })\n",
    "    \n",
    "    edge_color_map = defaultdict(lambda: \"black\", {\n",
    "        \"broadcaster\": \"orangered\", \n",
    "        \"rx\": \"red\",\n",
    "        rx_input_id: \"goldenrod\",\n",
    "        \"rx_input_predecessor\": \"green\"\n",
    "    })\n",
    "    \n",
    "    # see styles (https://graphviz.org/docs/attr-types/style/)\n",
    "    fill_map = defaultdict(lambda: \"filled\", { \n",
    "        \"broadcaster\": \"solid\", \n",
    "        \"rx\": \"filled\",\n",
    "        rx_input_id: \"filled\",\n",
    "        \"rx_input_predecessor\": \"solid\"        \n",
    "    })\n",
    "    \n",
    "    shape_map = {\n",
    "        'BroadcastModule': 'doublecircle', \n",
    "        'FlipFlopModule': 'box', \n",
    "        'ConjunctionModule': 'invtriangle'\n",
    "    }\n",
    "    \n",
    "    predecessors = set() # Track predecessors so we can stop a predecessor being re-coloured\n",
    "    \n",
    "    # Default format is PDF\n",
    "    # See layouts https://graphviz.org/docs/layouts/ - fdp looks nice here\n",
    "    graph = graphviz.Digraph(node_attr={'style': 'filled'}, format=\"png\", engine=\"fdp\")\n",
    "    for name, mod in modules.items():\n",
    "        graph.node(name, color=color_map[name], style=fill_map[name], shape=shape_map[type(mod).__name__]) # add parent\n",
    "        \n",
    "        for child in mod.outputs:\n",
    "            if child not in predecessors: # stop a predecessor being re-coloured\n",
    "                graph.node(child, style=fill_map[child], color=color_map[child]) # add child\n",
    "                \n",
    "            if child == rx_input_id: # identify predecessor\n",
    "                predecessors.add(name)\n",
    "                graph.node(name, style=fill_map[name], color=color_map[\"rx_input_predecessor\"])\n",
    "\n",
    "            graph.edge(name, child, color=edge_color_map[child]) # see https://graphviz.org/docs/edges/\n",
    "    \n",
    "    # Rendering options. We can...\n",
    "    #   Render the graph as an output file, e.g. module_graph.png\n",
    "    #   Or render the image in memory\n",
    "    # img_data = graph.render('module_graph', view=False) # as a file\n",
    "    img_data = BytesIO(graph.pipe()) # in memory\n",
    "\n",
    "    img = Image.open(img_data).convert('RGB') # Open and display the image\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data: list[str]) -> int:\n",
    "    \"\"\" When is a low pulse sent to rx? \"\"\"\n",
    "    modules = parse_modules(data)\n",
    "    manager = ModuleManager(modules)\n",
    "\n",
    "    # Get the input to \"rx\"\n",
    "    # Use the (var,) construct to unpack the single element tuple\n",
    "    # This is because get_input_modules returns a list of modules\n",
    "    # but we know there is only one input to \"rx\"\n",
    "    (input_module,) = manager.get_input_modules(\"rx\")\n",
    "    assert isinstance(input_module, ConjunctionModule)\n",
    "    input_module = cast(ConjunctionModule, input_module)\n",
    "\n",
    "    # Visualise with a NetworkX plot\n",
    "    # graph = build_nx_module_graph(manager.modules)\n",
    "    # draw_nx_graph(input_module.name, graph)\n",
    "    \n",
    "    # Visualise with a GraphViz plot\n",
    "    build_and_draw_graphviz_module_graph(manager.modules, input_module.name)\n",
    "    \n",
    "    # And now the inputs to that ConjunctionModule...\n",
    "    inputs_to_rx_input = manager.get_input_modules(input_module.name)\n",
    "    for mod in inputs_to_rx_input: # check these are also all ConjunctionModules\n",
    "        assert isinstance(mod, ConjunctionModule)\n",
    "\n",
    "    inputs_to_rx_input = [cast(ConjunctionModule, mod) for mod in inputs_to_rx_input]\n",
    "\n",
    "    # Push the button until every relevant module outputs a high pulse\n",
    "    while any(module.high_pulse_sent is None for module in inputs_to_rx_input):\n",
    "        manager.push_button()\n",
    "    \n",
    "    high_pulses_sent = [int(module.high_pulse_sent) for module in inputs_to_rx_input]\n",
    "    \n",
    "    return math.lcm(*high_pulses_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "soln = solve_part2(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")\n",
    "# 229414480926893 right answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 21: Step Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"21\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 21 Part 1\n",
    "\n",
    "Ha ha!  An elf wants to get his steps in for the day, and wants us to help him determine which plots he can reach exactly with his remaining 64 steps!!\n",
    "\n",
    "```text\n",
    "...........\n",
    ".....###.#.\n",
    ".###.##..#.\n",
    "..#.#...#..\n",
    "....#.#....\n",
    ".##..S####.\n",
    ".##..#...#.\n",
    ".......##..\n",
    ".##.#.####.\n",
    ".##..##.##.\n",
    "...........\n",
    "```\n",
    "\n",
    "We have:\n",
    "\n",
    "- `S` - the start position. Which is also a garden plot.\n",
    "- `.` - a garden plot\n",
    "- `#` - a rock, which blocks the way\n",
    "\n",
    "We can travel N, E, S, W _and_ we can backtrack.\n",
    "\n",
    "Starting from the garden plot marked S on your map, how many garden plots could the Elf reach in exactly 64 steps?\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- I read in the data and convert to a 2D grid, where each row contains elements, and each element contains a single character.\n",
    "- Then I read through the grid, to find the `S` and mark its location as tuple of `(x, y)`.\n",
    "- Now I can do a [BFS](https://aoc.just2good.co.uk/python/shortest_paths) to flood-fill the available locations we can move to.\n",
    "\n",
    "Initially I decided to store `(location, steps taken)` in my seen state, since there are multiple ways to arrive at the same location. But I wanted to allow for backtracking, because we're allowed to go back to locations we've been to before. However, I then realised that I can simply store every location previously visited. _Why is this true?_ It is true because:\n",
    "  - If we arrive at any location and we have an _even_ number of steps remaining, then we can _always_ get back to this location on our last step. So if this condition is met, this location is a valid final location.\n",
    "  - If we arrive at any location and we have an _odd_ number of steps remaining, then there's no way to get back to this location on our last step. That's because for every _n_ steps we move away from our current location, we need to move _n_ steps back for this location to one of solution locations.  But _2n_ will _always_ be an even number. There's no way to travel _2n_ squares with an odd number of steps remaining.\n",
    "  - So to conclude: any location we visit will either be a valid solution location, or impossible to reach on our last step. In either case, we can mark it as seen and never revisit.\n",
    "\n",
    "My implementation:\n",
    "\n",
    "- The queue itself is a `deque`, since this is an efficient structure in Python for implementing a queue with frequent appending and popping.\n",
    "- I retrieve the height of the grid, and assert it is also the width.\n",
    "- I add my starting location `(0, 0)` to the queue.\n",
    "- Then I iterate until the queue is empty:\n",
    "  - I pop from the queue, to retrieve the current location and remaining steps.\n",
    "  - As noted above, if we have an even number of steps remaining, then the current location is a valid end state, so store in our `answer` set.\n",
    "  - If we have any steps remaining, we now decrement the number of available steps by 1. Then we identify all the valid neighbours (within the grid). For any valid neighbour that isn't a rock (`#`), we can add it to the queue.\n",
    "  - When we've used up all the available steps, we will stop adding neighbours to the queue, so the queue will eventually be empty.\n",
    "- Finally, we can count how many locations were in the `answer` set.\n",
    "  \n",
    "**And some visualisation:**\n",
    "\n",
    "I've plotted the solution using [matplotlib](https://aoc.just2good.co.uk/python/matplotlib). I've converted the grid to a [NumPy](https://aoc.just2good.co.uk/python/numpy) array, since this makes it really easy to plot.\n",
    "\n",
    "Here's a plot of the sample data:\n",
    "\n",
    "![2023 Day 21 Part 1 Sample](https://aoc.just2good.co.uk/assets/images/2023d21_pt1_sample_plot.png)\n",
    "\n",
    "And here's a plot of my real input data:\n",
    "\n",
    "![2023 Day 21 Part 1 Real](https://aoc.just2good.co.uk/assets/images/2023d21_pt1_real_plot.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(grid, start: tuple[int,int], steps_available: int) -> set[tuple]:\n",
    "    \"\"\" Determine all the locations we can reach in exactly the number of steps available. \n",
    "    When steps_available > 0:\n",
    "    - If steps_available is odd, there is no way to get back to this location in the steps available. \n",
    "      This location should be marked as seen.\n",
    "    - If steps_available is even, then we can ALWAYS get back to this space in the steps available.\n",
    "      This location should be marked as seen, but also as part of our solution set. \"\"\"\n",
    "    # store (location, steps remaining)\n",
    "    queue: deque[tuple[tuple[int,int], int]] = deque([(start, steps_available)])\n",
    "    seen = set() \n",
    "    answer: set[tuple] = set() # the number of locations we can get to in the required number of steps\n",
    "\n",
    "    assert len(grid) == len(grid[0]), \"The grid should be square\"\n",
    "    side_len = len(grid)\n",
    "    \n",
    "    while queue:\n",
    "        current, steps_available = queue.popleft()\n",
    "\n",
    "        if steps_available >= 0:\n",
    "            if steps_available % 2 == 0: # we can always get back to this location in an even number of steps\n",
    "                answer.add(current) # so this location will be possible in our target number of steps\n",
    "            \n",
    "            if steps_available > 0: # get next possible location\n",
    "                steps_available -= 1\n",
    "                neighbours = ((current[0]+dx,current[1]+dy) for dx,dy in (VectorDicts.DIRS.values()))\n",
    "                for neighbour in neighbours:\n",
    "                    if neighbour in seen or grid[neighbour[1]][neighbour[0]] == \"#\":\n",
    "                        continue\n",
    "                    if 0 <= neighbour[0] < side_len and 0 <= neighbour[1] < side_len:\n",
    "                        queue.append((neighbour, steps_available))\n",
    "                        seen.add(neighbour)\n",
    "                        \n",
    "    return answer\n",
    "\n",
    "def plot(grid, start, visited: set):\n",
    "    # Map the characters to numbers: S -> 0, # -> 1, . -> 2, O -> 3\n",
    "    char_to_num = {'S': 0, '#': 1, '.': 2, 'O': 3}\n",
    "    cmap = mcolors.ListedColormap(['black', 'red', 'blue', 'yellow'])\n",
    "    numeric_grid = [[char_to_num[char] for char in row] for row in grid]\n",
    "        \n",
    "    # Convert to a NumPy array for better handling by Matplotlib\n",
    "    numeric_grid = np.array(numeric_grid)\n",
    "\n",
    "    for (ci,ri) in visited: # update visited\n",
    "        numeric_grid[ri][ci] = 3\n",
    "    numeric_grid[start[1],start[0]] = 0 # update start\n",
    "    \n",
    "    # Create custom patches for the legend\n",
    "    labels = ['Start', 'Rock', 'Plot', 'Reachable']\n",
    "    colors = ['black', 'red', 'blue', 'yellow']\n",
    "    patches = [mpatches.Patch(color=colors[i], label=labels[i]) for i in range(len(colors))]\n",
    "\n",
    "    plt.imshow(numeric_grid, cmap=cmap)\n",
    "    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data, steps_available=64):\n",
    "    grid = [[char for char in row] for row in data]\n",
    "    logger.debug(\"\\n\" + \"\\n\".join(''.join(row) for row in grid))\n",
    "    \n",
    "    # Find the start\n",
    "    start: Optional[tuple[int,int]] = None\n",
    "\n",
    "    # Find start, and also assert length == 1 by assinging to single value tuple    \n",
    "    (start, ) = [(ri, ci) for ri, row in enumerate(grid)\n",
    "                          for ci, char in enumerate(row) if char == \"S\"]\n",
    "    \n",
    "    answer = bfs(grid, start, steps_available)\n",
    "    plot(grid, start, visited=answer)\n",
    "    logger.debug(f\"We have {len(answer)} final locations.\")\n",
    "    \n",
    "    return len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"\\\n",
    "...........\n",
    ".....###.#.\n",
    ".###.##..#.\n",
    "..#.#...#..\n",
    "....#.#....\n",
    ".##..S####.\n",
    ".##..#...#.\n",
    ".......##..\n",
    ".##.#.####.\n",
    ".##..##.##.\n",
    "...........\"\"\")\n",
    "sample_answers = [16]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input.splitlines(), steps_available=6), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln_64 = solve_part1(input_data, steps_available=64)\n",
    "logger.info(f\"Part 1 soln_64={soln_64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 21 Part 2\n",
    "\n",
    "Oh, the elf needs to get in **26501365 steps**, not 64. _Sad times_. And **the grid repeats infinitely** in each direction. (Repeated `S` just become `.`, obviously.)\n",
    "\n",
    "I'm guessing the current BFS won't scale. _Surprise._\n",
    "\n",
    "#### Visualising the Infinite Grid\n",
    "\n",
    "I note that the real data has additional properties that aren't true for the sample data. Depending on how you go about solving the problem, these factors may or may not be relevant. Here are some key properties of our real data:\n",
    "\n",
    "1. Our start location is in the centre of the grid.\n",
    "1. The edges are all empty. I.e. they are garden plots.\n",
    "1. Both the row and the column of our `S` location are both also empty. (This is not true for the sample data!)\n",
    "\n",
    "Let's imagine that the last property applied to the sample grid:\n",
    "\n",
    "```text\n",
    "...........\n",
    "......##.#.\n",
    ".###..#..#.\n",
    "..#.#...#..\n",
    "....#.#....\n",
    ".....S.....\n",
    ".##......#.\n",
    ".......##..\n",
    ".##.#.####.\n",
    ".##...#.##.\n",
    "...........\n",
    "```\n",
    "\n",
    "I.e. it looks like this:\n",
    "\n",
    "![2023 Day 21 Part 2 New Sample Tile](https://aoc.just2good.co.uk/assets/images/2023d21_pt2_new_sample_tile.png)\n",
    "\n",
    "These properties are useful, because in our infinite grid made up of many replicated tiles, we can always walk the empty channels (I'll call them _highways_) to get to an adjacent tile.\n",
    "\n",
    "This makes it a bit more obvious:\n",
    "\n",
    "![2023 Day 21 Part 2 New Sample Tile - Highways](https://aoc.just2good.co.uk/assets/images/2023d21_pt2_new_sample_tile_highways.png)\n",
    "\n",
    "Here, I've replicated the above tile to form a 5x5 grid of tiles, with our original grid at the centre:\n",
    "\n",
    "![2023 Day 21 Part 2 New Sample Tiles](https://aoc.just2good.co.uk/assets/images/2023d21_pt2_new_sample_tiles.png)\n",
    "\n",
    "As an aside, here's how I plotted these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tiles(data):\n",
    "    grid = [[char for char in row] for row in data]\n",
    "    grid_side_len = len(grid) # Size of the original grid\n",
    "    scale_factor = 3 # to scale up the tiles so I can insert a thin border between them\n",
    "    center_index = grid_side_len // 2\n",
    "    tiles_width = 5\n",
    "    \n",
    "    # First, set the center row and column\n",
    "    for i in range(grid_side_len):\n",
    "        grid[center_index][i] = 'C'  # Center row\n",
    "        grid[i][center_index] = 'C'  # Center column\n",
    "    \n",
    "    # Then, set the edges\n",
    "    for i in range(grid_side_len):\n",
    "        grid[0][i] = 'E'\n",
    "        grid[-1][i] = 'E'\n",
    "        grid[i][0] = 'E'\n",
    "        grid[i][-1] = 'E'\n",
    "        \n",
    "    grid[center_index][center_index] = 'S'\n",
    "\n",
    "    char_to_num = {'S': 0, '.': 1, '#': 2, 'C': 3, 'E': 3, 'G': 4}\n",
    "    cmap = mcolors.ListedColormap(['black', 'red', 'blue', 'yellow'])\n",
    "    numeric_grid = np.array([[char_to_num.get(char, 0) for char in row] for row in grid])\n",
    "    expanded_grid = np.repeat(np.repeat(numeric_grid, scale_factor, axis=0), scale_factor, axis=1)\n",
    "\n",
    "    # Create custom patches for the legend\n",
    "    colors = ['red', 'blue', 'yellow']\n",
    "    labels = ['Blocked', 'Empty plot', 'Empty highway']\n",
    "    patches = [mpatches.Patch(color=colors[i], label=labels[i]) for i in range(len(colors))]\n",
    "    \n",
    "    # plot single tile\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(\"Single Grid Tile\")\n",
    "    ax.imshow(expanded_grid, cmap=cmap, interpolation='nearest') # the interpolation helps with colour bleed\n",
    "    ax.axis(\"off\")\n",
    "    plt.subplots_adjust(right=0.8) # Adjust the subplot to make room for the legend\n",
    "    plt.legend(handles=patches, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()  \n",
    "\n",
    "    # Now we'll plot the repeating grid...\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    \n",
    "    grid[center_index][center_index] = 'C'\n",
    "    numeric_grid = np.array([[char_to_num.get(char, 0) for char in row] for row in grid])\n",
    "    expanded_grid = np.repeat(np.repeat(numeric_grid, scale_factor, axis=0), scale_factor, axis=1)\n",
    "    tiled_grid = np.tile(expanded_grid, (tiles_width, tiles_width))\n",
    "    \n",
    "    expanded_len = grid_side_len * scale_factor\n",
    "    tiles_len = expanded_len*tiles_width\n",
    "    tiles_center = tiles_len // 2\n",
    "    tiled_grid[tiles_center-1:tiles_center + 2, tiles_center-1:tiles_center + 2] = 0\n",
    "    \n",
    "    for i in range(tiles_len):\n",
    "        for j in range(tiles_len):\n",
    "            mdist = abs(i - tiles_center) + abs(j - tiles_center)\n",
    "            if mdist == expanded_len // 2:\n",
    "                tiled_grid[i-1:i+1, j-1:j+1] = 4  # 4 corresponds to green in the colormap    \n",
    "            if mdist > 0 and mdist % expanded_len == expanded_len // 2:\n",
    "                tiled_grid[i-1:i+1, j-1:j+1] = 4  # 4 corresponds to green in the colormap    \n",
    "    \n",
    "    # Insert black borders (zeros) between tiles\n",
    "    border_width = 1 \n",
    "    for i in range(1, 5):\n",
    "        tiled_grid = np.insert(tiled_grid, i * expanded_len + (i - 1) * border_width, 0, axis=0)\n",
    "        tiled_grid = np.insert(tiled_grid, i * expanded_len + (i - 1) * border_width, 0, axis=1)\n",
    "    \n",
    "    cmap = mcolors.ListedColormap(['black', 'red', 'blue', 'yellow', 'green'])\n",
    "    ax.set_title(\"Infinitely Expanding Tiles\")\n",
    "    plt.subplots_adjust(right=0.8) # Adjust the subplot to make room for the legend\n",
    "    plt.legend(handles=patches, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax.axis(\"off\")\n",
    "    ax.imshow(tiled_grid, cmap=cmap, interpolation='nearest')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# Modified to clear the row and column where S lives\n",
    "modified_sample = \"\"\"\\\n",
    "...........\n",
    "......##.#.\n",
    ".###..#..#.\n",
    "..#.#...#..\n",
    "....#.#....\n",
    ".....S.....\n",
    ".##......#.\n",
    ".......##..\n",
    ".##.#.####.\n",
    ".##...#.##.\n",
    "...........\"\"\"\n",
    "\n",
    "plot_tiles(modified_sample.splitlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've also added green points that illustrate where we land on the last step, if the number of steps available to us is one half of the tile length, or subsequently, any additional tile length away. These are represented as _diamonds_ of reachable distance. \n",
    "\n",
    "- The entire perimeter of the inner diamond is exactly reached after _tile_size // 2_ steps.\n",
    "- Each successive diamond is reached after exactly _tile_size_ more steps.\n",
    "\n",
    "#### Problem Specifics\n",
    "\n",
    "Now let's examine the real data...\n",
    "\n",
    "- It has an initial grid width of 131 and it is a square grid. This is our central tile.\n",
    "- Consequently, the number of steps to get from the centre to the edge is 65.\n",
    "- We're asked to determine all positions reachable with 26501365 steps.\n",
    "  - Note that `26501365 // 131 == 202300`. I.e. this is the number of tile lengths that we can move away from the centre in a straight line, given this number of steps. It can't be coincidence that this puzzle is from AoC 2023!!\n",
    "  - Also, `26501365 % 131 == 65`.  So, we can move exactly 202300 and one half complete tile distances from the centre.\n",
    "- Comparing to the green diamonds above:\n",
    "  - `65` steps would take us to the edge of the first diamond.\n",
    "  - `65 + 131` steps would take us to the edge of the second diamond.\n",
    "  - `65 + (2*131)` steps would take us to the edge of the third diamond.\n",
    "\n",
    "_If we ignored the blocked squares_ (i.e. the rocks), then we could easily calculate the total number of positions reachable for a given number of steps: it would be the area of our diamond (which is a square) divided by 2, since we know that we can't we can't reach even squares with an odd number of steps available. I.e.\n",
    "\n",
    "$$p = \\frac{(2n + 1)^2}{2}$$\n",
    "\n",
    "Here $p$ is the number of reachable plots, and $n$ is the number of steps. Since $n$ is how many steps we can take in any direction, the total width of our square is the sum $n$ in opposite directions (e.g. right + left), plus the square we start in. Hence $(2n + 1)$. Once we have the length of a side, we simply square this number to get the area of our square.\n",
    "\n",
    "#### Quadratic Relationshiop\n",
    "\n",
    "Crucially, it's important to note that **the relationship between the number of locations ($p$) and the number of steps ($n$) is quadratic.** (Which intuitively makes sense, since the area is given by a square.)\n",
    "\n",
    "Alas, we can't just use the formula above, because we can't ignore the configuration of our tiles. I.e. we can't ignore all the rocks!! Eliminating certain locations (i.e. the rocks)  cannot introduce higher-order terms to our equation, so the most general form will be a standard quadratic, i.e.\n",
    "\n",
    "$$p = ax^2 + bx + c$$\n",
    "\n",
    "Here, $p$ will the total number of reachable positions, and $x$ represents how many additional _diamonds_ we have, beyond our initial diamond. Recall that:\n",
    "\n",
    "- The 0th diamond is created with 65 steps from the centre.\n",
    "- The 1st diamond is created by adding an additional 131 steps.\n",
    "- The 2nd diamond is created by adding another 131 steps.\n",
    "- And so on.\n",
    "\n",
    "In order to solve for an arbitrary number of diamonds, we need to determine the coefficients $a$, $b$ and $c$.\n",
    "\n",
    "So, a quick recap:\n",
    "\n",
    "- We know we need a quadratic equation, but we don't yet know what the coefficients are.\n",
    "- We also know that we can't simply BFS for 26501365 steps, because that will never complete and our computer will blow up.\n",
    "- We know that 26501365 steps will allow us to move 202300 complete tile lengths away from the centre, plus one half of a tile length.\n",
    "\n",
    "There is a cool way to determine the coefficients of a quadratic formula, if you have three points from a quadratic plot to work with. And we can get three such points!! The technique is called the _Three Point Formula_ and it is described at these links:\n",
    "\n",
    "- [Determining Quadratic Functions](https://sites.math.washington.edu/~conroy/m120-general/quadraticFunctionAlgebra.pdf)\n",
    "- [Equation of Parabola Given 3 Points](https://www.youtube.com/watch?v=ohc1futhFYM)\n",
    "- [3 Point Equation Worked Example](https://www.mathway.com/popular-problems/Finite%20Math/618194)\n",
    "- [3 Point Equation Calculator](https://www.mathcelebrity.com/3ptquad.php)\n",
    "\n",
    "The basic idea is that if you have any quadratic curve and you know the values of three specific points on the curve, then you can extrapolate to find the general formula of the quadratic.\n",
    "\n",
    "![Three points on a quadratic curve](https://aoc.just2good.co.uk/assets/images/three-points_quadratic.png)\n",
    "\n",
    "Three valid points will be the number of steps that represent our first, second and third diamonds. So, for our actual input data, this means:\n",
    "\n",
    "- $n$ = `65` steps, $x$ = `0`\n",
    "- $n$ = `65 + 131 = 196` steps, $x$ = `1`\n",
    "- $n$ = `65 + (2*131) = 327` steps, $x$ = `2`\n",
    "\n",
    "#### BFS to Get Quadratic Coefficients Using Three Point Formula\n",
    "\n",
    "We can do a BFS with this number of steps.  But first, we need to modify our BFS to allow us to expand beyond a single tile.  I.e. such that we can cross from one tile to an adjacent tile. The BFS can be modified to allow this as follows:\n",
    "\n",
    "- Instead of storing start position and steps available in our FIFO queue, now we store: tile coordinate, position in tile, and steps remaining.  The first tile would have a tile coordinate of `(0,0)`. So the tile to the right would be `(1,0)`, the tile below would be `(0,1)`, and so on.\n",
    "- When we retrieve the neighbours of the current point, if the neighbour's coordinates fall outside of the grid boundary, then we update the tile coordinate, _and_ then offset the current coordinate _within_ the tile. For example, if we move off the current tile to the tile on the left, then the `x` coordinate _within_ the tile will now be `-1` and outside of the grid boundaries. So we add the tile width to this `x` coordinate such that the x coordinate is `width-1`, which is a valid coordinate within the grid boundaries.\n",
    "\n",
    "That's pretty much all we need to do to the BFS. My new function is called `multi_tile_bfs()`. And this modified version still works for Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_tile_bfs(grid: list, start: tuple[int,int], steps_available: int) -> int:\n",
    "    \"\"\" Modified BFS that now also includes a tile coordinate as part of state. \n",
    "    Args:\n",
    "        grid (list): 2D grid of chars\n",
    "        start (tuple[int,int]): start location in the grid\n",
    "        steps_available (int): steps available\n",
    "        \n",
    "    Returns int: count of valid locations to land on, when we've run out of steps\n",
    "    \"\"\"\n",
    "    steps_remaining = steps_available\n",
    "    current_tile = (0,0)\n",
    "    # (tile coordinate, location in tile, steps remaining)\n",
    "    queue: deque[tuple[tuple[int,int],tuple[int,int],int]] =  deque([(current_tile, start, steps_remaining)])\n",
    "    \n",
    "    seen = set() # combination of (tile, location)\n",
    "    answer: set[tuple[tuple, tuple]] = set() # the number of locations we can get to in the required number of steps\n",
    "    \n",
    "    side_len = len(grid)\n",
    "    tiles_for_steps = (steps_available // side_len) + 1\n",
    "    logger.debug(f\"{tiles_for_steps=}\")\n",
    "    \n",
    "    while queue:\n",
    "        # When we pop, we have already updated tile and location in the tile to be valid        \n",
    "        current_tile, current_locn, steps_remaining = queue.popleft()\n",
    "        \n",
    "        if steps_remaining >= 0:\n",
    "            if steps_remaining % 2 == 0: # we can always get back to this location in an even number of steps\n",
    "                answer.add((current_tile, current_locn)) # so this location will be possible in our target number of steps\n",
    "            \n",
    "            if steps_remaining > 0: # get next possible location\n",
    "                steps_remaining -= 1\n",
    "                neighbours = [(current_locn[0]+dx,current_locn[1]+dy) for dx,dy in (VectorDicts.DIRS.values())]\n",
    "                for neighbour in neighbours: # update current tile, and offset location in tile by tile width/height, as required\n",
    "                    new_tile = current_tile\n",
    "                    if neighbour[0] < 0: # move to tile on the left\n",
    "                        new_tile = (current_tile[0]-1, current_tile[1])\n",
    "                        neighbour = (neighbour[0]+side_len, neighbour[1])\n",
    "                    if neighbour[0] >= side_len: # move to tile on the right\n",
    "                        new_tile = (current_tile[0]+1, current_tile[1])\n",
    "                        neighbour = (neighbour[0]-side_len, neighbour[1])\n",
    "                    if neighbour[1] < 0: # move to tile above\n",
    "                        new_tile = (current_tile[0], current_tile[1]-1)\n",
    "                        neighbour = (neighbour[0], neighbour[1]+side_len)\n",
    "                    if neighbour[1] >= side_len: # move to tile below\n",
    "                        new_tile = (current_tile[0], current_tile[1]+1)\n",
    "                        neighbour = (neighbour[0], neighbour[1]-side_len)\n",
    "\n",
    "                    if (new_tile, neighbour) in seen or grid[neighbour[1]][neighbour[0]] == \"#\" :\n",
    "                        continue # do nothing\n",
    "                    \n",
    "                    # With max distance of 327, this is 2.5 tiles. So 3 either side should be neough.\n",
    "                    # -3, -2, -1, 0, 1, 2, 3 = 7x7\n",
    "                    if (abs(current_tile[0]) > tiles_for_steps or abs(current_tile[1]) > tiles_for_steps):\n",
    "                        logger.debug(f\"{new_tile=}\")\n",
    "                        assert False, \"Not enough steps to move further out\"\n",
    "                                                        \n",
    "                    queue.append((new_tile, neighbour, steps_remaining))\n",
    "                    seen.add((new_tile, neighbour))\n",
    "                            \n",
    "    return len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachable_plots(data, steps_available:int):\n",
    "    \"\"\" Return the number of plots that can be reached in the given number of steps.\n",
    "    We call multi_tile_bfs() to perform a BFS for a limited number of repeating tiles.\n",
    "\n",
    "    Args:\n",
    "        data (list): List of strings representing the grid.\n",
    "        steps_available (int): Number of steps available.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of plots that can be reached.\n",
    "    \"\"\"\n",
    "    grid = [[char for char in row] for row in data]\n",
    "    grid_size = len(grid)    \n",
    "    \n",
    "    logger.debug(f\"Grid width={grid_size}\") \n",
    "    assert grid_size == len(grid[0]), \"The grid should be square\"\n",
    "    assert grid_size % 2 == 1, \"The grid size should be odd\"\n",
    "\n",
    "    # Retrieve the start position\n",
    "    (start, ) = [(ri, ci) for ri, row in enumerate(grid)\n",
    "                          for ci, char in enumerate(row) if char == \"S\"]\n",
    "\n",
    "    assert start[0] == start[1] == grid_size // 2, \"Start is in the middle\"\n",
    "               \n",
    "    answer = multi_tile_bfs(grid, start, steps_available)\n",
    "    logger.debug(f\"We have {answer} final plots for {steps_available} steps.\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll test my `multi_grid_BFS()` using the sample input and the sample answers provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "sample_input = \"\"\"\\\n",
    "...........\n",
    ".....###.#.\n",
    ".###.##..#.\n",
    "..#.#...#..\n",
    "....#.#....\n",
    ".##..S####.\n",
    ".##..#...#.\n",
    ".......##..\n",
    ".##.#.####.\n",
    ".##..##.##.\n",
    "...........\"\"\"\n",
    "\n",
    "step_counts = [6, 10, 20, 50, 100, 500]\n",
    "sample_answers = [16, 50, 216, 1594, 6536, 167004]\n",
    "\n",
    "for sample_step_count, sample_answer in zip(step_counts, sample_answers):\n",
    "    validate(reachable_plots(sample_input.splitlines(), sample_step_count), sample_answer)\n",
    "    \n",
    "logger.info(\"Tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If those all give the right numbers (and they do) then we're ready to BFS our real data. As determined before, we need to BFS for step values of `65`, `196`, and `327`. But first, I also test that my new BFS gives the same answer with `64` steps as we achieved for Part 1. This helps me get confidence that I haven't broken my BFS for the large grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to reach the edges of our three diamonds, for our actual input data\n",
    "step_counts = [64, 65, 196, 327] # 64 is just to check it matches what we had before\n",
    "\n",
    "validate(reachable_plots(input_data, step_counts[0]), soln_64)\n",
    "\n",
    "plot_counts = [(step_count, reachable_plots(input_data, step_count)) for step_count in step_counts[1:]]\n",
    "logger.info(f\"{plot_counts=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!  So now we have the number of reachable plots $p$ for three values of $x$.\n",
    "\n",
    "Let's substitute in these values of $x$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(0) &= a(0)^2 + b(0) + c \\\\\n",
    "     &= c \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This gives us the value of $c$. Next, for $x=1$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(1) &= a(1)^2 + b(1) + c \\\\\n",
    "     &= a + b + c \\\\\n",
    "     &= a + b + p(0) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And for $x=2$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(2) &= a(2)^2 + b(2) + c \\\\\n",
    "     &= 4a + 2b + c \\\\\n",
    "     &= 4a + 2b + p(0) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now subtract $p(1)$ from $p(2)$ to solve for $a$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(2) - 2 \\cdot p(1) &= 4a + 2b + p(0) - 2 \\cdot (a + b + p(0)) \\\\\n",
    "              &= 2a - p(0) \\\\\n",
    "           2a &= p(2) - 2 \\cdot p(1) + p(0) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Rearrange $p(1)$ again, to solve for $b$:\n",
    "\n",
    "$$ b = p(1) - a - p(0) $$\n",
    "\n",
    "So now we can determine the values of $a$, $b$ and $c$ by plugging in the values of $p(0)$, $p(1)$, and $p(2)$, which we determined using BFS. Note that these coefficients are only valid for our _real data_ grid, since these coefficients are calculated based on our specific grid sizes and resulting _diamond_ sizes.\n",
    "\n",
    "Finally, we run the quadratic formula, but setting $x$ to `202300`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_quadratic(data: list, plot_counts: list[int], steps:int):\n",
    "    \"\"\" Return the total number of reachable plots in a specified number of steps, \n",
    "    by calculating the answer to the quadratic formula. \n",
    "    Here we calculate the coefficients a, b and c by using three sample values,\n",
    "    obtained from a smaller grid.\n",
    "\n",
    "    Args:\n",
    "        data (list): The original grid tile.\n",
    "        plot_counts (list[int]): The plot counts determined for small step counts.\n",
    "        steps (int): The number of steps we must take.\n",
    "    \"\"\"\n",
    "    assert len(plot_counts) > 2, \"We need at least three points to solve\"\n",
    "    grid = [[char for char in row] for row in data]\n",
    "    grid_size = len(grid)\n",
    "\n",
    "    # determine coefficients\n",
    "    c = plot_counts[0] # p(0)\n",
    "    a = (plot_counts[2] - 2*plot_counts[1] + plot_counts[0]) // 2 # (p(2) - 2*p(1) + p(0)) / 2\n",
    "    b = plot_counts[1] - a - c # p(1) - a - c\n",
    "\n",
    "    logger.debug(f\"Coefficients: a={a}, b={b}, c={c}\")\n",
    "\n",
    "    # determine the number of steps we can take in whole tile lengths\n",
    "    x = (steps - grid_size//2) // grid_size # number of whole tile lengths\n",
    "    logger.debug(f\"Solving for x={x}\")\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ans = solve_quadratic(input_data, plot_counts=[ct[1] for ct in plot_counts], steps=26501365)\n",
    "logger.info(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Could Use SymPy!\n",
    "\n",
    "It occurred to me that I spent a fair amount of time manually rearranging the quadratics for our values of $p(0)$, $p(1)$, and $p(2)$.  But actually, if I use SymPy, I don't need to do this at all.  Instead, I can provide a general solution that will solve for any quadratic with three values.\n",
    "\n",
    "If you're interesting in reading more about this solution, I have documented in [here](https://medium.com/python-in-plain-english/finding-the-coefficients-of-a-quadratic-with-sympy-777cd9479ea1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_quadratic_with_sympy(data: list, plot_counts: list[int], steps:int):\n",
    "    \"\"\" Return the total number of reachable plots in a specified number of steps, \n",
    "    by calculating the answer to the quadratic formula. \n",
    "    Here we calculate the coefficients a, b and c by using three sample values,\n",
    "    obtained from a smaller grid.\n",
    "\n",
    "    Args:\n",
    "        data (list): The original grid tile.\n",
    "        plot_counts (list[int]): The plot counts determined for small step counts.\n",
    "        steps (int): The number of steps we must take.\n",
    "    \"\"\"\n",
    "    assert len(plot_counts) > 2, \"We need at least three points to solve\"\n",
    "    grid = [[char for char in row] for row in data]\n",
    "    grid_size = len(grid)\n",
    "\n",
    "    a, b, c = sympy.symbols('a b c')  # Setup the symbols for SymPy\n",
    "\n",
    "    # Set up three equations based on the given plot_counts\n",
    "    equations = []\n",
    "    equations.append(sympy.Eq(c, plot_counts[0]))\n",
    "    equations.append(sympy.Eq(a + b + c, plot_counts[1]))\n",
    "    equations.append(sympy.Eq(4*a + 2*b + c, plot_counts[2]))\n",
    "\n",
    "    # Solve the system of equations to determine coefficients\n",
    "    solutions = sympy.solve(equations, (a, b, c))\n",
    "    logger.debug(f\"Coefficients: a={solutions[a]}, b={solutions[b]}, c={solutions[c]}\")\n",
    "\n",
    "    # determine the number of whole tile lengths we need\n",
    "    x = (steps - grid_size//2) // grid_size # number of whole tile lengths\n",
    "    logger.debug(f\"Solving for x={x}\")\n",
    "\n",
    "    # Calculate the total number of reachable plots for the given number of steps\n",
    "    total_plots = solutions[a] * x**2 + solutions[b] * x + solutions[c]\n",
    "    return total_plots.evalf()\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ans = solve_quadratic_with_sympy(input_data, plot_counts=[ct[1] for ct in plot_counts], steps=26501365)\n",
    "logger.info(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful resources\n",
    "\n",
    "- [Reddit 2023 Day 21 Megathread](https://www.reddit.com/r/adventofcode/comments/18nevo3/2023_day_21_solutions/)\n",
    "- [Geometric solution - Useful sketches](https://github.com/villuna/aoc23/wiki/A-Geometric-solution-to-advent-of-code-2023,-day-21)\n",
    "- [Ian's YouTube Walkthrough with useful commentary on the quadratic](https://www.youtube.com/watch?v=99Mjs1i0JxU&lc=UgxwS0greNfsrFXYPA14AaABAg.9ydjxdKIlaC9ydpDNgSwyb)\n",
    "- [Finding the Coefficients of a Quadratic with SymPy](https://medium.com/python-in-plain-english/finding-the-coefficients-of-a-quadratic-with-sympy-777cd9479ea1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 22: Sand Slabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"22\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 22 Part 1\n",
    "\n",
    "We need to disintegrate bricks of sand into free flowing sand. The bricks are falling into a stack.\n",
    "\n",
    "We have a snaptshot of the bricks whilst they were falling. The example looks like this:\n",
    "\n",
    "```text\n",
    "1,0,1~1,2,1\n",
    "0,0,2~2,0,2\n",
    "0,2,3~2,2,3\n",
    "0,0,4~0,2,4\n",
    "2,0,5~2,2,5\n",
    "0,1,6~2,1,6\n",
    "1,1,8~1,1,9\n",
    "```\n",
    "\n",
    "- Each row represents two 3D coordinates, `x,y,z`. Each coordinate is a 1x1x1 cube. Thus, each brick can be represented as a collection of cubes.\n",
    "  - The z-axis extends up into their air, whereas the x and y axes represent the horizontal plane at any given value of z.\n",
    "  - 2,2,2~2,2,2: This is a single cube brick.\n",
    "  - 0,0,10~1,0,10: This is a two cube brick, oriented horizontally. I.e. the x axis has coordinates 0 and 1.\n",
    "  - 0,0,1~0,0,10: This is a 10-cube brick, oriented vertically. I.e. the y axis is from 1 to 10 inclusive.\n",
    "- The ground is at `z=0`.\n",
    "- The lowest possible z coordinate of a brick is 1. I.e. resting above the ground.\n",
    "- The input shows the order the bricks fell. I.e. FIFO with the first at the top. (Careful... The real data isn't presorted in this way!)\n",
    "- Some bricks are still falling through the air at the time of this snapshot. For example, there is a one unit z gap between the last two bricks.\n",
    "- Bricks always fall with constant orientation. I.e. only their z value will change as they fall.\n",
    "\n",
    "**Determine how the blocks will settle based on the snapshot. Once they've settled, consider disintegrating a single brick; how many bricks could be safely chosen as the one to get disintegrated?**\n",
    "\n",
    "_Oh god.  It's Jenga!_\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "We need to:\n",
    "\n",
    "- Determine the configuration of the bricks once they've settled.\n",
    "- Determine which bricks then support other bricks.\n",
    "- We can disintegrate any bricks that are not the _sole_ support of another brick.\n",
    "\n",
    "Approach:\n",
    "\n",
    "- I create `Brick` class, to store the state of each brick. As a minimum, our brick needs a pair of `(x,y,z)` tuples that represent the two opposite coordinates.\n",
    "- I parse the input data, and convert each row into a `Brick`. This is easy enough, since we just split at the `~` to get the two sets of coordinates that we will use for our pair of tuples.\n",
    "- Next, I've added `min_for_axis()` and `max_for_axis()` methods to my `Brick` class. These methods determine the minimum and maximum values for the axis specified, where `0 = x`, `1 = y`, `2 = z`.\n",
    "- We can go through each `Brick` and determine the minima and maxima for `x` and `y`, which gives us the overall `x,y` area that all our bricks fall within.  Think of this as representing the overall horizontal and vertical widths of our Jenga tower.\n",
    "- Next, I want to create a 2D grid for this area, with each location storing the maximum height (`z` value) that has been achieved in our stack so far. I do this by creating a [NumPy](https://aoc.just2good.co.uk/python/numpy) grid called `height_map` with the required dimensions, and initialising all values to `0` initially.  I.e. the flat surface that we start with.\n",
    "- Now the fun part... It's time to drop the bricks!! A brick falls down through the air, but the coordinates it occupies in the `x,y` plane remains constant. Think of it as a falling down a rectangular groove. In this groove, it can not tilt or rotate.\n",
    "- First... **It's crucial to _sort_ the bricks in ascending `z` order!** This is because we want to be able to drop a brick and check for bricks below it. And we want to be sure that the bricks are settling in `z` order, i.e. that no bricks that come _later_ in the input can be at lower height than our current brick. In the sample data, they are already sorted this way. But in the real data, this isn't the case.  (This delayed me getting the right answer for a while!!)\n",
    "  - So I've added a `__lt__()` method to my `Brick` class, which allows me to sort on the `z` value. \n",
    "  - In Python, implementing `__lt__()` in your class is typically sufficient to allow sorting of classes using standard sort functions.\n",
    "- Next, I want to determine the _xy area_ occupied by my current brick. So I've implemented an `xy_area()` method. This simply returns the two opposite coordinates of the xy rectangle.\n",
    "- Then, I determine what is the current highest point (`z` value) of my `height_map`, for the section of section of the grid that corresponds to my brick's `xy_area`. This is because the section might contain any number of existing bricks. And one might be taller than others.  But we only care about the tallest point in the section.  Our new brick will end up sitting _on top_ of the tallest point.\n",
    "- I then update my brick's lowest `z` value to be equal to this previous highest point, plus `1`.\n",
    "- Next, I need to update the `height_map` such that the entire `xy_area` section has a height value that is equal to the previous maximum height, plus the height of our `Brick`. So now, our `height_map` is up to date and includes the new `Brick`.\n",
    "\n",
    "That's it for settling the brick into place.\n",
    "\n",
    "All that remains is to determine which existing bricks (which I call _base bricks_) our new brick sits on.  I do this by:\n",
    "\n",
    "- Adding our newly laid `Brick` into a dictionary that contains a set of all bricks at a given `z` level.\n",
    "- Then, for our newly laid `Brick`, I determine if my brick's `xy_area` intersects with any of the bricks in the set for the `z` level we've just landed on top of.  To perform the intersection check, I've added a method to my `Brick` called `area_intersects()`.  It works by:\n",
    "  - Checking if the right edge of either brick is to the left of the left edge of the other brick. If so, these don't intersect.\n",
    "  - And then doing the same check with the y axis.\n",
    "- If there is an intersection, then this new brick must sit on top of one or more base bricks. I add these base bricks to a `brick_to_base_bricks` dictionary, which maps this current `Brick` to a set of `Base` bricks.\n",
    "\n",
    "- So we can build a `Brick` class that knows how to determine its own `x,y` space, and can also check for the intersection with another brick's `x,y` space. If the two bricks intersect for any given value of `z` then the falling brick can not descend to this `z` level, since it would hit another brick.\n",
    "\n",
    "Lastly, to solve for Part 1, we just want to get all the _base_ bricks, where they are the one and only member of a `brick_to_base_bricks` set. Where this is true, this base brick is alone, and there are no other bricks supporting this particular resting brick. I've labelled these as `critical_base_bricks`. We just subtract this number from the total number of bricks, in order to get the bricks that can safely be disintegrated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True) # so it's immutable and we can store it in a set\n",
    "class Brick():\n",
    "    \"\"\" A brick is a cuboid. It is represented by two opposite corners, which are each tuples of (x,y,z). \"\"\"\n",
    "    \n",
    "    corner_1: tuple[int,int,int]\n",
    "    corner_2: tuple[int,int,int]\n",
    "    \n",
    "    def __lt__(self, other: Brick):\n",
    "        \"\"\" Compare based on z axis minimum. \"\"\"\n",
    "        return self.min_for_axis(2) < other.min_for_axis(2)\n",
    "    \n",
    "    def max_for_axis(self, axis: int):\n",
    "        \"\"\" The maximum value for this axis. Valid axes are 0, 1, 2 for x, y, z, respectively. \"\"\"\n",
    "        return self._axis_limit(max, axis)\n",
    "    \n",
    "    def min_for_axis(self, axis: int):\n",
    "        \"\"\" The minimum value for this axis. Valid axes are 0, 1, 2 for x, y, z, respectively. \"\"\"\n",
    "        return self._axis_limit(min, axis)\n",
    "    \n",
    "    def _axis_limit(self, op: Callable, axis: int):\n",
    "        assert 0 <= axis < 3, \"Valid axes are 0, 1, 2 for x, y, z, respectively. \"\n",
    "        return op(self.corner_1[axis], self.corner_2[axis])\n",
    "    \n",
    "    def update_z(self, new_bottom_z: int) -> Brick:\n",
    "        \"\"\" Update the z values of this brick, by setting a new min z value.\n",
    "        Note that Brick is frozen, so we have to return a new Brick. \"\"\"\n",
    "        z_delta = self.min_for_axis(2) - new_bottom_z\n",
    "        return Brick((self.corner_1[0], self.corner_1[1], self.corner_1[2] - z_delta), \n",
    "                     (self.corner_2[0], self.corner_2[1], self.corner_2[2] - z_delta))\n",
    "    \n",
    "    def height(self) -> int:\n",
    "        \"\"\" The height of this brick, i.e. inclusive z length \"\"\"\n",
    "        return (self.max_for_axis(2)-self.min_for_axis(2) + 1)\n",
    "    \n",
    "    def xy_area(self):\n",
    "        \"\"\" Return the x,y area occupied by this brick, as a tuple of ((corner),(corner)) \"\"\"\n",
    "        return ((self.min_for_axis(0), (self.min_for_axis(1))), \n",
    "                (self.max_for_axis(0), (self.max_for_axis(1))))\n",
    "        \n",
    "    def area_intersects(self, other: Brick) -> bool:\n",
    "        \"\"\" Determine if the xy_area of this brick intersects with the xy_area of another brick. \"\"\"\n",
    "        self_x1, self_y1 = self.xy_area()[0]\n",
    "        self_x2, self_y2 = self.xy_area()[1]\n",
    "        other_x1, other_y1 = other.xy_area()[0]\n",
    "        other_x2, other_y2 = other.xy_area()[1]\n",
    "        \n",
    "        # Check if one brick is to the left of the other\n",
    "        if self_x2 < other_x1 or other_x2 < self_x1:\n",
    "            return False\n",
    "\n",
    "        # Check if one brick is above the other\n",
    "        if self_y2 < other_y1 or other_y2 < self_y1:\n",
    "            return False\n",
    "\n",
    "        return True        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bricks(data: list[str]) -> list[Brick]:\n",
    "    \"\"\" Parse rows like: 1,0,1~1,2,1 \"\"\"\n",
    "    bricks = []\n",
    "    for line in data:\n",
    "        corner_1, corner_2 = line.split(\"~\")\n",
    "        (x1, y1, z1) = [int(val) for val in corner_1.split(\",\")]\n",
    "        (x2, y2, z2) = [int(val) for val in corner_2.split(\",\")]\n",
    "        bricks.append(Brick((x1, y1, z1), (x2, y2, z2)))\n",
    "        \n",
    "    return bricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data):\n",
    "    bricks = parse_bricks(data) # in drop order\n",
    "    bricks.sort() # naughty real input... Not in ascending z order!!\n",
    "    bricks_at_level: dict[int,set[Brick]] = defaultdict(set) # { 5: { brick_4, brick5 }}\n",
    "    upper_to_base_bricks: dict[Brick,set[Brick]] = defaultdict(set) # store the bricks supported directly by this break { brick_1: { brick_2, brick3, ... }, ... }\n",
    "    \n",
    "    min_x = max_x = 0\n",
    "    min_y = max_y = 0\n",
    "    \n",
    "    # let's get the overall xy area occupied by our stack\n",
    "    for brick in bricks:\n",
    "        min_x = min(brick.min_for_axis(0), min_x)\n",
    "        max_x = max(brick.max_for_axis(0), max_x)\n",
    "        min_y = min(brick.min_for_axis(1), min_y)\n",
    "        max_y = max(brick.max_for_axis(1), max_y)\n",
    "    \n",
    "    assert min_x == min_y == 0, \"Our area has min x,y at 0,0\"\n",
    "    logger.debug(f\"Min (x,y) = ({min_x}, {min_y})\")\n",
    "    logger.debug(f\"Max (x,y) = ({max_x}, {max_y})\")\n",
    "    \n",
    "    # Initialise to a flat surface\n",
    "    height_map = np.zeros((max_y-min_y+1, max_x-min_x+1), dtype=np.int32)\n",
    "    \n",
    "    # Settle the bricks\n",
    "    for brick in bricks: # bricks drop in z order\n",
    "        (x1, y1), (x2, y2) = brick.xy_area() # the x,y area this brick occupies\n",
    "        old_max_height = int(np.max(height_map[y1:y2+1, x1:x2+1])) # Retrieve the max height in that section\n",
    "        brick.update_z(old_max_height+1) # place the brick here, i.e. immediatey above the max height\n",
    "        new_max_height = old_max_height + brick.height()\n",
    "        height_map[y1:y2+1, x1:x2+1] = new_max_height # update the height of this section\n",
    "        bricks_at_level[new_max_height].add(brick) # include this brick in bricks that make up the new max level\n",
    "        \n",
    "        # what bricks does this brick sit on?\n",
    "        # Get all the bricks that were at the previous height, and work out which ones intersect\n",
    "        upper_to_base_bricks[brick].update([base_brick for base_brick in bricks_at_level[old_max_height] \n",
    "                                                       if base_brick.area_intersects(brick)])\n",
    "    \n",
    "    # create empty sets\n",
    "    critical_bricks = set() # base bricks that solely support an upper brick, i.e. where an upper brick has ONLY ONE base brick\n",
    "    base_to_upper_bricks = { brick: set() for brick in bricks }\n",
    "    \n",
    "    for brick in bricks:\n",
    "        if len(upper_to_base_bricks[brick]) == 1: # this upper brick depends on ONLY 1 base brick\n",
    "            (base_brick, ) = upper_to_base_bricks[brick] # so this base brick is critical\n",
    "            critical_bricks.add(base_brick)\n",
    "        \n",
    "        # This upper brick has one or more base bricks\n",
    "        for base_brick in upper_to_base_bricks[brick]:\n",
    "            base_to_upper_bricks[base_brick].add(brick) # store the upper brick that depends on this base brick\n",
    "    \n",
    "    # Check every brick is mapped to base and uppers   \n",
    "    assert len(upper_to_base_bricks) == len(base_to_upper_bricks) == len(bricks)\n",
    "    \n",
    "    safe_bricks_count = len(bricks) - len(critical_bricks) # if not critical, then it's safe to remove\n",
    "    logger.debug(f\"{safe_bricks_count=}\")\n",
    "    \n",
    "    return safe_bricks_count, upper_to_base_bricks, base_to_upper_bricks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"\\\n",
    "1,0,1~1,2,1\n",
    "0,0,2~2,0,2\n",
    "0,2,3~2,2,3\n",
    "0,0,4~0,2,4\n",
    "2,0,5~2,2,5\n",
    "0,1,6~2,1,6\n",
    "1,1,8~1,1,9\"\"\")\n",
    "sample_answers = [5]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input.splitlines())[0], curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "safe_brick_count, upper_to_base_bricks, base_to_upper_bricks = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={safe_brick_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 22 Part 2\n",
    "\n",
    "We need to determine how many other bricks would fall, if a given brick is disintegrated.\n",
    "\n",
    "**For each brick, determine how many other bricks would fall if that brick were disintegrated. What is the sum of the number of bricks that would fall?**\n",
    "\n",
    "Well, this was _inevitable!!_\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "Iterate over all the bricks. For each...\n",
    "- Get all upper bricks that depend on this brick, and only this brick. Put these on a queue. And mark them as falling (by adding them to a `set`). Also, mark the removed brick as falling.\n",
    "- Now pop these upper bricks off the queue. For each:\n",
    "  - Check if they're already in falling. \n",
    "  - If ALL the base bricks of this upper brick are falling, then this upper brick is also falling. So mark it as such and add it to the queue.\n",
    "- When the queue is empty, add the number of falling queues to the total. Subtract 1 to account for the original disintegrated brick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data, upper_to_base_bricks: dict[Brick, set], base_to_upper_bricks: dict[Brick, set]) -> int:\n",
    "    \"\"\" Determine the number of bricks that will fall when we remove a single brick.\n",
    "    Repeat for every brick, and the return the total.\n",
    "\n",
    "    Returns:\n",
    "        int: total number of bricks that would fall\n",
    "    \"\"\"\n",
    "    bricks = parse_bricks(data) # in drop order\n",
    "    bricks.sort() # naughty real input... Not in ascending z order!!\n",
    "    \n",
    "    total = 0 # total of falling bricks\n",
    "    # remove each brick, and determine how many bricks would then fall\n",
    "    for brick in upper_to_base_bricks:\n",
    "        # bricks that will fall is all the bricks that are dependent on a single base brick\n",
    "        queue = deque(upper for upper in base_to_upper_bricks[brick] \n",
    "                            if len(upper_to_base_bricks[upper]) == 1)\n",
    "        falling = set(queue) # all bricks that will fall\n",
    "        falling.add(brick) # the brick that we've removed\n",
    "        \n",
    "        while queue:\n",
    "            upper = queue.popleft() # pop the next falling (upper) brick\n",
    "            \n",
    "            # now look at all bricks that are supported by a falling (upper) brick\n",
    "            # this allows us to cascade up\n",
    "            for base in (base_to_upper_bricks[upper] - falling): # and not already falling\n",
    "                if upper_to_base_bricks[base] < falling: # if ALL base bricks are in falling\n",
    "                    queue.append(base) # this brick is no longer supported\n",
    "                    falling.add(base) # so it is falling\n",
    "                    \n",
    "        total += len(falling) - 1 # -1 because we need to account for the original removed brick\n",
    "    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [7]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    curr_safe, curr_upper_to_base_bricks, curr_base_to_upper_bricks = solve_part1(curr_input.splitlines())\n",
    "    validate(solve_part2(curr_input.splitlines(), curr_upper_to_base_bricks, curr_base_to_upper_bricks), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2(input_data, upper_to_base_bricks, base_to_upper_bricks)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 23: A Long Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"23\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 23 Part 1\n",
    "\n",
    "We have a map of _Snow Island_, which contains paths (`.`), forest (`#`) and steep slopes (`^, >, v, <`). E.g.\n",
    "\n",
    "```text\n",
    "#S#####################\n",
    "#.......#########...###\n",
    "#######.#########.#.###\n",
    "###.....#.>.>.###.#.###\n",
    "###v#####.#v#.###.#.###\n",
    "###.>...#.#.#.....#...#\n",
    "###v###.#.#.#########.#\n",
    "###...#.#.#.......#...#\n",
    "#####.#.#.#######.#.###\n",
    "#.....#.#.#.......#...#\n",
    "#.#####.#.#.#########v#\n",
    "#.#...#...#...###...>.#\n",
    "#.#.#v#######v###.###v#\n",
    "#...#.>.#...>.>.#.###.#\n",
    "#####v#.#.###v#.#.###.#\n",
    "#.....#...#...#.#.#...#\n",
    "#.#########.###.#.#.###\n",
    "#...###...#...#...#.###\n",
    "###.###.#.###v#####v###\n",
    "#...#...#.#.>.>.#.>.###\n",
    "#.###.###.#.###.#.#v###\n",
    "#.....###...###...#...#\n",
    "#####################.#\n",
    "```\n",
    "\n",
    "We need to find our way from the start on the top row to the exit on the bottom row. Rules:\n",
    "\n",
    "- If we step on an arrow then the next move must be in the direction the arrow is pointing.\n",
    "- We can never visit the same tile twice.\n",
    "\n",
    "**Find the _longest_ hike you can take through the hiking trails listed on your map. How many steps long is the longest hike?**\n",
    "\n",
    "Some observations:\n",
    "\n",
    "- The grid represents a maze.\n",
    "- For most locations, there are is no choice but to move forward.\n",
    "- Very few locations are intersections which offer a branch.\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "- We need to represent the grid as a [graph](https://aoc.just2good.co.uk/python/graph). Recall that a graph is a model that represents a set of _vertices_, linked together by _edges_.\n",
    "- We can use _path contraction_ to eliminate all the nodes where there is no path choice. I.e. for nodes where the number of edges is two, then we can eliminate these nodes from the graph. Why?  Because if the number of edges is two, then the node only has a single forward and a single backward path. And we can't walk backwards, so we must continue forwards. This helps improve the performance of our longest path solution.\n",
    "- Thus, vertices that we want to keep will have at least three viable next moves.\n",
    "\n",
    "How to do this?\n",
    "\n",
    "- For each point in the grid (that isn't a wall), identify how many valid neighbours a point has, where a neighbour is a valid next move. If we're in a _channel_, then there will only be two neighbours: forwards and backwards. But if we're at a vertex, then there will be more than two. _Store these `vertices`._\n",
    "- Add our `start` and `end` points to the set of `vertices`.\n",
    "- Next, walk the maze from each `vertex` to the next, using [BFS](https://aoc.just2good.co.uk/python/shortest_paths). Whenever we detect the next vertex, join it to the previous vertex to create an _edge_. The edge weight is the distance between one vertex and the next.\n",
    "\n",
    "Then I decided to plot my new graph with NetworkX and Maptlotlib.\n",
    "\n",
    "![Snow Island Sample Graph](https://aoc.just2good.co.uk/assets/images/snow_island_graph_sample.png)\n",
    "\n",
    "By the way, here's a neat trick! If you'd rather not programmatically render the graph, e.g. with NetworkX, you can simply output the values of the adjacency dictionary, and then copy/paste them into the [CS Academy Graph Editor](https://csacademy.com/app/graph_editor/).  That way, you an quickly visualise your graph.\n",
    "\n",
    "![CS Academy Graph Editor](https://aoc.just2good.co.uk/assets/images/csacademy-graph-editor.png)\n",
    "\n",
    "Now we can use a _Depth First Search (DFS)_ to **explore all paths from start to end.** We have to use DFS rather than BFS, because the BFS would give us the _shortest path_.  But we need the longest path. So we need to explore all paths.\n",
    "\n",
    "As I describe [here](https://aoc.just2good.co.uk/python/shortest_paths), a DFS can be implemented in the same way as a BFS, but instead of a first-in, first-out (FIFO) queue, we want to implement a last-in, first-out (LIFO) stack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_vertices(grid):\n",
    "    \"\"\" Find all vertices in this grid. I.e. locations where more than one path is possible. \"\"\"\n",
    "    vertices = set()\n",
    "    \n",
    "    for y, row in enumerate(grid):\n",
    "        for x, char in enumerate(row):\n",
    "            if char == \"#\":\n",
    "                continue\n",
    "            \n",
    "            # we must be at a . or an arrow\n",
    "            neighbours = []\n",
    "            for dx, dy in VectorDicts.ARROWS.values():\n",
    "                if (0 <= x+dx < len(grid[0]) and 0 <= y+dy < len(grid)\n",
    "                        and grid[y+dy][x+dx] != \"#\"): # check if neighbour is a path\n",
    "                    neighbours.append((x+dx, y+dy))\n",
    "            \n",
    "            if len(neighbours) > 2:\n",
    "                vertices.add((x, y))\n",
    "    \n",
    "    return vertices\n",
    "    \n",
    "def build_edges(grid, vertices: set[tuple[int,int]], part) -> dict[tuple, set]:\n",
    "    \"\"\" BFS from each vertex to find all connected vertices. It returns an adjacency list \n",
    "    mapping each vertex to all adjacent vertices, along with distance. \"\"\"\n",
    "    edges: dict[tuple, set] = defaultdict(set) # { { edge_1: { (edge_2, distance), (edge_3, distance), ... }, ... }\n",
    "    \n",
    "    for vertex in vertices:\n",
    "        queue: deque[tuple[tuple,int]] = deque([(vertex, 0)]) # create deque from initial list\n",
    "        explored = set()\n",
    "        \n",
    "        while queue:\n",
    "            (x,y), distance = queue.popleft() # get current node and distance from last vertex\n",
    "            \n",
    "            if (x,y) in explored:\n",
    "                continue\n",
    "            \n",
    "            explored.add((x, y))\n",
    "            \n",
    "            # for each direction arrow, return the zip, e.g. (`<`, (-1, 0))\n",
    "            for arrow, (dx, dy) in zip(VectorDicts.ARROWS, VectorDicts.ARROWS.values()):\n",
    "                next_x, next_y = x+dx, y+dy\n",
    "                if (0 <= next_x < len(grid[0]) and 0 <= next_y < len(grid)\n",
    "                        and grid[next_y][next_x] != \"#\"): # valid move\n",
    "                    if (next_x, next_y) in vertices and (next_x, next_y) != vertex: # check if we've found the next vertex\n",
    "                        edges[vertex].add(((next_x, next_y), distance+1))\n",
    "                        continue # move on to next path with a different next vertex\n",
    "                    \n",
    "                    if part == 1:\n",
    "                        # Check we're following the required direction\n",
    "                        next_char = grid[next_y][next_x]\n",
    "                        if next_char in (VectorDicts.ARROWS) and next_char != arrow:\n",
    "                            continue # if we're not moving in the direction of the arrow\n",
    "                    \n",
    "                    # continue walking the current path\n",
    "                    queue.append(((next_x, next_y), distance+1))\n",
    "    \n",
    "    logger.debug(edges)\n",
    "    \n",
    "    # We can output the adjacency dicionary, e.g. to input into https://csacademy.com/app/graph_editor/\n",
    "    adjacency_vals = \"\"\n",
    "    for edge, adjacents in edges.items(): # e.g. (11, 3), { ((21, 11), 30), ((13, 13), 24) }\n",
    "        for adjacent, _ in adjacents:\n",
    "            edge_str = str(edge).replace(\" \", \"\")\n",
    "            adjacent_str = str(adjacent).replace(\" \", \"\")\n",
    "            adjacency_vals += edge_str + \" \" + adjacent_str + \"\\n\"\n",
    "\n",
    "    logger.debug(adjacency_vals)\n",
    "    \n",
    "    return edges\n",
    "\n",
    "def dfs(grid, edges: dict[tuple, set], from_vertex: tuple, goal: tuple, seen: set=set()) -> list:\n",
    "    \"\"\" Recursive DFS to find all path lengths from from_vertex to goal. \n",
    "\n",
    "    Args:\n",
    "        grid (_type_): _description_\n",
    "        edges (dict[tuple, set]): Adjacency dictionary mapping vertex to a set of (vertex, distance)\n",
    "        from_vertex (tuple): The current vertex\n",
    "        goal (tuple): The final vertex we want to reach\n",
    "        seen (set, optional): To track visisted vertices. First call will set it to empty.\n",
    "\n",
    "    Returns:\n",
    "        list: lengths of all valid paths\n",
    "    \"\"\"\n",
    "    if from_vertex == goal:\n",
    "        return [0] # Found a path, return a list with length 0 (since no more distance is needed)\n",
    "    \n",
    "    seen.add(from_vertex) # tp prevent backtracking in THIS path\n",
    "    path_lengths = []\n",
    "    \n",
    "    # explore each connected vertex\n",
    "    for next_vertex, distance in edges[from_vertex]:\n",
    "        if next_vertex not in seen: # prevent backtracking for this path\n",
    "            # recursively call from the next vertex onwards\n",
    "            for path_len in dfs(grid, edges, next_vertex, goal, seen):\n",
    "                path_lengths.append(path_len + distance) # adjust each length by adding the current dist\n",
    "        \n",
    "    seen.remove(from_vertex) # to allow other paths to visit this vertex\n",
    "    \n",
    "    return path_lengths\n",
    "\n",
    "def draw_graph(graph, start, end):\n",
    "    # Create a list of colors, one for each node\n",
    "    node_colors = ['blue' if node not in [start, end] else 'red' for node in graph.nodes()]\n",
    "    \n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos=pos, edge_color=\"grey\", width=1, with_labels=True, node_color=node_colors)\n",
    "    # Create a dictionary of edge labels. This dictionary maps each edge to its weight value.\n",
    "    edge_weights = {(u, v): d['weight'] for u, v, d in graph.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(graph, pos=pos, edge_labels=edge_weights)\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(data: list[str], part=1) -> int:\n",
    "    grid = [[char for char in row] for row in data]\n",
    "    logger.debug(\"\\n\" + \"\\n\".join(\"\".join(row) for row in grid))\n",
    "    \n",
    "    start = (data[0].index(\".\"), 0)\n",
    "    end = (data[-1].index(\".\"), len(data)-1)\n",
    "    logger.debug(f\"{start=}\")\n",
    "    logger.debug(f\"{end=}\")\n",
    "    \n",
    "    # identify the vertices\n",
    "    vertices = find_vertices(grid)\n",
    "    vertices.add(start)\n",
    "    vertices.add(end)\n",
    "    edges = build_edges(grid, vertices, part)\n",
    "        \n",
    "    # build a graph in NetworkX\n",
    "    graph = nx.Graph()\n",
    "    for edge_left, edges_right in edges.items():\n",
    "        for edge_r, dist in edges_right:\n",
    "            graph.add_edge(edge_left, edge_r, weight=dist)\n",
    "    \n",
    "    draw_graph(graph, start, end) # let's have a look at it\n",
    "\n",
    "    valid_path_lengths = dfs(grid, edges, start, end)\n",
    "    \n",
    "    return max(valid_path_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"\\\n",
    "#.#####################\n",
    "#.......#########...###\n",
    "#######.#########.#.###\n",
    "###.....#.>.>.###.#.###\n",
    "###v#####.#v#.###.#.###\n",
    "###.>...#.#.#.....#...#\n",
    "###v###.#.#.#########.#\n",
    "###...#.#.#.......#...#\n",
    "#####.#.#.#######.#.###\n",
    "#.....#.#.#.......#...#\n",
    "#.#####.#.#.#########v#\n",
    "#.#...#...#...###...>.#\n",
    "#.#.#v#######v###.###v#\n",
    "#...#.>.#...>.>.#.###.#\n",
    "#####v#.#.###v#.#.###.#\n",
    "#.....#...#...#.#.#...#\n",
    "#.#########.###.#.#.###\n",
    "#...###...#...#...#.###\n",
    "###.###.#.###v#####v###\n",
    "#...#...#.#.>.>.#.>.###\n",
    "#.###.###.#.###.#.#v###\n",
    "#.....###...###...#...#\n",
    "#####################.#\"\"\")\n",
    "sample_answers = [94]\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "soln = solve(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 23 Part 2\n",
    "\n",
    "Now we can ignore the arrows. They just behave like normal path `.` locations.\n",
    "\n",
    "**Find the longest hike you can take through the surprisingly dry hiking trails listed on your map. How many steps long is the longest hike?**\n",
    "\n",
    "Here, all we need to do is remove the check for the direction arrow. It does make the resulting graph significantly more complicated, and makes the number of valid paths huge.\n",
    "\n",
    "But the solution still runs in a reasonable amount of time. This is because we've already applied the edge contraction, so the number of paths we have to test is significantly smaller than it would have been."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_answers = [154]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve(curr_input.splitlines(), part=2), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve(input_data, part=2)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 24: Never Tell Me The Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"24\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 24 Part 1\n",
    "\n",
    "We have water in the air forming hail instead of snow. Hailstones are travelling through the air with linear trajectories described by input like this:\n",
    "\n",
    "```text\n",
    "19, 13, 30 @ -2,  1, -2\n",
    "18, 19, 22 @ -1, -1, -2\n",
    "20, 25, 34 @ -2, -2, -4\n",
    "12, 31, 28 @ -1, -2, -1\n",
    "20, 19, 15 @  1, -5, -3\n",
    "```\n",
    "\n",
    "Each row describes the position and velocity of a hailstone at time t=0.\n",
    "\n",
    "- The left part represents `x, y, z` location.\n",
    "- The right part represents the `x, y, z` velocities as units/nanosecond.\n",
    "\n",
    "We're asked to count intersections of hailstone paths within the test area, and in the future, ignoring the z axis. For the test data, we're told to check for intersections with `x` and `y` both between 7 and 27. For the real data, between 200000000000000 and 400000000000000.\n",
    "\n",
    "(I'm already scared about Part 2.)\n",
    "\n",
    "**Considering only the X and Y axes, check all pairs of hailstones' future paths for intersections. How many of these intersections occur within the test area?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "This feels like a `y = mx + c` problem. If you remember basic math from school, you'll recall that any straight line can be represented as `y = mx + c`, where `m` is the gradient of the line, and `c` is where the line crosses the `y` axis when `x=0`. When you know this formula for a given line, you can calculate any value of `y` for a given value of `x`, and vice versa.\n",
    "\n",
    "Furthermore, if you have two such lines, you can combine these equations to work out the point when the lines intersect. And that's what we need to do here.\n",
    "\n",
    "I've created a `Hailstone` class. (Though maybe it would be better named as _Trajectory_!)  This class:\n",
    "\n",
    "- Stores position at time `t=0` as a tuple of arbitrary length. (I did this because I want to be able to deal with just `x` and `y` in Part 1, but also include `z` in Part 2. We know this is coming!)\n",
    "- Stores the velocity in each axis. (We're told the velocity is constant.)\n",
    "- Since the gradient of a line is given by the change in `y` divided by the corresponding change in `x`, we can easily calculate the gradient for our `Hailstone` trajectories, by dividing the `y` velocity component (which is - by definition - a change over time of `1`) by the `x` velocity component. I need to handle the situation where the gradient is infinite, i.e. where the velocity in the `x` direction is `0`. I handle this by [raising an exception](https://aoc.just2good.co.uk/python/exceptions), which I can catch later and handle appropriately.\n",
    "- So now we have the `mx` component of `y = mx + c`. But we still need to determine `c`. So let's rearrange:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= mx + c \\\\\n",
    "c &= y - mx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- I've added a `y_intercept()` method to my class, to return this value.\n",
    "- Now that we have everything we need to express any given `Hailstone` trajectory as a line `y = mx + c`, we're ready to find the intersection between two such lines. Here, we have two lines, labelled `a` and `b`:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_{a} &= m_{a}x + c_{a} \\\\\n",
    "y_{b} &= m_{b}x + c_{b} \\\\\n",
    "\\\\\n",
    "\\text{At the intersection:} \\\\\n",
    "m_{a}x + c_{a} &= m_{b}x + c_{b} \\\\\n",
    "m_{a}x - m_{b}x &= c_{b} -  c_{a} \\\\\n",
    "x(m_{a} - m_{b}) = c_{b} -  c_{a} \\\\\n",
    "x = \\frac{c_{b} -  c_{a}}{m_{a} - m_{b}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- So this is exactly what my `intersects_with(hailstone)` does. And after it determines the value of `x`, it substitutes this back into `y = mx + c` to determine the corresponding value of `y`. It then returns the intersection location as the `(x,y)` tuple.\n",
    "\n",
    "Okay, so now we've implemented everything we need in order to determine if two hailstones have intersecting paths. And if they do intersect, we can check if the `x` and `y` values are within the required area.\n",
    "\n",
    "But wait! There's one more catch. We only want to count such trajectory intersections if they happen at some point in the future. So we need to determine the time `t` when any given hailstone will occupy the intersection point.  Again, this is just simple math. It's simply: the position now, minus the position originally, divided by the velocity.\n",
    "\n",
    "$$\n",
    "t_{x} = \\frac{x_{intersection} - x_{initial}}{v_{x}}\n",
    "$$\n",
    "\n",
    "Also: because this problem is simply looking for a crossover of two trajectories, rather than an intersection at a given point in time, then two hailstones might be at the intersection point at different times. So we need to determine _both_ times.\n",
    "\n",
    "So finally, to solve:\n",
    "\n",
    "- I use [itertools.combinations()](https://aoc.just2good.co.uk/python/perms_combos) to obtain every combination of two hailstones from the original supplied input.\n",
    "- Then I look for intersections for every pair.\n",
    "- If there is an intersection, I check that the `x` and `y` values of the intersection are within the test area.\n",
    "- And I check that the time that the hailstone arrives at that intersection is not in the past (i.e. negative `t`).\n",
    "\n",
    "And that's it!\n",
    "\n",
    "In conclusion... Part 1 needs a tiny bit of maths, but it's not the worst!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hailstone():\n",
    "    \"\"\" Represent the position of the stone at t=0, and the velocity \"\"\"\n",
    "    posn: tuple[float, ...]\n",
    "    velocity: tuple[int, ...]\n",
    "    \n",
    "    def posn_at_t(self, t: int) -> tuple[float, ...]:\n",
    "        \"\"\" Return the position at time t \"\"\"\n",
    "        new_axis_vals = []\n",
    "        for axis, axix_val in enumerate(self.posn):\n",
    "            new_axis_vals.append(axix_val + t*self.velocity[axis])\n",
    "            \n",
    "        return tuple(new_axis_vals)\n",
    "    \n",
    "    @property\n",
    "    def gradient(self) -> float:\n",
    "        \"\"\" \n",
    "        Determine the gradient as given by (y velocity) / (x velocity)\n",
    "        Raises ValueError if the x velocity is 0, then the gradient is infinite\n",
    "        \"\"\"\n",
    "        if self.velocity[0] == 0:\n",
    "            raise ValueError(\"Infinite gradient\")\n",
    "            \n",
    "        return self.velocity[1] / self.velocity[0]\n",
    "    \n",
    "    @property\n",
    "    def y_intercept(self) -> float:\n",
    "        \"\"\" \n",
    "        Determine the c value of y = mx + c \n",
    "        c = y - mx \n",
    "        \"\"\"\n",
    "        return self.posn[1] - self.gradient*self.posn[0]\n",
    "    \n",
    "    def intersects_with(self, other: Hailstone) -> tuple[float, ...]:\n",
    "        \"\"\" \n",
    "        For y = mx + c\n",
    "        x = (c2-c1) / (m1-m2)\n",
    "        \"\"\"\n",
    "        if self.gradient == other.gradient:\n",
    "            raise ValueError(\"Parallel paths\")\n",
    "            \n",
    "        x = (other.y_intercept - self.y_intercept) / (self.gradient - other.gradient)\n",
    "        y = self.gradient*x + self.y_intercept\n",
    "        \n",
    "        return (x,y)\n",
    "    \n",
    "    def time_at_posn(self, posn: tuple[float, ...]) -> float:\n",
    "        \"\"\" \n",
    "        Determine the time that this stone was at this position.\n",
    "        t = (xi - x0) / vx\n",
    "        where xi is the x coord at a given location\n",
    "              x0 is the original x coord\n",
    "              vx is the velocity in the x direction\n",
    "        \"\"\"\n",
    "        return (posn[0] - self.posn[0]) / self.velocity[0]\n",
    "            \n",
    "def parse_stones(data) -> list[Hailstone]:\n",
    "    stones = []\n",
    "    for row in data:\n",
    "        posn, velocity = row.split(\"@\")\n",
    "        posn = [int(val) for val in posn.split(\",\")]\n",
    "        velocity = [int(val) for val in velocity.split(\",\")]\n",
    "        stones.append(Hailstone(tuple(posn), tuple(velocity)))\n",
    "    return stones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data: list[str], coord_min: int, coord_max: int) -> int:\n",
    "    \"\"\" \n",
    "    Detect any intersection of paths within the specified x,y area\n",
    "    and where the time that this hailstone occupies this location is not in the past. \n",
    "    \"\"\"\n",
    "    stones = parse_stones(data)\n",
    "    \n",
    "    intersections = 0\n",
    "    for stone_a, stone_b in combinations(stones, 2):\n",
    "        try:\n",
    "            intersect = stone_a.intersects_with(stone_b)\n",
    "            t_a = stone_a.time_at_posn(intersect)\n",
    "            t_b = stone_b.time_at_posn(intersect)\n",
    "            if (coord_min <= intersect[0] <= coord_max \n",
    "                    and coord_min <= intersect[1] <= coord_max\n",
    "                    and t_a >= 0 and t_b >= 0):\n",
    "                intersections += 1\n",
    "            # logger.debug(f\"{intersect} at {t_a=}, {t_b=}\")\n",
    "        except ValueError as e:\n",
    "            logger.debug(e) # e.g. parallel lines\n",
    "    \n",
    "    return intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append((\"\"\"\\\n",
    "19, 13, 30 @ -2,  1, -2\n",
    "18, 19, 22 @ -1, -1, -2\n",
    "20, 25, 34 @ -2, -2, -4\n",
    "12, 31, 28 @ -1, -2, -1\n",
    "20, 19, 15 @  1, -5, -3\"\"\", 7, 27))\n",
    "sample_answers = [2]\n",
    "\n",
    "for (curr_input, curr_min, curr_max), curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input.splitlines(), curr_min, curr_max), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part1(input_data, 200000000000000, 400000000000000)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 24 Part 2\n",
    "\n",
    "Whilst I could just about write the math here, the idea to use the [SymPy](https://www.sympy.org/en/index.html) library came from the amazing [Hyperneutrino](https://hyper-neutrino.xyz/). I'm not sure I would have been able to solve this one without that bit of magic.\n",
    "\n",
    "Anyhoo...\n",
    "\n",
    "We want to throw a rock that will collide with EVERY hailstone at some point in time. (In Part 1, we were only checking that the trajectories intersected.) We can throw our rock from any starting position, and with any arbitrary velocity. The rock itself won't change position or velocity when it hits a hailstone.\n",
    "\n",
    "**Determine the exact position and velocity the rock needs to have at time 0 so that it perfectly collides with every hailstone. What do you get if you add up the X, Y, and Z coordinates of that initial position?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "First, let's understand the math:\n",
    "\n",
    "We can determine the time `t` when the rock (`r`) will hit a hailstone (`h`). Recall that location is given by initial location, plus `time * velocity`.\n",
    "\n",
    "First, in the `x` direction only:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Let } & x_{h}\\text{ be the location of the hailstone} \\notag \\\\\n",
    "& x_{r}\\text{ be the location of the rock} \\notag \\\\\n",
    "& v_{x_{h}}\\text{ be the x velocity of the hailstone} \\notag \\\\\n",
    "& v_{x_{r}}\\text{ be the x velocity of the hailstone} \\notag \\\\\n",
    "\\notag \\\\ \n",
    "x_{h} + tv_{x_{h}} &= x_{r} + tv_{x_{r}} \\\\\n",
    "tv_{x_{h}} - tv_{x_{r}} &= x_{r} - x_{h} \\\\\n",
    "t &= \\frac{x_{r} - x_{h}}{v_{x_{h}} - v_{x_{r}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So at this time `t`, we have a collision, if only considering `x`. But this equation must be true for all three vectors. I.e. the rock and hailstone must collide at the same time in all three dimensions:\n",
    "\n",
    "$$\n",
    "t = \\frac{x_{r} - x_{h}}{v_{x_{h}} - v_{x_{r}}} = \\frac{y_{r} - y_{h}}{v_{y_{h}} - v_{y_{r}}} = \\frac{z_{r} - z_{h}}{v_{z_{h}} - v_{z_{r}}}\n",
    "$$\n",
    "\n",
    "So now we can rearrange such that we can relate the rock's position and velocity to each hailstone's known position and velocity.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(x_{r} - x_{h})(v_{y_{h}} - v_{y_{r}}) &= (y_{r} - y_{h})(v_{x_{h}} - v_{x_{r}}) \\\\\n",
    "(y_{r} - y_{h})(v_{z_{h}} - v_{z_{r}}) &= (z_{r} - z_{h})(v_{y_{h}} - v_{y_{r}}) \\\\\n",
    "(z_{r} - z_{h})(v_{x_{h}} - v_{x_{r}}) &= (x_{r} - x_{h})(v_{z_{h}} - v_{z_{r}}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now we have a set of equations that are true for any given hailstone, we can loop through hailstones and build up a set of equestions that must be true.\n",
    "\n",
    "Here I'm using the awesome SymPy library, which allows us to [solve equations](https://docs.sympy.org/latest/guides/solving/index.html#solving-guide). We can use the `solve()` function to [find the solution to a system of equations](https://docs.sympy.org/latest/guides/solving/solve-system-of-equations-algebraically.html). \n",
    "\n",
    "Here's how we use it:\n",
    "\n",
    "- Define the mathematical (unknown) symbols we want to find solutions for.\n",
    "- Build a set of equations, using hailstone positions and velocities to fill in the known variables.\n",
    "- Then use the `solve()` function to obtain all solutions. With a sufficient number of hailstones, there will be only one solution. (Note that we don't need many hailstones to get a unique solution. If we don't limit the number of hailstones, the solution still runs fairly quickly. But it much much faster if we limit the number of hailstones.)\n",
    "- Retrieve the one-and-only solution, and extract the rock location values from it. (We don't care about the velocities.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data: list[str]):\n",
    "    \"\"\" \n",
    "    Determine the sum of the rock's (x,y,z) coordinate at t=0, for a rock that will hit every hailstone\n",
    "    in our input data. The rock has constant velocity and is not affected by collisions.\n",
    "    \"\"\"\n",
    "    stones = parse_stones(data)\n",
    "    logger.debug(f\"We have {len(stones)} stones.\")\n",
    "    \n",
    "    # define SymPy rock symbols - these are our unknowns representing:\n",
    "    # initial rock location (xr, yr, zr) and rock velocity (vxr, vyr, vzr)\n",
    "    xr, yr, zr, vxr, vyr, vzr = sympy.symbols(\"xr yr zr vxr vyr vzr\")\n",
    "    \n",
    "    equations = [] # we assemble a set of equations that must be true\n",
    "    for stone in stones[:10]: # we don't need ALL the stones to find a solution. We need just enough.\n",
    "        x, y, z = stone.posn\n",
    "        vx, vy, vz = stone.velocity\n",
    "        # equations.append(sympy.Eq((xr-x)/(vx-vxr), (yr-y)/(vy-vyr)))\n",
    "        # equations.append(sympy.Eq((yr-y)/(vy-vyr), (zr-z)/(vz-vzr)))\n",
    "        # Cross-multiply...\n",
    "        equations.append(sympy.Eq((xr-x)*(vy-vyr), (yr-y)*(vx-vxr)))\n",
    "        equations.append(sympy.Eq((yr-y)*(vz-vzr), (zr-z)*(vy-vyr)))\n",
    "    \n",
    "    solutions = sympy.solve(equations, dict=True) # SymPy does the hard work\n",
    "    if solutions:\n",
    "        solution = solutions[0]\n",
    "        logger.info(solution)\n",
    "        return sum([solution[xr], solution[yr], solution[zr]])\n",
    "    \n",
    "    logger.info(\"No solutions found.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"\\\n",
    "19, 13, 30 @ -2,  1, -2\n",
    "18, 19, 22 @ -1, -1, -2\n",
    "20, 25, 34 @ -2, -2, -4\n",
    "12, 31, 28 @ -1, -2, -1\n",
    "20, 19, 15 @  1, -5, -3\"\"\")\n",
    "sample_answers = [47]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2(curr_input.splitlines()), curr_ans) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day 25: Snowverload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"25\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 25 Part 1\n",
    "\n",
    "We have too many components plugged into the Snow Producer.\n",
    "\n",
    "We have to disconnect three wires.\n",
    "\n",
    "Sample input:\n",
    "\n",
    "```text\n",
    "jqt: rhn xhk nvd\n",
    "rsh: frs pzl lsr\n",
    "xhk: hfx\n",
    "cmg: qnr nvd lhk bvb\n",
    "rhn: xhk bvb hfx\n",
    "bvb: xhk hfx\n",
    "pzl: lsr hfx nvd\n",
    "qnr: nvd\n",
    "ntq: jqt hfx bvb xhk\n",
    "nvd: lhk\n",
    "lsr: lhk\n",
    "rzs: qnr cmg lsr rsh\n",
    "frs: qnr lhk lsr\n",
    "```\n",
    "\n",
    "This shows undirected connections between components.  A component might only be on one side.\n",
    "\n",
    "**Find the three wires you need to disconnect in order to divide the components into two separate groups. What do you get if you multiply the sizes of these two groups together?**\n",
    "\n",
    "**My solution:**\n",
    "\n",
    "Okay, we have a graph of connected parts. \n",
    "\n",
    "![2023 Day 25 Part Graph](https://aoc.just2good.co.uk/assets/images/2023d25_sample_graph.png)\n",
    "\n",
    "\n",
    "\n",
    "We need to split this into two separate graphs. We're told that we can split our graph into two graphs by making three cuts. So _I assume that there is only one way to split our graph into two graphs, with three cuts._\n",
    "\n",
    "Splitting a graph into two separate graphs is a well known problem, and can be solved with the **max-flow cut-cut theorem.** Check out some links:\n",
    "\n",
    "- [Max-flow min-cut theorem - Wikipedia](https://en.wikipedia.org/wiki/Max-flow_min-cut_theorem)\n",
    "- [Max-flow min-cut algorithm - Brilliant.org](https://brilliant.org/wiki/max-flow-min-cut-algorithm)\n",
    "\n",
    "We can build and visualise a graph using [NetworkX](https://aoc.just2good.co.uk/python/networkx). And fortunately, NetworkX has a method that partitions the graph with a minimum number of cuts, called [minimum_cut](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.flow.minimum_cut.html). \n",
    "\n",
    "The method requires:\n",
    "\n",
    "- The graph\n",
    "- A node on the left that represents a source\n",
    "- A node on the right that represents a sink\n",
    "\n",
    "So, we can keep our left constant, and then try all other nodes as sinks.  Then, for we apply the algorithm to determine the minimum number of cuts required to partition the graph into two graphs, where one contains the source, and the other contains the sink.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(data):\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    for line in data:\n",
    "        left, right = (part.strip() for part in line.split(\":\"))\n",
    "        right_parts = right.split()\n",
    "        \n",
    "        graph.add_node(left)\n",
    "        for r_part in right_parts:\n",
    "            # When using minimum_cut, edges are expected to have capacity\n",
    "            # Here, all the edges are equal, so we can use capacity of 1.\n",
    "            graph.add_edge(left, r_part, capacity=1.0)\n",
    "            \n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos=pos, edge_color=\"grey\", width=1, with_labels=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    plt.show()\n",
    "    \n",
    "    left = next(iter(graph.nodes)) # pick an arbitrary node to be the source (for partitin 1)\n",
    "    for right in graph.nodes: # iterate through remaining nodes as sinks (for partition 2)\n",
    "        if left != right:\n",
    "            cut_val, partitions = nx.minimum_cut(graph, left, right)\n",
    "            if cut_val == 3:\n",
    "                return math.prod(len(partition) for partition in partitions)\n",
    "    \n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_input = \"\"\"\\\n",
    "jqt: rhn xhk nvd\n",
    "rsh: frs pzl lsr\n",
    "xhk: hfx\n",
    "cmg: qnr nvd lhk bvb\n",
    "rhn: xhk bvb hfx\n",
    "bvb: xhk hfx\n",
    "pzl: lsr hfx nvd\n",
    "qnr: nvd\n",
    "ntq: jqt hfx bvb xhk\n",
    "nvd: lhk\n",
    "lsr: lhk\n",
    "rzs: qnr cmg lsr rsh\n",
    "frs: qnr lhk lsr\"\"\"\n",
    "\n",
    "sample_answer = 54\n",
    "\n",
    "validate(solve(sample_input.splitlines()), sample_answer) # test with sample data\n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 25 Part 2\n",
    "\n",
    "As usual, there is no Part 2!  We simply need to have obtained all the rest of the stars, in order to collect this bonus star."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Day n: title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAY = \"n\" # replace with actual number (without leading digit)\n",
    "day_link = f\"#### See [Day {DAY}](https://adventofcode.com/{YEAR}/day/{DAY}).\"\n",
    "display(Markdown(day_link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = \"d\" + str(DAY).zfill(2) # e.g. d01\n",
    "script_name = \"aoc\" + str(YEAR) + d_name # e.g. aoc2017d01\n",
    "locations = get_locations(d_name)\n",
    "\n",
    "# SETUP LOGGING\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# td.setup_file_logging(logger, locations.output_dir)\n",
    "\n",
    "# Retrieve input and store in local file\n",
    "try:\n",
    "    write_puzzle_input_file(YEAR, DAY, locations)\n",
    "    with open(locations.input_file, mode=\"rt\") as f:\n",
    "        input_data = f.read().splitlines()\n",
    "\n",
    "    logger.info(\"Input data:\\n%s\", top_and_tail(input_data))\n",
    "except ValueError as e:\n",
    "    logger.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day n Part 1\n",
    "\n",
    "Overview..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part1(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"abcdef\"\"\")\n",
    "sample_answers = [\"uvwxyz\"]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part1(curr_input), curr_ans) # test with sample data\n",
    "    logger.info(\"Test passed\")\n",
    "\n",
    "logger.info(\"All tests passed!\")\n",
    "\n",
    "soln = solve_part1(input_data)\n",
    "logger.info(f\"Part 1 soln={soln}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day n Part 2\n",
    "\n",
    "Overview..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_part2(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_inputs = []\n",
    "sample_inputs.append(\"\"\"abcdef\"\"\")\n",
    "sample_answers = [\"uvwxyz\"]\n",
    "\n",
    "for curr_input, curr_ans in zip(sample_inputs, sample_answers):\n",
    "    validate(solve_part2(curr_input), curr_ans) # test with sample data\n",
    "    logger.info(\"Test passed\")    \n",
    "\n",
    "logger.info(\"Tests passed!\")\n",
    "\n",
    "soln = solve_part2(input_data)\n",
    "logger.info(f\"Part 2 soln={soln}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "aca_aoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
